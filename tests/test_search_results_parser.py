"""
æµ‹è¯•æœç´¢ç»“æœè§£æå™¨

ç”¨æ³•ï¼š
1. å°†æœç´¢å¼•æ“ç»“æœé¡µçš„ HTML ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚ google_search.htmlï¼‰
2. è¿è¡Œæµ‹è¯•ï¼špython tests/test_search_results_parser.py <html_file_path>
"""

import sys
import os
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from agentmatrix.skills.search_results_parser import SearchResultsParser


def test_parse_html_file(html_file_path: str, search_url: str):
    """
    æµ‹è¯•è§£ææœ¬åœ° HTML æ–‡ä»¶

    Args:
        html_file_path: HTML æ–‡ä»¶è·¯å¾„
        search_url: æ¨¡æ‹Ÿçš„æœç´¢ç»“æœé¡µ URLï¼ˆç”¨äºåˆ¤æ–­æœç´¢å¼•æ“ï¼‰
    """
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•æ–‡ä»¶: {html_file_path}")
    print(f"æ¨¡æ‹Ÿ URL: {search_url}")
    print(f"{'='*80}\n")

    # è¯»å– HTML æ–‡ä»¶
    print("ğŸ“‚ è¯»å– HTML æ–‡ä»¶...")
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        print(f"âœ“ æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {len(html_content)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # åˆ›å»ºè§£æå™¨
    print("\nğŸ” åˆ›å»ºè§£æå™¨...")

    # åˆ›å»ºç®€å•çš„ logger
    import logging
    logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')
    logger = logging.getLogger(__name__)

    parser = SearchResultsParser(logger=logger)

    # è§£ææœç´¢ç»“æœ
    print("\nâš™ï¸  å¼€å§‹è§£æ...")
    try:
        parsed_data = parser.parse(html_content, search_url)
        print(f"âœ“ è§£æå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # æ˜¾ç¤ºè§£æç»“æœ
    print(f"\nğŸ“Š è§£æç»“æœç»Ÿè®¡:")
    print(f"  æœç´¢å¼•æ“: {parsed_data['search_engine']}")
    print(f"  æœç´¢ç»“æœæ•°é‡: {len(parsed_data['results'])}")
    print(f"  æœ‰æ™ºèƒ½å›ç­”: {'æ˜¯' if parsed_data['featured_snippet'] else 'å¦'}")

    if parsed_data['featured_snippet']:
        print(f"\nğŸ“ æ™ºèƒ½å›ç­” (å‰200å­—ç¬¦):")
        print(f"  {parsed_data['featured_snippet'][:200]}...")

    # æ˜¾ç¤ºå‰3ä¸ªæœç´¢ç»“æœ
    print(f"\nğŸ”— æœç´¢ç»“æœ (å‰3æ¡):")
    for idx, result in enumerate(parsed_data['results'][:3], start=1):
        print(f"\n  [{idx}] {result.link_id}")
        print(f"      æ ‡é¢˜: {result.title}")
        print(f"      URL: {result.url}")
        if result.site_info:
            print(f"      ç«™ç‚¹: {result.site_info}")
        if result.snippet:
            snippet_preview = result.snippet[:150] + "..." if len(result.snippet) > 150 else result.snippet
            print(f"      æ‘˜è¦: {snippet_preview}")

    # ç”Ÿæˆæ ¼å¼åŒ–çš„ Markdown
    print(f"\nğŸ“ æ ¼å¼åŒ–çš„ Markdown:")
    print(f"{'='*80}")
    formatted_markdown = parser.format_as_markdown(parsed_data)
    print(formatted_markdown)
    print(f"{'='*80}")

    # æ˜¾ç¤ºé“¾æ¥æ˜ å°„
    print(f"\nğŸ”— é“¾æ¥æ˜ å°„ (å‰3æ¡):")
    link_mapping = parser.build_link_mapping(parsed_data)
    for idx, (link_id, url) in enumerate(list(link_mapping.items())[:3], start=1):
        print(f"  [{idx}] {link_id}")
        print(f"      -> {url}")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = html_file_path.replace('.html', '_parsed.txt')
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"=== è§£æç»“æœ ===\n\n")
            f.write(f"æœç´¢å¼•æ“: {parsed_data['search_engine']}\n")
            f.write(f"æœç´¢ç»“æœæ•°é‡: {len(parsed_data['results'])}\n\n")
            f.write(f"=== æ ¼å¼åŒ–çš„ Markdown ===\n\n")
            f.write(formatted_markdown)
            f.write(f"\n\n=== é“¾æ¥æ˜ å°„ ===\n\n")
            for link_id, url in link_mapping.items():
                f.write(f"{link_id} -> {url}\n")
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"\nâš ï¸  ä¿å­˜ç»“æœå¤±è´¥: {e}")

    print(f"\n{'='*80}")
    print("âœ“ æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*80}\n")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python test_search_results_parser.py <html_file_path>")
        print("\nç¤ºä¾‹:")
        print("  python test_search_results_parser.py google_search.html")
        print("  python test_search_results_parser.py bing_search.html")
        print("\næç¤º:")
        print("  1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æœç´¢å¼•æ“ï¼ˆGoogle æˆ– Bingï¼‰")
        print("  2. æœç´¢ä»»æ„å†…å®¹")
        print("  3. å³é”® -> ä¿å­˜ç½‘é¡µ -> é€‰æ‹©'ç½‘é¡µï¼Œå…¨éƒ¨' -> ä¿å­˜ä¸º .html æ–‡ä»¶")
        print("  4. è¿è¡Œæ­¤æµ‹è¯•è„šæœ¬")
        sys.exit(1)

    html_file_path = sys.argv[1]

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(html_file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {html_file_path}")
        sys.exit(1)

    # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æœç´¢å¼•æ“
    filename = Path(html_file_path).name.lower()
    if 'google' in filename:
        search_url = "https://www.google.com/search?q=test"
    elif 'bing' in filename:
        search_url = "https://www.bing.com/search?q=test"
    else:
        # é»˜è®¤ä½¿ç”¨ Google
        search_url = "https://www.google.com/search?q=test"
        print(f"âš ï¸  æ— æ³•ä»æ–‡ä»¶ååˆ¤æ–­æœç´¢å¼•æ“ï¼Œé»˜è®¤ä½¿ç”¨ Google")

    # è¿è¡Œæµ‹è¯•
    test_parse_html_file(html_file_path, search_url)


if __name__ == "__main__":
    main()
