import json
import traceback
from typing import Dict, Union, List, Optional, TYPE_CHECKING
import aiohttp
from ..core.log_util import AutoLoggerMixin
import logging
from ..core.exceptions import (
    LLMServiceUnavailableError,
    LLMServiceTimeoutError,
    LLMServiceConnectionError,
    LLMServiceAPIError
)

if TYPE_CHECKING:
    from ..core.log_config import LogConfig

class LLMClient(AutoLoggerMixin):

    _custom_log_level = logging.DEBUG
    def __init__(self, url: str, api_key: str, model_name: str,
                 parent_logger: Optional[logging.Logger] = None,
                 log_config: Optional['LogConfig'] = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            url (str): å¤§æ¨¡å‹APIçš„URL
            api_key (str): APIå¯†é’¥
            model_name (str): æ¨¡å‹åç§°
            parent_logger (Optional[logging.Logger]): çˆ¶ç»„ä»¶çš„loggerï¼ˆç”¨äºå…±äº«æ—¥å¿—ï¼‰
            log_config (Optional[LogConfig]): æ—¥å¿—é…ç½®
        """
        self.url = url
        self.api_key = api_key
        self.model_name = model_name

        # ä½¿ç”¨çˆ¶ loggerï¼ˆä¸åˆ›å»ºç‹¬ç«‹æ—¥å¿—æ–‡ä»¶ï¼‰
        self._parent_logger = parent_logger
        self._log_config = log_config
        self._log_prefix_template = log_config.prefix if log_config else ""

        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.gemini_headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": self.api_key
        }

    def _get_log_context(self) -> dict:
        """æä¾›æ—¥å¿—ä¸Šä¸‹æ–‡å˜é‡"""
        return {
            "model": self.model_name,
            "name": self.model_name
        }

    # In AdvancedMarkdownEditingMixin class

    async def think_with_retry(self,
                                    initial_messages: Union[str, List[str]],
                                    parser: callable,
                                    max_retries: int = 3,
                                    debug: bool = True,
                                    **parser_kwargs) -> any:
        """
        A generic micro-agent that interacts with an LLM in a loop until the
        output is successfully parsed.

        Args:
            initial_messages (list): The starting list of messages for the conversation.
            parser (callable): A function that takes a raw LLM reply string and
                            returns a dict following the Parser Contract.
            max_retries (int): The maximum number of attempts before failing.
            debug (bool): If True, output detailed debug information including LLM input/output.

        Returns:
            The "data" field from the successful parser result.

        Raises:
            ValueError: If the LLM fails to produce a parsable response after all retries.
        """

        if isinstance(initial_messages, str):
            #å¦‚æœmessages æ˜¯string,å°±åŒ…è£…æˆopen ai chat messages çš„æ ¼å¼
            messages =[{"role": "user", "content": initial_messages}]
        else:
            messages = initial_messages

        if debug:
            self.logger.debug(f"=== think_with_retry DEBUG START ===")
            self.logger.debug(f"Initial messages ({len(messages)} messages):")
            for i, msg in enumerate(messages):
                self.logger.debug(f"  [{i}] {msg.get('role')}: {msg.get('content')[:200]}{'...' if len(msg.get('content', '')) > 200 else ''}")
        
        for attempt in range(max_retries):
            try:
                response = await self.think(messages=messages)
                raw_reply = response['reply']

                if debug:
                    self.logger.debug(f"\nLLM Response (raw_reply):")
                    self.logger.debug(f"  {raw_reply[:500]}...")
                    

                # Delegate parsing to the provided parser function
                parsed_result = parser(raw_reply, **parser_kwargs)

                if debug:
                    self.logger.debug(f"\nParser result:")
                    self.logger.debug(f"  {parsed_result}")
                    

                if parsed_result.get("status") == "success":
                    
                    # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼š{"status": "success", "content": ...}
                    if "content" in parsed_result:
                        return parsed_result["content"]
                    else:
                        # æ²¡æœ‰å†…å®¹å­—æ®µï¼Œè¿”å›ç©ºå­—å…¸
                        return {}

                elif parsed_result.get("status") == "error":
                    if attempt == max_retries - 1:
                        # Final attempt failed
                        raise ValueError("LLM failed to produce a valid response after all retries.")

                    # ğŸ”¥ é‡è¯•ç­–ç•¥ï¼šå¢å¼ºåŸå§‹ promptï¼ˆä¸ç´¯ç§¯å†å²ï¼‰
                    # æå–åŸå§‹ user messageï¼ˆç¬¬ä¸€æ¡ï¼‰
                    original_prompt = messages[0]["content"]
                    feedback = parsed_result.get("feedback", "è¯·æ£€æŸ¥è¾“å‡ºæ ¼å¼")

                    # åœ¨åŸå§‹ prompt æœ«å°¾æ·»åŠ  feedbackï¼ˆå¼ºè°ƒæ ¼å¼ï¼‰
                    enhanced_prompt = f"{original_prompt}\n\n{feedback}"

                    # é‡ç½® messagesï¼ˆä¸ä¿ç•™é”™è¯¯å†å²ï¼‰
                    messages = [{"role": "user", "content": enhanced_prompt}]

                else:
                    # The parser itself is faulty
                    raise TypeError("Parser function returned an invalid contract response.")

            except Exception as e:
                self.logger.exception(f"Micro-Agent: An unexpected error occurred during invocation attempt {attempt + 1}.")
                raise
                
                
        # This line should theoretically be unreachable
        raise RuntimeError("Micro-Agent loop exited unexpectedly.")

    async def look_and_retry(self,
                            prompt: str,
                            image: str,
                            parser: callable,
                            max_retries: int = 3,
                            debug: bool = True,
                            **parser_kwargs) -> any:
        """
        Vision version of think_with_retry - interacts with a Vision LLM in a loop
        until the output is successfully parsed.

        Almost identical to think_with_retry, but:
        - Uses think_with_image() instead of think()
        - User messages must use format: {"role": "user", "content": [{"type": "text", "text": "..."}]}
          (to match the multi-modal content format expected by vision APIs)

        Args:
            prompt (str): The text prompt to send to the Vision LLM.
            image (str): Base64 encoded image string.
            parser (callable): A function that takes a raw LLM reply string and
                            returns a dict following the Parser Contract.
            max_retries (int): The maximum number of attempts before failing.
            debug (bool): If True, output detailed debug information including LLM input/output.

        Returns:
            The "content" field from the successful parser result.

        Raises:
            ValueError: If the Vision LLM fails to produce a parsable response after all retries.
        """
        # Initial message with proper multi-modal format
        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]

        if debug:
            print(f"\n{'='*80}")
            print(f"ğŸ”„ look_and_retry å¼€å§‹ (æœ€å¤š {max_retries} æ¬¡å°è¯•)")
            print(f"{'='*80}")
            print(f"ğŸ“ Prompt (å‰200å­—ç¬¦): {prompt[:200]}{'...' if len(prompt) > 200 else ''}")
            print(f"ğŸ“¸ å›¾ç‰‡å¤§å°: {len(image)} bytes (base64)")
            print(f"{'='*80}\n")

        for attempt in range(max_retries):
            try:
                if debug:
                    print(f"\n{'â”€'*80}")
                    print(f"ğŸ”„ ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•")
                    print(f"{'â”€'*80}")

                # Always use think_with_image (it handles the multi-modal format)
                raw_reply = await self.think_with_image(
                    messages=messages,
                    image=image
                )

                if debug:
                    print(f"ğŸ“¥ Vision LLM åŸå§‹å›å¤:")
                    print(f"{'â”€'*80}")
                    print(raw_reply[:500] + ('...' if len(raw_reply) > 500 else ''))
                    print(f"{'â”€'*80}\n")

                # Delegate parsing to the provided parser function
                parsed_result = parser(raw_reply, **parser_kwargs)

                if debug:
                    print(f"ğŸ” Parser è§£æç»“æœ:")
                    print(f"{'â”€'*80}")
                    for key, value in parsed_result.items():
                        print(f"  {key}: {value}")
                    print(f"{'â”€'*80}\n")

                if parsed_result.get("status") == "success":
                    if debug:
                        print(f"âœ… è§£ææˆåŠŸï¼")
                        print(f"{'='*80}\n")
                    # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼š{"status": "success", "content": ...}
                    if "content" in parsed_result:
                        return parsed_result["content"]
                    else:
                        # æ²¡æœ‰å†…å®¹å­—æ®µï¼Œè¿”å›ç©ºå­—å…¸
                        return {}

                elif parsed_result.get("status") == "error":
                    feedback = parsed_result.get("feedback", "Your previous response was invalid. Please try again.")

                    if debug:
                        print(f"âŒ è§£æå¤±è´¥")
                        print(f"ğŸ“ é”™è¯¯åé¦ˆ: {feedback[:300]}{'...' if len(feedback) > 300 else ''}")
                        print(f"ğŸ”„ å‡†å¤‡é‡è¯•...\n")

                    # Append assistant response (plain text)
                    messages.append({"role": "assistant", "content": raw_reply})
                    # Append user feedback (must use multi-modal format with only text, no image)
                    messages.append({
                        "role": "user",
                        "content": [{"type": "text", "text": feedback}]
                    })

                    if attempt == max_retries - 1:
                        # Final attempt failed
                        if debug:
                            print(f"\n{'='*80}")
                            print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œä»ç„¶å¤±è´¥")
                            print(f"æœ€åé”™è¯¯: {feedback}")
                            print(f"{'='*80}\n")
                        raise ValueError(f"Vision LLM failed to produce a valid response after {max_retries} retries. Last error: {feedback}")

                else:
                    # The parser itself is faulty
                    raise TypeError("Parser function returned an invalid contract response.")

            except ValueError:
                # Re-raise ValueError (from final attempt failure)
                raise
            except Exception as e:
                if debug:
                    print(f"âš ï¸  æ„å¤–é”™è¯¯: {str(e)}")
                    print(f"ğŸ”„ å‡†å¤‡é‡è¯•...\n")
                self.logger.exception(f"look_and_retry: An unexpected error occurred during invocation attempt {attempt + 1}.")
                if attempt == max_retries - 1:
                    if debug:
                        print(f"\n{'='*80}")
                        print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‘ç”Ÿå¼‚å¸¸")
                        print(f"å¼‚å¸¸: {str(e)}")
                        print(f"{'='*80}\n")
                    raise
                # If not the last attempt, add feedback and continue
                messages.append({
                    "role": "user",
                    "content": [{"type": "text", "text": f"Error occurred: {str(e)}. Please try again."}]
                })

        # This line should theoretically be unreachable
        raise RuntimeError("look_and_retry loop exited unexpectedly.")

    async def dialog_with_retry(
        self,
        producer_task: str,
        producer_persona: str,
        verifier_task_template: str,
        verifier_persona: str,
        producer_parser: Optional[callable] = None,
        approver_parser: Optional[callable] = None,
        max_rounds: int = 3
    ) -> dict:
        """
        Dialog-based retry with two-layer validation (structure + semantics).

        Layer 1 - Structural validation (producer_parser):
        - Ensures A's output format is correct (e.g., has required sections)
        - Validated by code rules (parser function)
        - B only sees structurally correct outputs

        Layer 2 - Semantic validation (approver_parser):
        - Ensures A's output quality is good enough
        - Validated by LLM intelligence (Verifier B)
        - Deep evaluation and improvement

        Args:
            producer_task: A's initial task
            producer_persona: A's persona
            verifier_task_template: B's evaluation task template with {producer_output}
            verifier_persona: B's persona
            producer_parser: Optional parser to validate A's output structure.
                            If provided, A uses think_with_retry internally.
            approver_parser: Optional parser to check if B approves.
                            Returns {"status": "success"} if approved.
            max_rounds: Maximum dialog rounds

        Returns:
            {
                "status": "success",
                "content": Parsed data from A (if producer_parser) or raw text,
                "rounds_used": int,
                "max_rounds_exceeded": bool,
                "last_feedback": str (only if exceeded)
            }

        Example:
            result = await llm_client.dialog_with_retry(
                producer_task="Write a research plan",
                producer_persona="You are a researcher",
                verifier_task_template="Review:\\n{producer_output}",
                verifier_persona="You are a director",
                producer_parser=research_plan_parser,  # Validates structure
                approver_parser=director_approval_parser,  # Validates approval
                max_rounds=3
            )
            # result["content"] is already parsed: {"[ç ”ç©¶è®¡åˆ’]": "...", ...}
        """
        last_a_output_raw = None
        last_a_output_parsed = None
        last_b_feedback = None

        self.logger.info(f"ğŸ­ Dialog-With-Retry: Starting (max {max_rounds} rounds)")

        for round_num in range(1, max_rounds + 1):
            self.logger.info(f"ğŸ­ Round {round_num}:")

            # ========== Phase 1: Producer (A) generates output ==========
            if round_num == 1:
                a_messages = [{"role": "user", "content": producer_task}]
            else:
                a_messages = [
                    {"role": "user", "content": producer_task},
                    {"role": "assistant", "content": last_a_output_raw},
                    {"role": "user", "content": last_b_feedback}
                ]

            # Call A with structural validation
            if producer_parser:
                try:
                    # Use think_with_retry to ensure structure is correct
                    parsed_result = await self.think_with_retry(
                        messages=a_messages,
                        parser=producer_parser,
                        max_retries=2
                    )
                    last_a_output_parsed = parsed_result

                    # Get raw output for B to see and for history
                    temp_response = await self.think(messages=a_messages)
                    last_a_output_raw = temp_response['reply']

                    self.logger.debug(f"ğŸ­ A output (validated): {str(parsed_result)[:200]}...")

                except Exception as e:
                    # Structural validation failed
                    self.logger.warning(f"A failed structural validation: {e}")
                    temp_response = await self.think(messages=a_messages)
                    last_a_output_raw = temp_response['reply']
                    last_a_output_parsed = last_a_output_raw
            else:
                # No structural validation
                a_response = await self.think(messages=a_messages)
                last_a_output_raw = a_response['reply']
                last_a_output_parsed = last_a_output_raw
                self.logger.debug(f"ğŸ­ A output: {last_a_output_raw[:200]}...")

            # ========== Phase 2: Verifier (B) evaluates ==========
            # Show B the formatted output if available, otherwise raw
            b_input = str(last_a_output_parsed) if producer_parser else last_a_output_raw

            b_task = verifier_task_template.format(producer_output=b_input)
            b_messages = [
                {"role": "system", "content": verifier_persona},
                {"role": "user", "content": b_task}
            ]

            b_response = await self.think(messages=b_messages)
            b_output = b_response['reply']
            self.logger.debug(f"ğŸ­ B output: {b_output[:200]}...")

            # ========== Phase 3: Check if B approves ==========
            if approver_parser:
                parser_result = approver_parser(b_output)

                if parser_result.get("status") == "success":
                    # B approves!
                    self.logger.info(f"âœ… Dialog approved at round {round_num}")
                    return {
                        "status": "success",
                        "content": last_a_output_parsed,  # Return parsed data
                        "rounds_used": round_num,
                        "max_rounds_exceeded": False
                    }
                else:
                    # B doesn't approve
                    last_b_feedback = parser_result.get(
                        "feedback",
                        f"{b_output}"
                    )
                    self.logger.info(f"âŒ Dialog feedback: {last_b_feedback[:200]}...")
            else:
                # No approver, single round mode
                self.logger.info(f"âœ… Dialog completed (no approver)")
                return {
                    "status": "success",
                    "content": last_a_output_parsed,
                    "rounds_used": round_num,
                    "max_rounds_exceeded": False
                }

        # Reached max_rounds without approval
        self.logger.warning(f"âš ï¸ Dialog reached max_rounds ({max_rounds}) without approval")
        return {
            "status": "success",
            "content": last_a_output_parsed,  # Return parsed data
            "rounds_used": max_rounds,
            "max_rounds_exceeded": True,
            "last_feedback": last_b_feedback
        }

    async def think(self, messages:  Union[str, List[Dict[str, str]]], **kwargs) -> Dict[str, str]:
        if isinstance(messages, str):
            #å¦‚æœmessages æ˜¯string,å°±åŒ…è£…æˆopen ai chat messages çš„æ ¼å¼
            messages =[{"role": "user", "content": messages}]
        if "googleapis.com" in self.url or "gemini" in self.model_name.lower():
            return await self._async_stream_think_gemini(messages, **kwargs)
        return await self.async_stream_think(messages, **kwargs)

    async def think_with_image(
        self,
        messages: Union[str, List[Dict[str, str]]],
        image: str,
        **kwargs
    ) -> str:
        """
        å¸¦å›¾ç‰‡çš„å¼‚æ­¥è°ƒç”¨å¤§æ¨¡å‹APIï¼ˆæ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼Œå¦‚ GPT-4V, Claude 3.5 Sonnet, Gemini Pro Visionï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰æˆ–å•ä¸ªå­—ç¬¦ä¸²
            image: base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®ï¼ˆä¸å« data:image/... å‰ç¼€ï¼‰
            **kwargs: é¢å¤–çš„å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

        Returns:
            str: LLM çš„æ–‡æœ¬å›å¤

        Raises:
            Exception: å¦‚æœ API è°ƒç”¨å¤±è´¥

        Example:
            result = await llm_client.think_with_image(
                messages="æè¿°è¿™å¼ å›¾ç‰‡",
                image="iVBORw0KGgoAAAANSUhEUgAAAAUA..."
            )
        """
        # ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼
        if isinstance(messages, str):
            # å•ä¸ªå­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            text_content = messages
            is_multi_turn = False
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šè½®å¯¹è¯ï¼ˆæœ‰å¤šæ¡æ¶ˆæ¯ï¼Œä¸”åŒ…å« assistant æ¶ˆæ¯ï¼‰
            has_assistant = any(msg.get("role") == "assistant" for msg in messages)
            is_multi_turn = has_assistant and len(messages) > 1

            if is_multi_turn:
                # å¤šè½®å¯¹è¯ï¼šç›´æ¥ä¼ é€’ç»™ _think_with_image_openai_multi_turn
                is_gemini = "googleapis.com" in self.url or "gemini" in self.model_name.lower()
                if is_gemini:
                    return await self._think_with_image_gemini_multi_turn(messages, image, **kwargs)
                else:
                    return await self._think_with_image_openai_multi_turn(messages, image, **kwargs)
            else:
                # å•è½®å¯¹è¯ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ multi-modal æ ¼å¼
                first_msg = messages[0]
                content = first_msg.get("content", "")

                if isinstance(content, list):
                    # å·²ç»æ˜¯ multi-modal æ ¼å¼ [{"type": "text", "text": "..."}]
                    # ç›´æ¥ä¼ é€’ç»™å¤šè½®æ–¹æ³•å¤„ç†
                    is_gemini = "googleapis.com" in self.url or "gemini" in self.model_name.lower()
                    if is_gemini:
                        return await self._think_with_image_gemini_multi_turn(messages, image, **kwargs)
                    else:
                        return await self._think_with_image_openai_multi_turn(messages, image, **kwargs)
                else:
                    # æ™®é€šå­—ç¬¦ä¸²æ ¼å¼ï¼Œåˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
                    text_content = "\n".join([
                        msg.get("content", "") for msg in messages
                    ])

        # æ£€æµ‹æ˜¯ Gemini è¿˜æ˜¯ OpenAI æ ¼å¼ï¼ˆå•è½®ï¼‰
        if not is_multi_turn:
            is_gemini = "googleapis.com" in self.url or "gemini" in self.model_name.lower()
            if is_gemini:
                return await self._think_with_image_gemini(text_content, image, **kwargs)
            else:
                return await self._think_with_image_openai(text_content, image, **kwargs)

    async def _think_with_image_openai(
        self,
        text_prompt: str,
        image_base64: str,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨ OpenAI Vision API æ ¼å¼è°ƒç”¨ï¼ˆå…¼å®¹ GPT-4V, Claude ç­‰ï¼‰
        """
        try:
            # æ„é€ æ”¯æŒå›¾ç‰‡çš„æ¶ˆæ¯æ ¼å¼
            message_content = [
                {
                    "type": "text",
                    "text": text_prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_base64}",
                        "detail": kwargs.get("detail", "high")  # low, high, auto
                    }
                }
            ]

            messages = [{"role": "user", "content": message_content}]

            # æ„é€ è¯·æ±‚æ•°æ®
            data = {
                "messages": messages,
                "model": self.model_name,
                "stream": True,
                **{k: v for k, v in kwargs.items() if k != "detail"}
            }

            final_content = ""
            buffer = ""

            timeout = aiohttp.ClientTimeout(total=120)

            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Vision API Error {resp.status}: {error_text}")

                    # æµå¼è§£æ
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")
                        buffer += text
                        lines = buffer.split("\n")
                        buffer = lines[-1]

                        for line in lines[:-1]:
                            line = line.strip()
                            if not line or not line.startswith("data: "):
                                continue

                            data_str = line[6:].strip()
                            if data_str == "[DONE]":
                                continue

                            try:
                                payload = json.loads(data_str)
                            except json.JSONDecodeError:
                                continue

                            if "choices" in payload and payload["choices"]:
                                delta = payload["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    final_content += content

            return final_content

        except aiohttp.ClientConnectorError as e:
            self.logger.exception(f"Vision API connection error: {str(e)}")
            raise LLMServiceConnectionError(f"Vision API connection failed: {str(e)}")
        except asyncio.TimeoutError as e:
            self.logger.exception(f"Vision API timeout")
            raise LLMServiceTimeoutError(f"Vision API timeout: {str(e)}")
        except aiohttp.ClientError as e:
            self.logger.exception(f"Vision API network error: {str(e)}")
            raise LLMServiceConnectionError(f"Vision API network error: {str(e)}")
        except Exception as e:
            self.logger.exception(f"Vision API è°ƒç”¨å¤±è´¥")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"Vision API service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Vision API è°ƒç”¨å¤±è´¥: {str(e)}")

    async def _think_with_image_openai_multi_turn(
        self,
        messages: List[Dict],
        image_base64: str,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨ OpenAI Vision API æ ¼å¼è¿›è¡Œå¤šè½®å¯¹è¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º:
                [
                    {"role": "user", "content": [{"type": "text", "text": "..."}]},
                    {"role": "assistant", "content": "..."},
                    {"role": "user", "content": [{"type": "text", "text": "..."}]}
                ]
            image_base64: Base64 ç¼–ç çš„å›¾ç‰‡ï¼ˆåªåœ¨ç¬¬ä¸€æ¡ user æ¶ˆæ¯ä¸­æ·»åŠ ï¼‰
            **kwargs: é¢å¤–å‚æ•°
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼šä¸ºç¬¬ä¸€æ¡ user æ¶ˆæ¯æ·»åŠ å›¾ç‰‡
            formatted_messages = []
            first_user_done = False

            for msg in messages:
                role = msg.get("role")
                content = msg.get("content")

                if role == "user":
                    if isinstance(content, list):
                        # å·²ç»æ˜¯ multi-modal æ ¼å¼
                        if not first_user_done:
                            # ç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼Œæ·»åŠ å›¾ç‰‡
                            content.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}",
                                    "detail": kwargs.get("detail", "high")
                                }
                            })
                            first_user_done = True
                        formatted_messages.append({"role": role, "content": content})
                    else:
                        # çº¯æ–‡æœ¬ï¼Œè½¬æ¢ä¸º multi-modal æ ¼å¼
                        if not first_user_done:
                            # ç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼Œæ·»åŠ å›¾ç‰‡
                            formatted_messages.append({
                                "role": role,
                                "content": [
                                    {"type": "text", "text": content},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{image_base64}",
                                            "detail": kwargs.get("detail", "high")
                                        }
                                    }
                                ]
                            })
                            first_user_done = True
                        else:
                            # åç»­ user æ¶ˆæ¯ï¼Œåªæœ‰æ–‡æœ¬
                            formatted_messages.append({
                                "role": role,
                                "content": [{"type": "text", "text": content}]
                            })
                else:
                    # assistant æˆ– system æ¶ˆæ¯ï¼Œç›´æ¥æ·»åŠ 
                    formatted_messages.append({"role": role, "content": content})

            # æ„é€ è¯·æ±‚æ•°æ®
            data = {
                "messages": formatted_messages,
                "model": self.model_name,
                "stream": True,
                **{k: v for k, v in kwargs.items() if k != "detail"}
            }

            final_content = ""
            buffer = ""
            timeout = aiohttp.ClientTimeout(total=120)
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Vision API Error {resp.status}: {error_text}")

                    # æµå¼è§£æ
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")
                        buffer += text
                        lines = buffer.split("\n")
                        buffer = lines[-1]
                        for line in lines[:-1]:
                            line = line.strip()
                            if not line or not line.startswith("data: "):
                                continue
                            data_str = line[6:].strip()
                            if data_str == "[DONE]":
                                continue
                            try:
                                payload = json.loads(data_str)
                            except json.JSONDecodeError:
                                continue
                            if "choices" in payload and payload["choices"]:
                                delta = payload["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    final_content += content

            return final_content

        except aiohttp.ClientConnectorError as e:
            self.logger.exception(f"Vision API multi-turn connection error: {str(e)}")
            raise LLMServiceConnectionError(f"Vision API multi-turn connection failed: {str(e)}")
        except asyncio.TimeoutError as e:
            self.logger.exception(f"Vision API multi-turn timeout")
            raise LLMServiceTimeoutError(f"Vision API multi-turn timeout: {str(e)}")
        except aiohttp.ClientError as e:
            self.logger.exception(f"Vision API multi-turn network error: {str(e)}")
            raise LLMServiceConnectionError(f"Vision API multi-turn network error: {str(e)}")
        except Exception as e:
            self.logger.exception(f"Vision API å¤šè½®å¯¹è¯è°ƒç”¨å¤±è´¥")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"Vision API multi-turn service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Vision API å¤šè½®å¯¹è¯è°ƒç”¨å¤±è´¥: {str(e)}")

    async def _think_with_image_gemini(
        self,
        text_prompt: str,
        image_base64: str,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨ Gemini Vision API æ ¼å¼è°ƒç”¨
        """
        try:
            # Gemini æ ¼å¼ï¼šparts æ•°ç»„ï¼ŒåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡
            content_parts = [
                {"text": text_prompt},
                {
                    "inline_data": {
                        "mime_type": "image/png",
                        "data": image_base64
                    }
                }
            ]

            data = {
                "contents": [
                    {
                        "role": "user",
                        "parts": content_parts
                    }
                ],
                "generationConfig": self._construct_gemini_config(**kwargs)
            }

            final_content = ""
            buffer = ""
            brace_count = 0
            in_string = False
            escape = False

            timeout = aiohttp.ClientTimeout(total=120)

            async with aiohttp.ClientSession(headers=self.gemini_headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Gemini Vision Error {resp.status}: {error_text}")

                    # Gemini æµå¼è§£æï¼ˆJSON Array Streamï¼‰
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")

                        for char in text:
                            # ç®€æ˜“ JSON å¯¹è±¡æå–å™¨
                            if char == '[' and brace_count == 0:
                                continue
                            if char == ']' and brace_count == 0:
                                continue
                            if char == ',' and brace_count == 0:
                                continue

                            buffer += char

                            if char == '"' and not escape:
                                in_string = not in_string
                            if char == '\\' and not escape:
                                escape = True
                            else:
                                escape = False

                            if not in_string:
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1

                                if brace_count == 0 and buffer.strip():
                                    try:
                                        obj = json.loads(buffer)
                                        candidates = obj.get("candidates", [])
                                        if candidates:
                                            content_obj = candidates[0].get("content", {})
                                            parts = content_obj.get("parts", [])

                                            for part in parts:
                                                part_text = part.get("text", "")
                                                final_content += part_text

                                    except json.JSONDecodeError:
                                        pass
                                    finally:
                                        buffer = ""

            return final_content

        except aiohttp.ClientConnectorError as e:
            self.logger.exception(f"Gemini Vision API connection error: {str(e)}")
            raise LLMServiceConnectionError(f"Gemini Vision API connection failed: {str(e)}")
        except asyncio.TimeoutError as e:
            self.logger.exception(f"Gemini Vision API timeout")
            raise LLMServiceTimeoutError(f"Gemini Vision API timeout: {str(e)}")
        except aiohttp.ClientError as e:
            self.logger.exception(f"Gemini Vision API network error: {str(e)}")
            raise LLMServiceConnectionError(f"Gemini Vision API network error: {str(e)}")
        except Exception as e:
            self.logger.exception(f"Gemini Vision API è°ƒç”¨å¤±è´¥")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"Gemini Vision API service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Gemini Vision API è°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _to_gemini_messages(self, messages: list[dict[str, str]]) -> dict:
        """
        OpenAI æ ¼å¼ -> Gemini æ ¼å¼è½¬æ¢
        """
        gemini_contents = []
        system_instruction = None

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")
            
            if role == "system":
                # Gemini system instruction æ˜¯é¡¶å±‚å­—æ®µ
                system_instruction = {"parts": [{"text": content}]}
            elif role == "user":
                gemini_contents.append({"role": "user", "parts": [{"text": content}]})
            elif role == "assistant":
                gemini_contents.append({"role": "model", "parts": [{"text": content}]})
        
        return {
            "contents": gemini_contents,
            "systemInstruction": system_instruction
        }

    def _construct_gemini_config(self, **kwargs) -> dict:
        """
        æ„å»ºç¬¦åˆå®˜æ–¹è§„èŒƒçš„ generationConfigï¼Œå¤„ç† thinkingConfig çš„åµŒå¥—
        """
        config = {}
        
        # æå– Thinking ç›¸å…³çš„å‚æ•°å¹¶å°è£…
        thinking_config = {}
        if "thinking_level" in kwargs:
            thinking_config["thinkingLevel"] = kwargs.pop("thinking_level")
        if "include_thoughts" in kwargs:
            thinking_config["includeThoughts"] = kwargs.pop("include_thoughts")
            
        # å…¶ä»–å¸¸è§å‚æ•°æ˜ å°„ (OpenAIå‘½å -> Geminiå‘½å)
        if "max_tokens" in kwargs:
            config["maxOutputTokens"] = kwargs.pop("max_tokens")
        if "temperature" in kwargs:
            config["temperature"] = kwargs.pop("temperature")
        if "top_p" in kwargs:
            config["topP"] = kwargs.pop("top_p")
            
        # å°†å‰©ä½™çš„ kwargs ä¹Ÿæ”¾å…¥ config
        config.update(kwargs)
        
        # å¦‚æœæœ‰ thinking é…ç½®ï¼ŒæŒ‰ç…§å®˜æ–¹æ ¼å¼åµŒå¥—
        if thinking_config:
            config["thinkingConfig"] = thinking_config
            
        return config

    async def _async_stream_think_gemini(self, messages: list[dict[str, str]], **kwargs) -> Dict[str, str]:
        """
        Gemini ä¸“ç”¨å¼‚æ­¥æµå¼æ–¹æ³•
        """
        try:
            # 1. æ¶ˆæ¯æ ¼å¼è½¬æ¢
            payload_parts = self._to_gemini_messages(messages)
            
            # 2. æ„å»º Request Body (åŒ¹é…å®˜æ–¹ç»“æ„)
            generation_config = self._construct_gemini_config(**kwargs)
            
            data = {
                "contents": payload_parts["contents"],
                "generationConfig": generation_config
            }
            
            if payload_parts["systemInstruction"]:
                data["systemInstruction"] = payload_parts["systemInstruction"]

            # 3. å¤„ç† Tools (å¦‚æœ kwargs é‡Œä¼ äº† toolsï¼ŒæŒ‰ç…§å®˜æ–¹ç»“æ„æ”¾å…¥é¡¶å±‚)
            # æ³¨æ„ï¼šè¿™é‡Œçš„å®ç°å‡è®¾ kwargs é‡Œçš„ 'tools' å·²ç»æ˜¯ Gemini æ ¼å¼ï¼Œæˆ–è€…ä½ å¯ä»¥åŠ è½¬æ¢é€»è¾‘
            if "tools" in kwargs:
                data["tools"] = kwargs.pop("tools")

            final_content = ""
            final_reasoning = ""

            timeout = aiohttp.ClientTimeout(total=120)
            
            async with aiohttp.ClientSession(headers=self.gemini_headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Gemini Error {resp.status}: {error_text}")
                    
                    # Gemini æµå¼è§£æ (JSON Array Stream)
                    buffer = ""
                    brace_count = 0
                    in_string = False
                    escape = False
                    
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk: continue
                        text = chunk.decode("utf-8", errors="ignore")
                        
                        for char in text:
                            # ç®€æ˜“ JSON å¯¹è±¡æå–å™¨
                            if char == '[' and brace_count == 0: continue
                            if char == ']' and brace_count == 0: continue
                            if char == ',' and brace_count == 0: continue
                            
                            buffer += char
                            
                            if char == '"' and not escape: in_string = not in_string
                            if char == '\\' and not escape: escape = True
                            else: escape = False
                            
                            if not in_string:
                                if char == '{': brace_count += 1
                                elif char == '}': brace_count -= 1
                                    
                                if brace_count == 0 and buffer.strip():
                                    try:
                                        obj = json.loads(buffer)
                                        # è§£æ candidates
                                        candidates = obj.get("candidates", [])
                                        if candidates:
                                            content_obj = candidates[0].get("content", {})
                                            parts = content_obj.get("parts", [])
                                            
                                            # éå† parts (Gemini å¯èƒ½åœ¨ä¸€ä¸ª chunk è¿”å›å¤šä¸ª part)
                                            for part in parts:
                                                part_text = part.get("text", "")
                                                
                                                # å°è¯•è¯†åˆ« Reasoning/Thought
                                                # ç›®å‰ Gemini API å°šæœªç»Ÿä¸€ "thought" å­—æ®µï¼Œ
                                                # ä½†å¦‚æœå®˜æ–¹å°†æ¥åœ¨ part é‡ŒåŠ äº† "thought": trueï¼Œå¯ä»¥åœ¨è¿™é‡Œæ•è·
                                                is_thought = part.get("thought", False) 
                                                
                                                if is_thought:
                                                    final_reasoning += part_text
                                                else:
                                                    final_content += part_text

                                    except json.JSONDecodeError:
                                        pass
                                    finally:
                                        buffer = ""

            return {
                "reasoning": final_reasoning,
                "reply": final_content
            }

        except aiohttp.ClientConnectorError as e:
            # Gemini è¿æ¥é”™è¯¯
            self.logger.error(f"Gemini connection error: {str(e)}")
            raise LLMServiceConnectionError(
                f"Failed to connect to Gemini service: {str(e)}"
            )
        except asyncio.TimeoutError as e:
            # Gemini è¶…æ—¶
            self.logger.error(f"Gemini request timeout")
            raise LLMServiceTimeoutError(
                f"Gemini request timeout: {str(e)}"
            )
        except aiohttp.ClientError as e:
            # å…¶ä»– Gemini ç½‘ç»œé”™è¯¯
            self.logger.error(f"Gemini client error: {str(e)}")
            raise LLMServiceConnectionError(
                f"Gemini network error: {str(e)}"
            )
        except Exception as e:
            self.logger.exception("Geminiè°ƒç”¨å¤±è´¥")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"Gemini service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Geminiè°ƒç”¨å¤±è´¥: {str(e)}")

    async def _think_with_image_gemini_multi_turn(
        self,
        messages: List[Dict],
        image_base64: str,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨ Gemini Vision API æ ¼å¼è¿›è¡Œå¤šè½®å¯¹è¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º:
                [
                    {"role": "user", "content": [{"type": "text", "text": "..."}]},
                    {"role": "assistant", "content": "..."},
                    {"role": "user", "content": [{"type": "text", "text": "..."}]}
                ]
            image_base64: Base64 ç¼–ç çš„å›¾ç‰‡ï¼ˆåªåœ¨ç¬¬ä¸€æ¡ user æ¶ˆæ¯ä¸­æ·»åŠ ï¼‰
            **kwargs: é¢å¤–å‚æ•°
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼šä¸ºç¬¬ä¸€æ¡ user æ¶ˆæ¯æ·»åŠ å›¾ç‰‡
            contents = []
            first_user_done = False

            for msg in messages:
                role = msg.get("role")
                content = msg.get("content")

                # Gemini çš„ role æ˜ å°„: user -> user, assistant -> model
                gemini_role = "model" if role == "assistant" else role

                if role == "user":
                    parts = []
                    if isinstance(content, list):
                        # å·²ç»æ˜¯ multi-modal æ ¼å¼
                        if not first_user_done:
                            # ç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼Œæ·»åŠ å›¾ç‰‡
                            parts.extend(content)  # å¤åˆ¶æ‰€æœ‰ partsï¼ˆtextï¼‰
                            parts.append({
                                "inline_data": {
                                    "mime_type": "image/png",
                                    "data": image_base64
                                }
                            })
                            first_user_done = True
                        else:
                            # åç»­ user æ¶ˆæ¯ï¼Œåªæœ‰æ–‡æœ¬
                            parts.extend(content)
                    else:
                        # çº¯æ–‡æœ¬
                        parts.append({"text": content})
                        if not first_user_done:
                            # ç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼Œæ·»åŠ å›¾ç‰‡
                            parts.append({
                                "inline_data": {
                                    "mime_type": "image/png",
                                    "data": image_base64
                                }
                            })
                            first_user_done = True

                    contents.append({
                        "role": gemini_role,
                        "parts": parts
                    })
                else:
                    # assistant (model) æ¶ˆæ¯
                    contents.append({
                        "role": gemini_role,
                        "parts": [{"text": content}]
                    })

            data = {
                "contents": contents,
                "generationConfig": self._construct_gemini_config(**kwargs)
            }

            final_content = ""
            final_reasoning = ""
            buffer = ""
            brace_count = 0
            in_string = False
            escape = False
            timeout = aiohttp.ClientTimeout(total=120)
            async with aiohttp.ClientSession(headers=self.gemini_headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Gemini Vision Error {resp.status}: {error_text}")

                    # Gemini æµå¼è§£æï¼ˆJSON Array Streamï¼‰
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")
                        for char in text:
                            # ç®€æ˜“ JSON å¯¹è±¡æå–å™¨
                            if char == '[' and brace_count == 0:
                                continue
                            if char == ']' and brace_count == 0:
                                continue
                            if char == ',' and brace_count == 0:
                                continue
                            buffer += char

                            if char == '\\' and not escape:
                                escape = True
                                continue

                            if char == '"' and not escape:
                                in_string = not in_string
                            else:
                                escape = False

                            if not in_string:
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        try:
                                            obj = json.loads(buffer)
                                            buffer = ""

                                            candidate = obj.get("candidates", [{}])[0]
                                            content = candidate.get("content", {})
                                            parts = content.get("parts", [])

                                            for part in parts:
                                                if "text" in part:
                                                    final_content += part["text"]
                                                elif "thought" in part:
                                                    final_reasoning += part["thought"]

                                        except json.JSONDecodeError:
                                            pass

            return final_content

        except aiohttp.ClientConnectorError as e:
            self.logger.exception(f"Gemini Vision multi-turn connection error: {str(e)}")
            raise LLMServiceConnectionError(f"Gemini Vision multi-turn connection failed: {str(e)}")
        except asyncio.TimeoutError as e:
            self.logger.exception(f"Gemini Vision multi-turn timeout")
            raise LLMServiceTimeoutError(f"Gemini Vision multi-turn timeout: {str(e)}")
        except aiohttp.ClientError as e:
            self.logger.exception(f"Gemini Vision multi-turn network error: {str(e)}")
            raise LLMServiceConnectionError(f"Gemini Vision multi-turn network error: {str(e)}")
        except Exception as e:
            self.logger.exception(f"Gemini Vision å¤šè½®å¯¹è¯è°ƒç”¨å¤±è´¥")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"Gemini Vision multi-turn service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Gemini Vision å¤šè½®å¯¹è¯è°ƒç”¨å¤±è´¥: {str(e)}")

    async def async_stream_think(self, messages: list[dict[str, str]], **kwargs) -> Dict[str, str]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨å¤§æ¨¡å‹APIï¼Œå®æ—¶æ‰“å°å“åº”å†…å®¹ï¼ˆä½¿ç”¨ aiohttpï¼‰
        """
        
        try:
            data = {
                "messages": messages,
                "model": self.model_name,
                "stream": True,
                **kwargs
            }

            final_reasoning_content = ""
            final_content = ""

            buffer = ""

            # è¶…æ—¶é…ç½®ï¼š
            # - total=None: ä¸é™åˆ¶æ€»æ—¶é—´ï¼ŒLLM è¾“å‡ºå¤šä¹…éƒ½å¯ä»¥ï¼ˆåªè¦åœ¨æŒç»­è¾“å‡ºï¼‰
            # - sock_read=300: å•æ¬¡è¯»å–è¶…æ—¶ 5 åˆ†é’Ÿï¼Œå¦‚æœ 5 åˆ†é’Ÿå†…æ²¡æœ‰æ”¶åˆ°ä»»ä½•æ•°æ® chunkï¼Œæ‰è®¤ä¸ºè¶…æ—¶
            timeout = aiohttp.ClientTimeout(
                total=None,      # ä¸é™åˆ¶æ€»æ—¶é•¿ï¼Œå…è®¸ LLM é•¿æ—¶é—´è¾“å‡º
                sock_read=300    # å•æ¬¡è¯»å–è¶…æ—¶ 5 åˆ†é’Ÿï¼ˆå®½å®¹çš„ç½‘ç»œå®¹é”™ï¼‰
            )
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {resp.status}, message='{error_text}', url='{self.url}'")
                    resp.raise_for_status()
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")
                        buffer += text
                        lines = buffer.split("\n")
                        buffer = lines[-1]  # ä¸å®Œæ•´è¡Œä¿ç•™åœ¨ buffer
                        for line in lines[:-1]:
                            line = line.strip()
                            if not line:
                                continue
                            if line.startswith("data: "):
                                data_str = line[6:].strip()
                                if data_str == "[DONE]":
                                    continue
                                try:
                                    payload = json.loads(data_str)
                                except json.JSONDecodeError:
                                    continue

                                if "choices" in payload and payload["choices"]:
                                    delta = payload["choices"][0].get("delta", {})
                                    reasoning_content = delta.get("reasoning_content", "")
                                    content = delta.get("content", "")

                                    if reasoning_content:
                                        final_reasoning_content += reasoning_content

                                    if content:
                                        final_content += content

            #print()  # ç¡®ä¿æ¢è¡Œ
            return {
                "reasoning": final_reasoning_content,
                "reply": final_content
            }

        except aiohttp.ClientConnectorError as e:
            # è¿æ¥é”™è¯¯ï¼ˆDNS å¤±è´¥ã€è¿æ¥è¢«æ‹’ç»ç­‰ï¼‰
            self.logger.error(f"LLM connection error: {str(e)}")
            raise LLMServiceConnectionError(
                f"Failed to connect to LLM service: {str(e)}"
            )
        except asyncio.TimeoutError as e:
            # è¶…æ—¶é”™è¯¯
            self.logger.error(f"LLM request timeout")
            raise LLMServiceTimeoutError(
                f"LLM request timeout: {str(e)}"
            )
        except aiohttp.ClientError as e:
            # å…¶ä»– aiohttp é”™è¯¯
            self.logger.error(f"LLM client error: {str(e)}")
            raise LLMServiceConnectionError(
                f"LLM network error: {str(e)}"
            )
        except Exception as e:
            # å…¶ä»–æœªçŸ¥é”™è¯¯
            traceback.print_exc()
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœåŠ¡ä¸å¯ç”¨ç›¸å…³çš„é”™è¯¯
            error_msg = str(e).lower()
            if any(code in error_msg for code in ['502', '503', '504']):
                raise LLMServiceAPIError(
                    f"LLM service unavailable: {str(e)}",
                    status_code=int(error_msg.split()[-1]) if error_msg.split()[-1].isdigit() else None
                )
            raise Exception(f"Unknown error: {str(e)}")


