import json
import traceback
from typing import Dict, Union, List, Optional
import aiohttp
from ..core.log_util import AutoLoggerMixin
import logging

class LLMClient(AutoLoggerMixin):

    _custom_log_level = logging.DEBUG
    def __init__(self, url: str, api_key: str,model_name: str):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            url (str): å¤§æ¨¡å‹APIçš„URL
            api_key (str): APIå¯†é’¥
        """
        self.url = url
        self.api_key = api_key
        self.model_name = model_name
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.gemini_headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": self.api_key
        }

    # In AdvancedMarkdownEditingMixin class

    async def think_with_retry(self,
                                    initial_messages: Union[str, List[str]],
                                    parser: callable,
                                    max_retries: int = 3,
                                    debug: bool = True,
                                    **parser_kwargs) -> any:
        """
        A generic micro-agent that interacts with an LLM in a loop until the
        output is successfully parsed.

        Args:
            initial_messages (list): The starting list of messages for the conversation.
            parser (callable): A function that takes a raw LLM reply string and
                            returns a dict following the Parser Contract.
            max_retries (int): The maximum number of attempts before failing.
            debug (bool): If True, output detailed debug information including LLM input/output.

        Returns:
            The "data" field from the successful parser result.

        Raises:
            ValueError: If the LLM fails to produce a parsable response after all retries.
        """

        if isinstance(initial_messages, str):
            #å¦‚æœmessages æ˜¯string,å°±åŒ…è£…æˆopen ai chat messages çš„æ ¼å¼
            messages =[{"role": "user", "content": initial_messages}]
        else:
            messages = initial_messages

        if debug:
            self.logger.debug(f"=== think_with_retry DEBUG START ===")
            self.logger.debug(f"Initial messages ({len(messages)} messages):")
            for i, msg in enumerate(messages):
                self.logger.debug(f"  [{i}] {msg.get('role')}: {msg.get('content')[:200]}{'...' if len(msg.get('content', '')) > 200 else ''}")
        
        for attempt in range(max_retries):
            try:
                response = await self.think(messages=messages)
                raw_reply = response['reply']

                if debug:
                    self.logger.debug(f"\nLLM Response (raw_reply):")
                    self.logger.debug(f"  {raw_reply[:500]}...")
                    

                # Delegate parsing to the provided parser function
                parsed_result = parser(raw_reply, **parser_kwargs)

                if debug:
                    self.logger.debug(f"\nParser result:")
                    self.logger.debug(f"  {parsed_result}")
                    

                if parsed_result.get("status") == "success":
                    
                    # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼š{"status": "success", "content": ...}
                    if "content" in parsed_result:
                        return parsed_result["content"]
                    else:
                        # æ²¡æœ‰å†…å®¹å­—æ®µï¼Œè¿”å›ç©ºå­—å…¸
                        return {}

                elif parsed_result.get("status") == "error":
                    feedback = parsed_result.get("feedback", "Your previous response was invalid. Please try again.")
                    # Append the failed response and the corrective feedback for the next attempt
                    messages.append({"role": "assistant", "content": raw_reply})
                    messages.append({"role": "user", "content": feedback})

                    if attempt == max_retries - 1:
                        # Final attempt failed
                        raise ValueError("LLM failed to produce a valid response after all retries.")

                else:
                    # The parser itself is faulty
                    raise TypeError("Parser function returned an invalid contract response.")

            except Exception as e:
                self.logger.exception(f"Micro-Agent: An unexpected error occurred during invocation attempt {attempt + 1}.")
                raise
                
                
        # This line should theoretically be unreachable
        raise RuntimeError("Micro-Agent loop exited unexpectedly.")

    async def dialog_with_retry(
        self,
        producer_task: str,
        producer_persona: str,
        verifier_task_template: str,
        verifier_persona: str,
        producer_parser: Optional[callable] = None,
        approver_parser: Optional[callable] = None,
        max_rounds: int = 3
    ) -> dict:
        """
        Dialog-based retry with two-layer validation (structure + semantics).

        Layer 1 - Structural validation (producer_parser):
        - Ensures A's output format is correct (e.g., has required sections)
        - Validated by code rules (parser function)
        - B only sees structurally correct outputs

        Layer 2 - Semantic validation (approver_parser):
        - Ensures A's output quality is good enough
        - Validated by LLM intelligence (Verifier B)
        - Deep evaluation and improvement

        Args:
            producer_task: A's initial task
            producer_persona: A's persona
            verifier_task_template: B's evaluation task template with {producer_output}
            verifier_persona: B's persona
            producer_parser: Optional parser to validate A's output structure.
                            If provided, A uses think_with_retry internally.
            approver_parser: Optional parser to check if B approves.
                            Returns {"status": "success"} if approved.
            max_rounds: Maximum dialog rounds

        Returns:
            {
                "status": "success",
                "content": Parsed data from A (if producer_parser) or raw text,
                "rounds_used": int,
                "max_rounds_exceeded": bool,
                "last_feedback": str (only if exceeded)
            }

        Example:
            result = await llm_client.dialog_with_retry(
                producer_task="Write a research plan",
                producer_persona="You are a researcher",
                verifier_task_template="Review:\\n{producer_output}",
                verifier_persona="You are a director",
                producer_parser=research_plan_parser,  # Validates structure
                approver_parser=director_approval_parser,  # Validates approval
                max_rounds=3
            )
            # result["content"] is already parsed: {"[ç ”ç©¶è®¡åˆ’]": "...", ...}
        """
        last_a_output_raw = None
        last_a_output_parsed = None
        last_b_feedback = None

        self.logger.info(f"ğŸ­ Dialog-With-Retry: Starting (max {max_rounds} rounds)")

        for round_num in range(1, max_rounds + 1):
            self.logger.info(f"ğŸ­ Round {round_num}:")

            # ========== Phase 1: Producer (A) generates output ==========
            if round_num == 1:
                a_messages = [{"role": "user", "content": producer_task}]
            else:
                a_messages = [
                    {"role": "user", "content": producer_task},
                    {"role": "assistant", "content": last_a_output_raw},
                    {"role": "user", "content": last_b_feedback}
                ]

            # Call A with structural validation
            if producer_parser:
                try:
                    # Use think_with_retry to ensure structure is correct
                    parsed_result = await self.think_with_retry(
                        messages=a_messages,
                        parser=producer_parser,
                        max_retries=2
                    )
                    last_a_output_parsed = parsed_result

                    # Get raw output for B to see and for history
                    temp_response = await self.think(messages=a_messages)
                    last_a_output_raw = temp_response['reply']

                    self.logger.debug(f"ğŸ­ A output (validated): {str(parsed_result)[:200]}...")

                except Exception as e:
                    # Structural validation failed
                    self.logger.warning(f"A failed structural validation: {e}")
                    temp_response = await self.think(messages=a_messages)
                    last_a_output_raw = temp_response['reply']
                    last_a_output_parsed = last_a_output_raw
            else:
                # No structural validation
                a_response = await self.think(messages=a_messages)
                last_a_output_raw = a_response['reply']
                last_a_output_parsed = last_a_output_raw
                self.logger.debug(f"ğŸ­ A output: {last_a_output_raw[:200]}...")

            # ========== Phase 2: Verifier (B) evaluates ==========
            # Show B the formatted output if available, otherwise raw
            b_input = str(last_a_output_parsed) if producer_parser else last_a_output_raw

            b_task = verifier_task_template.format(producer_output=b_input)
            b_messages = [
                {"role": "system", "content": verifier_persona},
                {"role": "user", "content": b_task}
            ]

            b_response = await self.think(messages=b_messages)
            b_output = b_response['reply']
            self.logger.debug(f"ğŸ­ B output: {b_output[:200]}...")

            # ========== Phase 3: Check if B approves ==========
            if approver_parser:
                parser_result = approver_parser(b_output)

                if parser_result.get("status") == "success":
                    # B approves!
                    self.logger.info(f"âœ… Dialog approved at round {round_num}")
                    return {
                        "status": "success",
                        "content": last_a_output_parsed,  # Return parsed data
                        "rounds_used": round_num,
                        "max_rounds_exceeded": False
                    }
                else:
                    # B doesn't approve
                    last_b_feedback = parser_result.get(
                        "feedback",
                        f"{b_output}"
                    )
                    self.logger.info(f"âŒ Dialog feedback: {last_b_feedback[:200]}...")
            else:
                # No approver, single round mode
                self.logger.info(f"âœ… Dialog completed (no approver)")
                return {
                    "status": "success",
                    "content": last_a_output_parsed,
                    "rounds_used": round_num,
                    "max_rounds_exceeded": False
                }

        # Reached max_rounds without approval
        self.logger.warning(f"âš ï¸ Dialog reached max_rounds ({max_rounds}) without approval")
        return {
            "status": "success",
            "content": last_a_output_parsed,  # Return parsed data
            "rounds_used": max_rounds,
            "max_rounds_exceeded": True,
            "last_feedback": last_b_feedback
        }

    async def think(self, messages:  Union[str, List[Dict[str, str]]], **kwargs) -> Dict[str, str]:
        if isinstance(messages, str):
            #å¦‚æœmessages æ˜¯string,å°±åŒ…è£…æˆopen ai chat messages çš„æ ¼å¼
            messages =[{"role": "user", "content": messages}]
        if "googleapis.com" in self.url or "gemini" in self.model_name.lower():
            return await self._async_stream_think_gemini(messages, **kwargs)
        return await self.async_stream_think(messages, **kwargs)
    
    def _to_gemini_messages(self, messages: list[dict[str, str]]) -> dict:
        """
        OpenAI æ ¼å¼ -> Gemini æ ¼å¼è½¬æ¢
        """
        gemini_contents = []
        system_instruction = None

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")
            
            if role == "system":
                # Gemini system instruction æ˜¯é¡¶å±‚å­—æ®µ
                system_instruction = {"parts": [{"text": content}]}
            elif role == "user":
                gemini_contents.append({"role": "user", "parts": [{"text": content}]})
            elif role == "assistant":
                gemini_contents.append({"role": "model", "parts": [{"text": content}]})
        
        return {
            "contents": gemini_contents,
            "systemInstruction": system_instruction
        }

    def _construct_gemini_config(self, **kwargs) -> dict:
        """
        æ„å»ºç¬¦åˆå®˜æ–¹è§„èŒƒçš„ generationConfigï¼Œå¤„ç† thinkingConfig çš„åµŒå¥—
        """
        config = {}
        
        # æå– Thinking ç›¸å…³çš„å‚æ•°å¹¶å°è£…
        thinking_config = {}
        if "thinking_level" in kwargs:
            thinking_config["thinkingLevel"] = kwargs.pop("thinking_level")
        if "include_thoughts" in kwargs:
            thinking_config["includeThoughts"] = kwargs.pop("include_thoughts")
            
        # å…¶ä»–å¸¸è§å‚æ•°æ˜ å°„ (OpenAIå‘½å -> Geminiå‘½å)
        if "max_tokens" in kwargs:
            config["maxOutputTokens"] = kwargs.pop("max_tokens")
        if "temperature" in kwargs:
            config["temperature"] = kwargs.pop("temperature")
        if "top_p" in kwargs:
            config["topP"] = kwargs.pop("top_p")
            
        # å°†å‰©ä½™çš„ kwargs ä¹Ÿæ”¾å…¥ config
        config.update(kwargs)
        
        # å¦‚æœæœ‰ thinking é…ç½®ï¼ŒæŒ‰ç…§å®˜æ–¹æ ¼å¼åµŒå¥—
        if thinking_config:
            config["thinkingConfig"] = thinking_config
            
        return config

    async def _async_stream_think_gemini(self, messages: list[dict[str, str]], **kwargs) -> Dict[str, str]:
        """
        Gemini ä¸“ç”¨å¼‚æ­¥æµå¼æ–¹æ³•
        """
        try:
            # 1. æ¶ˆæ¯æ ¼å¼è½¬æ¢
            payload_parts = self._to_gemini_messages(messages)
            
            # 2. æ„å»º Request Body (åŒ¹é…å®˜æ–¹ç»“æ„)
            generation_config = self._construct_gemini_config(**kwargs)
            
            data = {
                "contents": payload_parts["contents"],
                "generationConfig": generation_config
            }
            
            if payload_parts["systemInstruction"]:
                data["systemInstruction"] = payload_parts["systemInstruction"]

            # 3. å¤„ç† Tools (å¦‚æœ kwargs é‡Œä¼ äº† toolsï¼ŒæŒ‰ç…§å®˜æ–¹ç»“æ„æ”¾å…¥é¡¶å±‚)
            # æ³¨æ„ï¼šè¿™é‡Œçš„å®ç°å‡è®¾ kwargs é‡Œçš„ 'tools' å·²ç»æ˜¯ Gemini æ ¼å¼ï¼Œæˆ–è€…ä½ å¯ä»¥åŠ è½¬æ¢é€»è¾‘
            if "tools" in kwargs:
                data["tools"] = kwargs.pop("tools")

            final_content = ""
            final_reasoning = ""

            timeout = aiohttp.ClientTimeout(total=120)
            
            async with aiohttp.ClientSession(headers=self.gemini_headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"Gemini Error {resp.status}: {error_text}")
                    
                    # Gemini æµå¼è§£æ (JSON Array Stream)
                    buffer = ""
                    brace_count = 0
                    in_string = False
                    escape = False
                    
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk: continue
                        text = chunk.decode("utf-8", errors="ignore")
                        
                        for char in text:
                            # ç®€æ˜“ JSON å¯¹è±¡æå–å™¨
                            if char == '[' and brace_count == 0: continue
                            if char == ']' and brace_count == 0: continue
                            if char == ',' and brace_count == 0: continue
                            
                            buffer += char
                            
                            if char == '"' and not escape: in_string = not in_string
                            if char == '\\' and not escape: escape = True
                            else: escape = False
                            
                            if not in_string:
                                if char == '{': brace_count += 1
                                elif char == '}': brace_count -= 1
                                    
                                if brace_count == 0 and buffer.strip():
                                    try:
                                        obj = json.loads(buffer)
                                        # è§£æ candidates
                                        candidates = obj.get("candidates", [])
                                        if candidates:
                                            content_obj = candidates[0].get("content", {})
                                            parts = content_obj.get("parts", [])
                                            
                                            # éå† parts (Gemini å¯èƒ½åœ¨ä¸€ä¸ª chunk è¿”å›å¤šä¸ª part)
                                            for part in parts:
                                                part_text = part.get("text", "")
                                                
                                                # å°è¯•è¯†åˆ« Reasoning/Thought
                                                # ç›®å‰ Gemini API å°šæœªç»Ÿä¸€ "thought" å­—æ®µï¼Œ
                                                # ä½†å¦‚æœå®˜æ–¹å°†æ¥åœ¨ part é‡ŒåŠ äº† "thought": trueï¼Œå¯ä»¥åœ¨è¿™é‡Œæ•è·
                                                is_thought = part.get("thought", False) 
                                                
                                                if is_thought:
                                                    final_reasoning += part_text
                                                else:
                                                    final_content += part_text

                                    except json.JSONDecodeError:
                                        pass
                                    finally:
                                        buffer = ""

            return {
                "reasoning": final_reasoning,
                "reply": final_content
            }

        except Exception as e:
            self.logger.exception("Geminiè°ƒç”¨å¤±è´¥")
            raise Exception(f"Geminiè°ƒç”¨å¤±è´¥: {str(e)}")

    async def async_stream_think(self, messages: list[dict[str, str]], **kwargs) -> Dict[str, str]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨å¤§æ¨¡å‹APIï¼Œå®æ—¶æ‰“å°å“åº”å†…å®¹ï¼ˆä½¿ç”¨ aiohttpï¼‰
        """
        
        try:
            data = {
                "messages": messages,
                "model": self.model_name,
                "stream": True,
                **kwargs
            }

            final_reasoning_content = ""
            final_content = ""
            
            buffer = ""

            timeout = aiohttp.ClientTimeout(total=120)
            async with aiohttp.ClientSession(headers=self.headers, timeout=timeout, trust_env=True) as session:
                async with session.post(self.url, json=data) as resp:
                    if resp.status != 200:
                        error_text = await resp.text()
                        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {resp.status}, message='{error_text}', url='{self.url}'")
                    resp.raise_for_status()
                    async for chunk in resp.content.iter_chunked(1024):
                        if not chunk:
                            continue
                        text = chunk.decode("utf-8", errors="ignore")
                        buffer += text
                        lines = buffer.split("\n")
                        buffer = lines[-1]  # ä¸å®Œæ•´è¡Œä¿ç•™åœ¨ buffer
                        for line in lines[:-1]:
                            line = line.strip()
                            if not line:
                                continue
                            if line.startswith("data: "):
                                data_str = line[6:].strip()
                                if data_str == "[DONE]":
                                    continue
                                try:
                                    payload = json.loads(data_str)
                                except json.JSONDecodeError:
                                    continue

                                if "choices" in payload and payload["choices"]:
                                    delta = payload["choices"][0].get("delta", {})
                                    reasoning_content = delta.get("reasoning_content", "")
                                    content = delta.get("content", "")

                                    if reasoning_content:
                                        final_reasoning_content += reasoning_content

                                    if content:
                                        final_content += content

            #print()  # ç¡®ä¿æ¢è¡Œ
            return {
                "reasoning": final_reasoning_content,
                "reply": final_content
            }

        except aiohttp.ClientError as e:
            traceback.print_exc()
            raise Exception(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        except Exception as e:
            traceback.print_exc()
            raise Exception(f"æœªçŸ¥é”™è¯¯: {str(e)}")


