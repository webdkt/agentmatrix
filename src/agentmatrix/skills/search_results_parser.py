"""
æœç´¢ç»“æœé¡µè§£æå™¨

ä¸“é—¨ç”¨äºè§£æ Google/Bing æœç´¢ç»“æœé¡µçš„ HTMLï¼Œæå–ç»“æ„åŒ–çš„æœç´¢ç»“æœæ•°æ®ã€‚
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import re
import base64
from urllib.parse import urlparse, unquote


@dataclass
class SearchResultItem:
    """å•ä¸ªæœç´¢ç»“æœ"""
    title: str
    url: str
    snippet: str
    site_info: str = ""  # å¦‚ "www.example.com - 2 days ago"
    link_id: str = ""  # å¦‚ "Link1 To: www.example.com"


def decode_bing_redirect_url(bing_url: str) -> Optional[str]:
    """
    è§£æ Bing é‡å®šå‘ URLï¼Œæå–çœŸå® URL

    Bing é‡å®šå‘ URL æ ¼å¼ï¼š
    https://www.bing.com/ck/a?!&&p=...&u=a1aHR0cHM6Ly93d3cud2Vmb3J1bS5vcmcv...&ntb=1

    å…¶ä¸­ u å‚æ•°çš„å€¼æ˜¯çœŸå® URL çš„ base64 ç¼–ç ï¼ˆå»æ‰å‰ç¼€ "a1"ï¼‰

    Args:
        bing_url: Bing çš„é‡å®šå‘ URL

    Returns:
        çœŸå® URLï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å› None
    """
    try:
        # æå– u å‚æ•°
        parsed = urlparse(bing_url)
        if 'bing.com' not in parsed.netloc:
            return bing_url  # ä¸æ˜¯ Bing é‡å®šå‘ URLï¼Œç›´æ¥è¿”å›

        # ä»æŸ¥è¯¢å‚æ•°ä¸­æå– u å‚æ•°
        params = dict([p.split('=', 1) if '=' in p else (p, '')
                      for p in parsed.query.split('&')])
        u_param = params.get('u', '')

        if not u_param:
            return None

        # å»æ‰ "a1" å‰ç¼€
        if u_param.startswith('a1'):
            u_param = u_param[2:]

        # URL decode
        u_param = unquote(u_param)

        # Base64 è§£ç 
        # æ·»åŠ  padding ä»¥ç¡®ä¿é•¿åº¦æ˜¯ 4 çš„å€æ•°
        padding = 4 - len(u_param) % 4
        if padding != 4:
            u_param += '=' * padding

        decoded_bytes = base64.b64decode(u_param)
        real_url = decoded_bytes.decode('utf-8')

        return real_url

    except Exception as e:
        # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹ URL
        return None


class SearchResultsParser:
    """
    æœç´¢ç»“æœè§£æå™¨

    æ”¯æŒ Google å’Œ Bing æœç´¢ç»“æœé¡µçš„ HTML è§£æ
    """

    def __init__(self, logger=None):
        self.logger = logger
        self.should_process_url = None  # å›è°ƒå‡½æ•°ï¼Œç”¨äºè¿‡æ»¤URL

    def set_url_filter(self, should_process_url_func):
        """
        è®¾ç½®URLè¿‡æ»¤å‡½æ•°

        Args:
            should_process_url_func: å›è°ƒå‡½æ•°ï¼Œæ¥å—URLå‚æ•°ï¼Œè¿”å›bool
                                     True=åº”è¯¥å¤„ç†ï¼ŒFalse=è·³è¿‡
        """
        self.should_process_url = should_process_url_func

    def parse(self, html: str, url: str) -> Dict[str, Any]:
        """
        è§£ææœç´¢ç»“æœé¡µ HTML

        Args:
            html: æœç´¢ç»“æœé¡µçš„ HTML å†…å®¹
            url: å½“å‰é¡µé¢çš„ URLï¼ˆç”¨äºåˆ¤æ–­æœç´¢å¼•æ“ï¼‰

        Returns:
            {
                "search_engine": "google" | "bing",
                "featured_snippet": str or None,
                "results": List[SearchResultItem],
                "filtered_count": int  # è¢«è¿‡æ»¤æ‰çš„æœç´¢ç»“æœæ•°é‡
            }
        """
        # åˆ¤æ–­æœç´¢å¼•æ“
        is_google = 'google.com/search' in url or 'www.google.' in url
        is_bing = 'bing.com/search' in url

        if is_google:
            return self._parse_google_results(html, url)
        elif is_bing:
            return self._parse_bing_results(html, url)
        else:
            self.logger.warning(f"Unknown search engine for URL: {url}")
            return {
                "search_engine": "unknown",
                "featured_snippet": None,
                "results": [],
                "filtered_count": 0
            }

    def _parse_google_results(self, html: str, url: str) -> Dict[str, Any]:
        """
        è§£æ Google æœç´¢ç»“æœ

        ç­–ç•¥ï¼šä½¿ç”¨ç¨³å®šçš„HTMLç»“æ„ç‰¹å¾ï¼Œä¸ä¾èµ–åŠ¨æ€classåç§°
        - æœç´¢ç»“æœå¿…å®šæœ‰ <h3> æ ‡é¢˜
        - æ ‡é¢˜é™„è¿‘å¿…å®šæœ‰ <a href> é“¾æ¥
        - æ™ºèƒ½å›ç­”æ˜¯è¾ƒé•¿çš„æ–‡æœ¬å—ï¼Œé€šå¸¸åœ¨ç‰¹æ®Šä½ç½®
        """
        from bs4 import BeautifulSoup
        from urllib.parse import urlparse

        soup = BeautifulSoup(html, 'html.parser')
        results = []
        featured_snippet = None
        filtered_count = 0  # è¢«è¿‡æ»¤çš„æœç´¢ç»“æœæ•°é‡

        # 1. æå–æ™ºèƒ½å›ç­”ï¼ˆFeatured Snippetï¼‰
        # ç­–ç•¥ï¼šå¯»æ‰¾åŒ…å«è¾ƒé•¿æ–‡æœ¬çš„divï¼Œä¸”åœ¨å‰é¢ï¼ˆæ¥è¿‘bodyå¼€å§‹ï¼‰
        # æ™ºèƒ½å›ç­”é€šå¸¸æ˜¯ç‹¬ç«‹çš„ç»“æ„ï¼Œæ–‡æœ¬é•¿åº¦åœ¨100-1000å­—ç¬¦ä¹‹é—´
        all_divs = soup.find_all('div')
        for div in all_divs[:50]:  # åªæ£€æŸ¥å‰50ä¸ªdivï¼ˆæ™ºèƒ½å›ç­”é€šå¸¸åœ¨å‰é¢ï¼‰
            text = div.get_text(strip=True)
            # æ™ºèƒ½å›ç­”ç‰¹å¾ï¼š
            # - æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼ˆ100-1000å­—ç¬¦ï¼‰
            # - åŒ…å«å®Œæ•´çš„å¥å­
            # - ä¸å¤ªçŸ­ï¼ˆä¸æ˜¯å•ä¸ªè¯æˆ–æ ‡é¢˜ï¼‰
            if 100 < len(text) < 1000:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å¥å­ï¼ˆæœ‰å¥å·æˆ–æ¢è¡Œï¼‰
                if '.' in text or '\n' in text:
                    # é¿å…é‡å¤ï¼ˆæ£€æŸ¥æ˜¯å¦å·²ç»è¢«è®°å½•ä¸ºfeatured_snippetï¼‰
                    if not featured_snippet or len(text) > len(featured_snippet):
                        # ç¡®ä¿è¿™ä¸ªdivä¸æ˜¯æœç´¢ç»“æœçš„ä¸€éƒ¨åˆ†ï¼ˆæœç´¢ç»“æœé€šå¸¸æœ‰é“¾æ¥ï¼‰
                        if not div.find('a'):
                            featured_snippet = text
                            self.logger.debug(f"Found featured snippet: {featured_snippet[:100]}...")
                            break

        # 2. æå–æœç´¢ç»“æœ
        # æ ¸å¿ƒç­–ç•¥ï¼šæ‰€æœ‰æœç´¢ç»“æœéƒ½æœ‰ <h3> æ ‡é¢˜
        h3_elements = soup.find_all('h3')

        self.logger.debug(f"Found {len(h3_elements)} <h3> elements")

        seen_titles = set()  # ç”¨äºå»é‡

        for idx, h3 in enumerate(h3_elements, start=1):
            try:
                # æå–æ ‡é¢˜
                title = h3.get_text(strip=True)
                if not title or len(title) < 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    continue

                # å»é‡ï¼šé¿å…é‡å¤å¤„ç†ç›¸åŒæ ‡é¢˜
                if title in seen_titles:
                    continue
                seen_titles.add(title)

                # æŸ¥æ‰¾å¯¹åº”çš„é“¾æ¥
                result_url = None

                # æƒ…å†µ1: <h3><a href="...">æ ‡é¢˜</a></h3>
                link_elem = h3.find('a')
                if link_elem and link_elem.get('href'):
                    result_url = link_elem.get('href')
                else:
                    # æƒ…å†µ2: <a><h3>æ ‡é¢˜</h3></a>
                    parent_a = h3.find_parent('a')
                    if parent_a and parent_a.get('href'):
                        result_url = parent_a.get('href')
                    else:
                        # æƒ…å†µ3: h3çš„å…„å¼Ÿå…ƒç´ ä¸­æœ‰é“¾æ¥
                        parent = h3.parent
                        if parent:
                            sibling_a = parent.find('a')
                            if sibling_a and sibling_a.get('href'):
                                result_url = sibling_a.get('href')

                if not result_url or not result_url.startswith('http'):
                    continue

                # ã€æ–°å¢ã€‘è¿‡æ»¤å·²è®¿é—®/è¯„ä¼°çš„URL
                if self.should_process_url and not self.should_process_url(result_url):
                    filtered_count += 1
                    self.logger.debug(f"Filtered result {idx}: {title[:50]}... (URL: {result_url})")
                    continue

                # æŸ¥æ‰¾åŒ…å«è¿™ä¸ªh3çš„æœç´¢ç»“æœå®¹å™¨
                # å‘ä¸ŠæŸ¥æ‰¾ï¼Œæ‰¾åˆ°ä¸€ä¸ªåŒ…å«å¤šä¸ªå­å…ƒç´ çš„div
                container = h3
                level = 0
                while container and level < 5:
                    container = container.find_parent('div')
                    if not container:
                        break
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€ä¸ªåˆç†çš„ç»“æœå®¹å™¨
                    # åº”è¯¥æœ‰å¤šä¸ªå­divæˆ–æœ‰ <cite> å…ƒç´ ï¼ˆæ˜¾ç¤ºURLï¼‰
                    div_count = len(container.find_all('div'))
                    has_cite = container.find('cite') is not None
                    if div_count >= 2 or has_cite:
                        break
                    level += 1

                if not container:
                    container = h3.parent

                # ä»å®¹å™¨ä¸­æå–æ‘˜è¦å’Œç«™ç‚¹ä¿¡æ¯
                snippet = ""
                site_info = ""

                if container:
                    # æå–ç«™ç‚¹ä¿¡æ¯ï¼ˆä¼˜å…ˆä» <cite> å…ƒç´ ï¼‰
                    cite_elem = container.find('cite')
                    if cite_elem:
                        cite_text = cite_elem.get_text(strip=True)
                        # æ¸…ç†citeæ–‡æœ¬ï¼Œç§»é™¤URLè·¯å¾„ï¼Œåªä¿ç•™åŸŸå
                        if 'â€º' in cite_text:
                            site_info = cite_text.split('â€º')[0].strip()
                        elif '/' in cite_text:
                            site_info = cite_text.split('/')[0].strip()
                        else:
                            site_info = cite_text

                        # å¦‚æœciteåŒ…å«å®Œæ•´URLï¼Œæå–åŸŸå
                        if site_info.startswith('http'):
                            parsed = urlparse(site_info)
                            site_info = parsed.netloc

                    if not site_info:
                        # å°è¯•ä»URLæå–åŸŸå
                        parsed = urlparse(result_url)
                        site_info = parsed.netloc if parsed.netloc else ""

                    # æå–æ‘˜è¦
                    # ç­–ç•¥ï¼šæŸ¥æ‰¾åŒ…å«è¾ƒé•¿çº¯æ–‡æœ¬çš„spanæˆ–divï¼Œé¿å…åŒ…å«é“¾æ¥citeå…ƒç´ 
                    # æ‘˜è¦é€šå¸¸åœ¨ç‰¹å®šç»“æ„çš„spanä¸­ï¼Œä¸åŒ…å«å…¶ä»–HTMLå…ƒç´ 
                    for elem in container.find_all(['span', 'div']):
                        # è·³è¿‡citeå…ƒç´ å’ŒåŒ…å«é“¾æ¥çš„å…ƒç´ 
                        if elem.name == 'cite' or elem.find('a') or elem.find('cite'):
                            continue

                        text = elem.get_text(strip=True)
                        # æ‘˜è¦ç‰¹å¾ï¼š
                        # - é•¿åº¦é€‚ä¸­ï¼ˆ50-600å­—ç¬¦ï¼‰
                        # - ä¸æ˜¯æ ‡é¢˜æœ¬èº«
                        # - ä¸æ˜¯çº¯URLæˆ–åŸŸå
                        # - åŒ…å«å®Œæ•´å¥å­
                        if 50 < len(text) < 600:
                            # é¿å…URLã€åŸŸåç­‰
                            text_lower = text.lower()
                            if not any(skip in text_lower for skip in [
                                'translate', 'translation', 'cookie', 'privacy',
                                'sign in', 'login', 'skip to', 'http', 'https',
                                'www.', '.com', '.org', '.net'
                            ]):
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹ï¼ˆå­—æ¯æ¯”ä¾‹é«˜ï¼‰
                                alpha_ratio = sum(c.isalpha() or c.isspace() for c in text) / len(text)
                                if alpha_ratio > 0.7:  # è‡³å°‘70%æ˜¯å­—æ¯æˆ–ç©ºæ ¼
                                    snippet = text
                                    break

                # ç”Ÿæˆé“¾æ¥ID
                site_name = site_info.split(' â€º')[0].strip() if site_info else urlparse(result_url).netloc
                link_id = f"Link{idx} To: {site_name}"

                result = SearchResultItem(
                    title=title,
                    url=result_url,
                    snippet=snippet,
                    site_info=site_info,
                    link_id=link_id
                )
                results.append(result)

                self.logger.debug(f"Parsed result {idx}: {title[:50]}...")

            except Exception as e:
                if self.logger:
                    self.logger.warning(f"Failed to parse h3 element: {e}")
                continue

        self.logger.info(f"âœ“ Parsed {len(results)} Google search results")
        if filtered_count > 0:
            self.logger.info(f"  â†³ Filtered {filtered_count} already visited/evaluated results")

        # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥ï¼ˆä¼ å…¥å½“å‰é¡µé¢ URL ç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
        next_page_url = self._find_next_page_link(soup, url)
        if next_page_url:
            # å°†"ä¸‹ä¸€é¡µ"ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æœç´¢ç»“æœè¿½åŠ åˆ°åˆ—è¡¨ä¸­
            results.append(SearchResultItem(
                title="â†’ ä¸‹ä¸€é¡µ",
                url=next_page_url,
                snippet="ç‚¹å‡»æŸ¥çœ‹æ›´å¤šæœç´¢ç»“æœ",
                site_info="",
                link_id="ç¿»é¡µ: ä¸‹ä¸€é¡µ"
            ))
            self.logger.info(f"âœ“ Found next page link: {next_page_url}")

        return {
            "search_engine": "google",
            "featured_snippet": featured_snippet,
            "results": results,
            "filtered_count": filtered_count
        }

    def _parse_bing_results(self, html: str, url: str) -> Dict[str, Any]:
        """
        è§£æ Bing æœç´¢ç»“æœ

        Bing æœç´¢ç»“æœç»“æ„ï¼š
        - <li class="b_algo"> å•ä¸ªæœç´¢ç»“æœ
        - <h2><a href="...">æ ‡é¢˜</a></h2>
        - <div class="b_caption"> æ‘˜è¦
        """
        from bs4 import BeautifulSoup
        from urllib.parse import urlparse

        soup = BeautifulSoup(html, 'html.parser')
        results = []
        featured_snippet = None
        filtered_count = 0  # è¢«è¿‡æ»¤çš„æœç´¢ç»“æœæ•°é‡

        # 1. æå–æ™ºèƒ½å›ç­”ï¼ˆBing å¯èƒ½æ²¡æœ‰æ˜æ˜¾çš„ Featured Snippetï¼‰
        # æš‚æ—¶è·³è¿‡

        # 2. æå–æœç´¢ç»“æœ
        result_items = soup.find_all('li', class_='b_algo')

        for idx, item in enumerate(result_items, start=1):
            try:
                # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
                # ç»“æ„ï¼š<h2><a href="...">æ ‡é¢˜</a></h2>
                h2 = item.find('h2')
                if not h2:
                    continue

                link_elem = h2.find('a')
                if not link_elem or not link_elem.get('href'):
                    continue

                result_url = link_elem.get('href')
                title = link_elem.get_text(strip=True)

                # å°è¯•è§£æ Bing é‡å®šå‘ URL
                real_url = decode_bing_redirect_url(result_url)
                if real_url:
                    result_url = real_url

                # ã€æ–°å¢ã€‘è¿‡æ»¤å·²è®¿é—®/è¯„ä¼°çš„URL
                if self.should_process_url and not self.should_process_url(result_url):
                    filtered_count += 1
                    self.logger.debug(f"Filtered result {idx}: {title[:50]}... (URL: {result_url})")
                    continue

                # æå–æ‘˜è¦
                caption_div = item.find('div', class_='b_caption')
                snippet = ""
                site_info = ""

                if caption_div:
                    # æå–æè¿°æ–‡æœ¬
                    # é€šå¸¸åœ¨ <p> æˆ– <div> ä¸­
                    p_elem = caption_div.find('p')
                    if p_elem:
                        # è·å–çº¯æ–‡æœ¬ï¼Œä½†æ’é™¤æŸäº›å…ƒä¿¡æ¯
                        snippet_parts = []
                        for child in p_elem.children:
                            if hasattr(child, 'name') and child.name in ['span', 'cite']:
                                continue
                            if hasattr(child, 'get_text'):
                                text = child.get_text(strip=True)
                                if text:
                                    snippet_parts.append(text)
                        snippet = ' '.join(snippet_parts)

                    # æå–ç«™ç‚¹ä¿¡æ¯ï¼ˆé€šå¸¸æ˜¯ <cite> æ ‡ç­¾ï¼‰
                    cite_elem = caption_div.find('cite')
                    if cite_elem:
                        site_info = cite_elem.get_text(strip=True)

                # å¦‚æœæ²¡æœ‰ç«™ç‚¹ä¿¡æ¯ï¼Œä»çœŸå®URLæå–åŸŸå
                if not site_info:
                    parsed_url = urlparse(result_url)
                    site_info = parsed_url.netloc if parsed_url.netloc else ""

                # ç”Ÿæˆé“¾æ¥IDï¼ˆä½¿ç”¨çœŸå®URLçš„åŸŸåï¼‰
                parsed_url = urlparse(result_url)
                site_name = parsed_url.netloc if parsed_url.netloc else (site_info.split(' ')[0] if site_info else "unknown")
                link_id = f"Link{idx} To: {site_name}"

                result = SearchResultItem(
                    title=title,
                    url=result_url,
                    snippet=snippet,
                    site_info=site_info,
                    link_id=link_id
                )
                results.append(result)

            except Exception as e:
                if self.logger:
                    self.logger.warning(f"Failed to parse a Bing search result: {e}")
                continue

        self.logger.info(f"âœ“ Parsed {len(results)} Bing search results")
        if filtered_count > 0:
            self.logger.info(f"  â†³ Filtered {filtered_count} already visited/evaluated results")

        # æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥ï¼ˆä¼ å…¥å½“å‰é¡µé¢ URL ç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
        next_page_url = self._find_next_page_link(soup, url)
        if next_page_url:
            # å°†"ä¸‹ä¸€é¡µ"ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æœç´¢ç»“æœè¿½åŠ åˆ°åˆ—è¡¨ä¸­
            results.append(SearchResultItem(
                title="â†’ ä¸‹ä¸€é¡µ",
                url=next_page_url,
                snippet="ç‚¹å‡»æŸ¥çœ‹æ›´å¤šæœç´¢ç»“æœ",
                site_info="",
                link_id="ç¿»é¡µ: ä¸‹ä¸€é¡µ"
            ))
            self.logger.info(f"âœ“ Found next page link: {next_page_url}")

        return {
            "search_engine": "bing",
            "featured_snippet": featured_snippet,
            "results": results,
            "filtered_count": filtered_count
        }

    def format_as_markdown(self, parsed_data: Dict[str, Any]) -> str:
        """
        å°†è§£æç»“æœæ ¼å¼åŒ–ä¸º Markdown

        Args:
            parsed_data: parse() è¿”å›çš„æ•°æ®

        Returns:
            æ ¼å¼åŒ–çš„ Markdown æ–‡æœ¬
        """
        lines = []

        # 1. æ™ºèƒ½å›ç­”ï¼ˆå¦‚æœæœ‰ï¼‰
        if parsed_data.get("featured_snippet"):
            lines.append("# æ™ºèƒ½å›ç­”")
            lines.append("")
            lines.append(parsed_data["featured_snippet"])
            lines.append("")
            lines.append("")

        # 2. æœç´¢ç»“æœåˆ—è¡¨
        results = parsed_data.get("results", [])
        if not results:
            return "# æœªæ‰¾åˆ°æœç´¢ç»“æœ\n\n"

        lines.append(f"# æœç´¢ç»“æœ (å…± {len(results)} æ¡)")
        lines.append("")

        for idx, result in enumerate(results, start=1):
            # é“¾æ¥ID
            link_id = result.link_id
            lines.append(f"[ğŸ”—{link_id}]")
            lines.append(f"**{result.title}**")

            # ç«™ç‚¹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if result.site_info:
                lines.append(f"*{result.site_info}*")

            # æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if result.snippet:
                lines.append(result.snippet)

            # ç©ºè¡Œåˆ†éš”
            lines.append("")

        return "\n".join(lines)

    def build_link_mapping(self, parsed_data: Dict[str, Any]) -> Dict[str, str]:
        """
        æ„å»ºé“¾æ¥IDåˆ°URLçš„æ˜ å°„

        Args:
            parsed_data: parse() è¿”å›çš„æ•°æ®

        Returns:
            {"Link1 To: www.example.com": "https://www.example.com/..."}
        """
        mapping = {}
        for result in parsed_data.get("results", []):
            if result.link_id:
                mapping[result.link_id] = result.url
        return mapping

    def _find_next_page_link(self, soup, base_url: str) -> Optional[str]:
        """
        æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥

        Args:
            soup: BeautifulSoup å¯¹è±¡
            base_url: å½“å‰é¡µé¢çš„ URLï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰

        Returns:
            ä¸‹ä¸€é¡µçš„ URLï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            all_links = soup.find_all('a')

            for link in all_links:
                text = link.get_text(strip=True)
                aria_label = link.get('aria-label', '')

                href = link.get('href', '')
                if not href:
                    continue

                # åˆ¤æ–­æ˜¯å¦æ˜¯"ä¸‹ä¸€é¡µ"é“¾æ¥
                is_next = (
                    'next' in text.lower() or
                    'next' in aria_label.lower() or
                    'ä¸‹ä¸€é¡µ' in text or
                    'ä¸‹ä¸€é¡µ' in aria_label
                )

                if not is_next:
                    continue

                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if href.startswith('/'):
                    # ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥ base URL
                    try:
                        parsed_base = urlparse(base_url)
                        absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
                        href = absolute_url
                    except Exception:
                        continue

                # Bing: è§£ç é‡å®šå‘ URL
                real_url = decode_bing_redirect_url(href)
                if real_url:
                    href = real_url

                # ç¡®ä¿æœ€ç»ˆæ˜¯ http/https URL
                if not href.startswith('http'):
                    continue

                return href

        except Exception as e:
            if self.logger:
                self.logger.warning(f"Failed to find next page link: {e}")

        return None  # æ²¡æœ‰æ‰¾åˆ°"ä¸‹ä¸€é¡µ"é“¾æ¥
