"""
çˆ¬è™«æŠ€èƒ½çš„å…¬å…±è¾…åŠ©æ–¹æ³•

æä¾›é“¾æ¥ç­›é€‰ã€æŒ‰é’®é€‰æ‹©ç­‰ LLM å†³ç­–åŠŸèƒ½ã€‚
"""

import json
import re
import asyncio
from collections import deque
from typing import Dict, List, Optional, Any


class CrawlerHelperMixin:
    """
    çˆ¬è™«æŠ€èƒ½çš„å…¬å…±è¾…åŠ©æ–¹æ³• Mixin

    ä¾èµ–ï¼š
    - self.cerebellum.backend.think() - LLM è°ƒç”¨
    - self.logger - æ—¥å¿—è®°å½•
    """
    


    async def _filter_relevant_links(
        self,
        candidates: Dict[str, str],
        page_summary: str,
        ctx,
        current_url: str = None,
        prompt_template: str = None
    ) -> List[str]:
        """
        [Brain] æ‰¹é‡ç­›é€‰é“¾æ¥ï¼ˆç»Ÿä¸€å®ç°ï¼‰

        æ”¹è¿›ï¼šä½¿ç”¨æ–‡æœ¬æ˜ å°„æœºåˆ¶ï¼Œä¸è®© LLM çœ‹é•¿ URLï¼Œæå‡ Token æ•ˆç‡

        Args:
            candidates: å€™é€‰é“¾æ¥å­—å…¸ {url: link_text}
            page_summary: å½“å‰é¡µé¢æ‘˜è¦
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆMissionContext æˆ– WebSearcherContextï¼‰
            current_url: å½“å‰é¡µé¢ URLï¼ˆç”¨äºåˆ¤æ–­é“¾æ¥æ˜¯å¦æŒ‡å‘æœ¬ç«™ï¼‰
            prompt_template: å¯é€‰çš„è‡ªå®šä¹‰ promptï¼ˆç”¨äºç‰¹æ®Šéœ€æ±‚ï¼‰

        Returns:
            ç­›é€‰åçš„ URL åˆ—è¡¨
        """
        from urllib.parse import urlparse

        # 1. è§„åˆ™é¢„è¿‡æ»¤
        ignored_keywords = [
            "login", "signin", "sign up", "register", "password",
            "privacy policy", "terms of use", "contact us", "about us",
            "customer service", "language", "sitemap", "javascript:",
            "mailto:", "tel:", "unsubscribe"
        ]

        clean_candidates = {}
        for link, link_text in candidates.items():
            if not link or len(link_text) < 2:
                continue
            text_lower = link_text.lower()
            if any(k in text_lower for k in ignored_keywords):
                continue
            clean_candidates[link] = link_text

        if not clean_candidates:
            self.logger.debug(f"No clean links found for {ctx.purpose}")
            return []

        # 2. æå–å½“å‰åŸŸåï¼ˆç”¨äºåˆ¤æ–­é“¾æ¥æ˜¯å¦æŒ‡å‘æœ¬ç«™ï¼‰
        current_domain = None
        if current_url:
            try:
                current_domain = urlparse(current_url).netloc
            except:
                pass

        # 3. ä¸ºæ‰€æœ‰å€™é€‰é“¾æ¥æ„å»ºå¢å¼ºæ–‡æœ¬æ˜ å°„
        text_to_url = {}
        candidates_with_enhanced_text = []
        for url, text in clean_candidates.items():
            # è§£æé“¾æ¥çš„åŸŸå
            link_domain = None
            try:
                link_domain = urlparse(url).netloc
            except:
                pass

            # ç”Ÿæˆå¢å¼ºçš„æ˜¾ç¤ºæ–‡æœ¬
            if link_domain == current_domain:
                # æœ¬ç«™é“¾æ¥
                display_text = f"{text}ï¼ˆæœ¬ç«™ï¼‰"
            elif link_domain:
                # å¤–éƒ¨é“¾æ¥
                display_text = f"{text}ï¼ˆè·³è½¬åˆ° {link_domain}ï¼‰"
            else:
                # æ— æ³•è§£æåŸŸå
                display_text = text

            # å¤„ç†é‡åï¼ˆå¦‚æœå¤šä¸ªé“¾æ¥æœ‰ç›¸åŒçš„å¢å¼ºæ–‡æœ¬ï¼‰
            base_text = display_text
            counter = 2
            while display_text in text_to_url:
                display_text = f"{base_text}({counter})"
                counter += 1

            text_to_url[display_text] = url
            candidates_with_enhanced_text.append((url, display_text))

        # 4. æ ¹æ®å€™é€‰æ•°é‡ç¡®å®šæå‰ç»ˆæ­¢é˜ˆå€¼
        total_candidates = len(candidates_with_enhanced_text)
        if total_candidates <= 20:
            # å°‘é‡å€™é€‰ï¼šä¸è®¾é˜ˆå€¼ï¼Œå…¨éƒ¨å¤„ç†å®Œ
            early_stop_threshold = None
        else:
            # å¤§é‡å€™é€‰ï¼šè®¾ä½é˜ˆå€¼ï¼Œè¾¾åˆ°å³åœæ­¢
            early_stop_threshold = 3

        # 5. åˆ†æ‰¹ LLM è¿‡æ»¤
        batch_size = 10
        selected_urls = []
        candidates_list = candidates_with_enhanced_text

        for i in range(0, len(candidates_list), batch_size):
            batch = candidates_list[i:i + batch_size]

            # æ„å»ºç»™ LLM çœ‹çš„åˆ—è¡¨ï¼ˆåªæ˜¾ç¤ºå¢å¼ºæ–‡æœ¬ï¼Œä¸æ˜¾ç¤º URLï¼‰
            list_str = "\n".join([f"- {enhanced_text}" for url, enhanced_text in batch])

            # æ„å»º batch çš„æ–‡æœ¬åˆ°URLæ˜ å°„
            batch_text_to_url = {enhanced_text: url for url, enhanced_text in batch}

            # 6. æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = f"Mission: Find links relevant to \"{ctx.purpose}\".\n\n"
            if current_domain:
                context_info += f"Current website: {current_domain}\n"
            context_info += f"This page is about: {page_summary}\n\n"

            # ä½¿ç”¨è‡ªå®šä¹‰æˆ–é»˜è®¤ prompt
            if prompt_template:
                prompt = prompt_template.format(
                    purpose=ctx.purpose,
                    current_domain=current_domain or "unknown",
                    page_summary=page_summary,
                    list_str=list_str
                )
            else:
                # é»˜è®¤ promptï¼ˆé€‚é…æ–°çš„æ–‡æœ¬æ ¼å¼ï¼‰
                prompt = f"""{context_info}[Candidates]
{list_str}

[Instructions]
1. Select ONLY links that are DIRECTLY relevant to the Mission
2. Each link must have HIGH probability of containing useful information
3. IGNORE ambiguous or generic links (e.g., "Learn More", "Click Here"ï¼‰
4. AVOID links that clearly point to non-relevant content
5. å¦‚æœæ˜¯ç™¾åº¦ç™¾ç§‘è¿™ç±»ç½‘é¡µï¼Œä¸Šé¢çš„é“¾æ¥å¾ˆå¤šæ˜¯æ— å…³çš„ï¼Œè¦ä»”ç»†ç”„åˆ«ï¼Œåªé€‰æ‹©ç¡®å®šæœ‰å…³çš„
6. Be SELECTIVE - only choose links you are CONFIDENT will help
7. Note: Links marked with ï¼ˆæœ¬ç«™ï¼‰stay on the same website, others navigate away

OUTPUT FORMAT: Just list the link text exactly as shown above, one per line.
"""

            try:
                # è°ƒç”¨å°è„‘
                resp = await self.cerebellum.backend.think(
                    messages=[{"role": "user", "content": prompt}]
                )
                raw_reply = resp.get('reply', '')
                self.logger.debug(f"LLM reply: {raw_reply}")

                # 7. è§£æ LLM è¿”å›çš„æ–‡æœ¬ï¼Œæ˜ å°„å› URL
                reply_lines = [line.strip() for line in raw_reply.split('\n') if line.strip()]

                for reply_text in reply_lines:
                    # å®¹é”™ï¼šç§»é™¤å¯èƒ½çš„åºå·å‰ç¼€ï¼ˆå¦‚ "1. ", "- "ï¼‰
                    clean_reply_text = reply_text.strip()
                    if clean_reply_text.startswith('- '):
                        clean_reply_text = clean_reply_text[2:].strip()
                    if '. ' in clean_reply_text[:5]:  # åªåœ¨å¼€å¤´æ£€æŸ¥åºå·
                        try:
                            parts = clean_reply_text.split('. ', 1)
                            if parts[0].isdigit():
                                clean_reply_text = parts[1].strip()
                        except:
                            pass

                    # ä»æœ¬æ‰¹æ¬¡çš„æ˜ å°„ä¸­æŸ¥æ‰¾ URL
                    if clean_reply_text in batch_text_to_url:
                        url = batch_text_to_url[clean_reply_text]
                        if url not in selected_urls:
                            selected_urls.append(url)
                    else:
                        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå®¹é”™ LLM è¾“å‡ºæ—¶çš„å¾®å°å˜åŒ–ï¼‰
                        for display_text, url in batch_text_to_url.items():
                            if clean_reply_text in display_text or display_text in clean_reply_text:
                                if url not in selected_urls:
                                    selected_urls.append(url)
                                break

                # 8. æå‰ç»ˆæ­¢ï¼šè¾¾åˆ°é˜ˆå€¼ååœæ­¢
                if early_stop_threshold and len(selected_urls) >= early_stop_threshold:
                    self.logger.info(
                        f"âœ“ Early stop: selected {len(selected_urls)} links "
                        f"(threshold: {early_stop_threshold}) from {total_candidates} candidates"
                    )
                    return selected_urls[:early_stop_threshold]

            except Exception as e:
                self.logger.error(f"Link filtering batch failed: {e}")
                continue

        self.logger.debug(f"Selected {len(selected_urls)} links from {total_candidates} candidates")
        return selected_urls

    async def _choose_best_interaction(
        self,
        candidates: List[Dict],
        page_summary: str,
        ctx,
        prompt_template: str = None
    ) -> Optional:
        """
        [Brain] é€‰æ‹©æœ€ä½³æŒ‰é’®ç‚¹å‡»ï¼ˆç»Ÿä¸€å®ç°ï¼‰

        ä½¿ç”¨ä¸²è¡Œæ·˜æ±°æœºåˆ¶ + ä¸‰çº§ç­›é€‰ç­–ç•¥ï¼š
        1. Immediate (ç«‹å³è®¿é—®): é«˜åº¦å»åˆï¼Œç›´æ¥è¿”å›
        2. Potential (æ½œåœ¨ç›¸å…³): å¯èƒ½ç›¸å…³ï¼Œæ”¾å›é˜Ÿåˆ—å¤´éƒ¨ç»§ç»­ç«äº‰
        3. None (æ— ä»·å€¼): åˆ é™¤ï¼Œç»§ç»­ä¸‹ä¸€ç»„

        Args:
            candidates: List[Dict] æ ¼å¼ï¼Œæ¯ä¸ª Dict æ˜¯ {button_text: PageElement}
            page_summary: å½“å‰é¡µé¢æ‘˜è¦
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆMissionContext æˆ– WebSearcherContextï¼‰
            prompt_template: å¯é€‰çš„è‡ªå®šä¹‰ prompt æ¨¡æ¿

        Returns:
            é€‰ä¸­çš„ PageElementï¼Œå¦‚æœæ²¡æœ‰åˆé€‚çš„åˆ™è¿”å› None
        """
        if not candidates:
            return None

        BATCH_SIZE = 10

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼: [(button_text, element), ...]
        all_candidates = []
        for candidate_dict in candidates:
            for text, element in candidate_dict.items():
                all_candidates.append((text, element))

        # ä½¿ç”¨ deque æ”¯æŒé«˜æ•ˆçš„å¤´éƒ¨æ“ä½œ
        candidate_deque = deque(all_candidates)

        while candidate_deque:
            # å–å‰ batch_size ä¸ªï¼ˆå¦‚æœä¸è¶³åˆ™å–å…¨éƒ¨ï¼‰
            batch_size = min(BATCH_SIZE, len(candidate_deque))
            batch = [candidate_deque.popleft() for _ in range(batch_size)]

            # è¯„ä¼°è¿™æ‰¹
            if prompt_template:
                result = await self._evaluate_button_batch(
                    batch, page_summary, ctx, prompt_template
                )
            else:
                result = await self._evaluate_button_batch(
                    batch, page_summary, ctx
                )

            if result["priority"] == "immediate":
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼Œç«‹å³è¿”å›
                self.logger.info(
                    f"âš¡ Immediate match found: [{result['text']}] | "
                    f"Reason: {result['reason']}"
                )
                return result["element"]

            elif result["priority"] == "potential":
                # å°† winner æ”¾å›é˜Ÿåˆ—å¤´éƒ¨ï¼Œå‚ä¸ä¸‹ä¸€è½®ç«äº‰
                winner_tuple = (result["text"], result["element"])
                if len(candidate_deque) > 0:
                    candidate_deque.appendleft(winner_tuple)
                    self.logger.debug(
                        f"    Potential: [{result['text']}] â†’ Put back to queue front. "
                        f"Queue size: {len(candidate_deque)}"
                    )
                else:
                    return result["element"]
            # else: Noneï¼Œè¿™æ‰¹å…¨éƒ¨ä¸¢å¼ƒï¼Œç»§ç»­ä¸‹ä¸€è½®

        return None

    async def _evaluate_button_batch(
        self,
        batch: List[tuple],
        page_summary: str,
        ctx,
        prompt_template: str = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°ä¸€æ‰¹æŒ‰é’®ï¼ˆç»Ÿä¸€å®ç°ï¼‰

        Args:
            batch: [(button_text, element), ...]
            page_summary: é¡µé¢æ‘˜è¦
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
            prompt_template: å¯é€‰çš„è‡ªå®šä¹‰ prompt æ¨¡æ¿

        Returns:
            {
                "priority": "immediate" | "potential" | "none",
                "text": str,
                "element": PageElement,
                "reason": str
            }
        """
        if not batch:
            return {"priority": "none", "text": None, "element": None, "reason": "Empty batch"}

        options_str = ""
        for idx, (text, element) in enumerate(batch):
            options_str += f"{idx + 1}. [{text}]\n"
        options_str += "0. [None of these are useful]"

        # ä½¿ç”¨è‡ªå®šä¹‰æˆ–é»˜è®¤ prompt
        if prompt_template:
            prompt = prompt_template.format(
                purpose=ctx.purpose,
                page_summary=page_summary,
                options_str=options_str,
                batch_size=len(batch)
            )
        else:
            # é»˜è®¤ promptï¼ˆå– data_crawler çš„ç‰ˆæœ¬ï¼Œæ›´è¯¦ç»†ï¼‰
            prompt = f"""
            You are evaluating buttons on a webpage to see if it can help with your research topic:
            [Research Topic]
                 {ctx.purpose}

            [Page Context]
            {page_summary}

            [Task]
            Categorize your choice into THREE levels:

            **LEVEL 1 - IMMEDIATE** (åº”ç«‹å³è®¿é—®)
            - Button clearly leads to information that achieves the purpose

            **LEVEL 2 - POTENTIAL** (å¯èƒ½ç›¸å…³)
            - Button might lead to relevant information
            - Examples: "Learn More", "Details", "Next", "View Resources"

            **LEVEL 3 - NONE** (éƒ½ä¸ç›¸å…³)
            - Buttons unrelated to the purpose
            - Examples: "Share", "Login", "Home", "Contact"

            [Options]
            {options_str}

            [Output Format]
            JSON:
            {{
                "choice_id": <number 0-{len(batch)}>,
                "priority": "immediate" | "potential" | "none",
                "reason": "short explanation"
            }}
            """

        try:
            resp = await self.cerebellum.backend.think(
                messages=[{"role": "user", "content": prompt}]
            )
            raw_reply = resp.get('reply', '')

            # æå– JSONï¼ˆå…¼å®¹å„ç§æ ¼å¼ï¼‰
            json_str = raw_reply.replace("```json", "").replace("```", "").strip()
            result = json.loads(json_str)

            choice_id = int(result.get("choice_id", 0))
            priority = result.get("priority", "none").lower()
            reason = result.get("reason", "")

            if priority not in ["immediate", "potential", "none"]:
                priority = "none"

            if choice_id == 0 or priority == "none":
                return {"priority": "none", "text": None, "element": None, "reason": reason}

            selected_index = choice_id - 1

            if 0 <= selected_index < len(batch):
                selected_text, selected_element = batch[selected_index]
                return {
                    "priority": priority,
                    "text": selected_text,
                    "element": selected_element,
                    "reason": reason
                }
            else:
                return {"priority": "none", "text": None, "element": None, "reason": "Invalid choice"}

        except Exception as e:
            self.logger.exception(f"Batch evaluation failed: {e}")
            return {"priority": "none", "text": None, "element": None, "reason": f"Error: {e}"}

    async def _get_full_page_markdown(self, tab, ctx) -> str:
        """
        è·å–å®Œæ•´é¡µé¢çš„ Markdownï¼Œè‡ªåŠ¨è¯†åˆ« HTML/PDF

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        from ..core.browser.browser_adapter import PageType

        content_type = await self.browser.analyze_page_type(tab)

        if content_type == PageType.STATIC_ASSET:
            return await self._pdf_to_full_markdown(tab, ctx)
        else:
            return await self._html_to_full_markdown(tab)

    

    async def _html_to_full_markdown(self, tab) -> str:
        """
        å°† HTML é¡µé¢è½¬æ¢ä¸ºå®Œæ•´ Markdownï¼ˆå¸¦æ™ºèƒ½å›é€€æœºåˆ¶ï¼‰

        ç­–ç•¥ï¼š
        1. å¦‚æœæ˜¯ Google/Bing æœç´¢ç»“æœé¡µï¼šç›´æ¥ç”¨ html2textï¼ˆä¸ä½¿ç”¨ trafilaturaï¼‰
        2. å¦åˆ™ï¼šä¼˜å…ˆä½¿ç”¨ trafilaturaï¼ˆæ™ºèƒ½æå–æ­£æ–‡ï¼‰ï¼Œå¤±è´¥åˆ™å›é€€åˆ° html2text

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        import trafilatura
        import re
        import html2text

        raw_html = tab.html
        url = self.browser.get_tab_url(tab)

        # åˆ¤æ–­æ˜¯å¦æ˜¯æœç´¢ç»“æœé¡µ
        is_google_search = 'google.com/search' in url or 'www.google.' in url
        is_bing_search = 'bing.com/search' in url
        is_search_results = is_google_search or is_bing_search

        # é¢„å¤„ç†ï¼šç§»é™¤ç¿»è¯‘æŒ‰é’®ï¼ˆå­—ç¬¦ä¸²æ›¿æ¢æ–¹å¼ï¼‰
        if is_search_results:
            if is_google_search:
                # Google: <a><span>ç¿»è¯‘æ­¤é¡µ</span></a> - åˆ é™¤æ•´ä¸ªaæ ‡ç­¾
                raw_html = re.sub(
                    r'<a\s+[^>]*><span[^>]*>\s*ç¿»è¯‘æ­¤é¡µ\s*</span></a>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE
                )
                raw_html = re.sub(
                    r'<a\s+[^>]*href="https://translate\.google\.com[^"]*"[^>]*>.*?</a>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE | re.DOTALL
                )
                # ä¹Ÿå¤„ç†å•å¼•å·çš„æƒ…å†µ
                raw_html = re.sub(
                    r"<a\s+[^>]*href='https://translate\.google\.com[^']*'[^>]*>.*?</a>",
                    '',
                    raw_html,
                    flags=re.IGNORECASE | re.DOTALL
                )
                self.logger.debug("âœ“ Removed Google translate buttons via string replacement")
            elif is_bing_search:
                # Bing: <span>ç¿»è¯‘æ­¤ç»“æœ</span> - åªåˆ é™¤span
                raw_html = re.sub(
                    r'<span[^>]*>\s*ç¿»è¯‘æ­¤ç»“æœ\s*</span>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE
                )
                self.logger.debug("âœ“ Removed Bing translate buttons via string replacement")

            # æœç´¢ç»“æœé¡µç›´æ¥ä½¿ç”¨ html2text
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            converter.ignore_images = True
            converter.body_width = 0
            converter.ignore_emphasis = False

            markdown = converter.handle(raw_html)
            self.logger.info(f"âœ“ Search results page converted with html2text: {len(markdown)} chars")
            return markdown or ""

        
            

        # å°è¯• trafilatura æå–æ­£æ–‡
        markdown = trafilatura.extract(
            raw_html,
            include_links=True,
            include_formatting=True,
            output_format='markdown',
            url=url
        )
        self.logger.debug(f"\n\n {markdown} \n\n")

        

        return markdown or ""

    async def _pdf_to_full_markdown(self, tab, ctx) -> str:
        """
        å°† PDF è½¬æ¢ä¸ºå®Œæ•´ Markdown

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„ PDF å†…å®¹
        """
        # ä¸‹è½½ PDF åˆ°æœ¬åœ°
        pdf_path = await self.browser.save_static_asset(tab)

        # è°ƒç”¨ PDF è½¬æ¢ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ï¼‰
        markdown = await asyncio.to_thread(
            self._convert_pdf_to_markdown_text,
            pdf_path
        )

        # å¯é€‰ï¼šä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœ ctx æ”¯æŒï¼‰
        if hasattr(ctx, 'temp_file_dir') and ctx.temp_file_dir:
            self._save_temp_markdown(markdown, pdf_path, ctx.temp_file_dir)

        return markdown

    def _convert_pdf_to_markdown_text(self, pdf_path: str) -> str:
        """
        å°† PDF æ–‡ä»¶è½¬æ¢ä¸º Markdown æ–‡æœ¬ï¼ˆæ ¸å¿ƒå®ç°ï¼‰

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            str: Markdown æ ¼å¼çš„æ–‡æœ¬

        Raises:
            Exception: PDF è½¬æ¢å¤±è´¥
        """
        from marker.converters.pdf import PdfConverter
        from marker.models import create_model_dict
        from marker.output import text_from_rendered
        import fitz

        try:
            # è·å– PDF æ€»é¡µæ•°
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)

            self.logger.debug(f"PDFæ€»é¡µæ•°: {total_pages}")

            # åˆå§‹åŒ– marker æ¨¡å‹
            self.logger.debug("åŠ è½½ marker æ¨¡å‹...")
            converter = PdfConverter(
                artifact_dict=create_model_dict(),
            )

            # æ‰§è¡Œè½¬æ¢
            self.logger.debug("æ­£åœ¨è½¬æ¢ PDF åˆ° Markdown...")
            rendered = converter(pdf_path)

            # ä»æ¸²æŸ“ç»“æœä¸­æå–æ–‡æœ¬
            text, _, images = text_from_rendered(rendered)

            self.logger.debug(f"è½¬æ¢å®Œæˆ! å…± {len(text)} ä¸ªå­—ç¬¦")

            return text

        except Exception as e:
            self.logger.error(f"PDF è½¬æ¢å¤±è´¥: {e}")
            raise

    def _save_temp_markdown(self, markdown: str, pdf_path: str, temp_dir: str):
        """
        ä¿å­˜ä¸´æ—¶ Markdown æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            markdown: Markdown æ–‡æœ¬
            pdf_path: åŸå§‹ PDF è·¯å¾„
            temp_dir: ä¸´æ—¶ç›®å½•
        """
        import os
        from slugify import slugify

        os.makedirs(temp_dir, exist_ok=True)
        filename = slugify(f"pdf_{os.path.basename(pdf_path)}") + ".md"
        temp_path = os.path.join(temp_dir, filename)

        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(markdown)

        self.logger.info(f"ğŸ“„ ä¿å­˜ä¸´æ—¶ Markdown: {temp_path}")
