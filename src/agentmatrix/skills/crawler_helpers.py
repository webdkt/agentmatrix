"""
çˆ¬è™«æŠ€èƒ½çš„å…¬å…±è¾…åŠ©æ–¹æ³•

æä¾›é“¾æ¥ç­›é€‰ã€æŒ‰é’®é€‰æ‹©ç­‰ LLM å†³ç­–åŠŸèƒ½ã€‚
"""

import json
import re
import asyncio
from collections import deque
from typing import Dict, List, Optional, Any


class CrawlerHelperMixin:
    """
    çˆ¬è™«æŠ€èƒ½çš„å…¬å…±è¾…åŠ©æ–¹æ³• Mixin

    ä¾èµ–ï¼š
    - self.cerebellum.backend.think() - LLM è°ƒç”¨
    - self.logger - æ—¥å¿—è®°å½•
    """

    async def _get_full_page_markdown(self, tab, ctx, original_url: str = None) -> str:
        """
        è·å–å®Œæ•´é¡µé¢çš„ Markdownï¼Œè‡ªåŠ¨è¯†åˆ« HTML/PDF

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰
            original_url: åŸå§‹URLï¼ˆå¯é€‰ï¼Œç”¨äºå¤„ç†Chrome PDF viewerç­‰è½¬æ¢åçš„URLï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        from ..core.browser.browser_adapter import PageType

        content_type = await self.browser.analyze_page_type(tab)

        if content_type == PageType.STATIC_ASSET:
            return await self._pdf_to_full_markdown(tab, ctx, original_url)
        else:
            return await self._html_to_full_markdown(tab)

    

    async def _html_to_full_markdown(self, tab) -> str:
        """
        å°† HTML é¡µé¢è½¬æ¢ä¸ºå®Œæ•´ Markdownï¼ˆå¸¦æ™ºèƒ½å›é€€æœºåˆ¶ï¼‰

        ç­–ç•¥ï¼š
        1. å¦‚æœæ˜¯ Google/Bing æœç´¢ç»“æœé¡µï¼šç›´æ¥ç”¨ html2textï¼ˆä¸ä½¿ç”¨ trafilaturaï¼‰
        2. å¦åˆ™ï¼šä¼˜å…ˆä½¿ç”¨ trafilaturaï¼ˆæ™ºèƒ½æå–æ­£æ–‡ï¼‰ï¼Œå¤±è´¥åˆ™å›é€€åˆ° html2text

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        import trafilatura
        import re
        import html2text

        raw_html = tab.html
        url = self.browser.get_tab_url(tab)

        # åˆ¤æ–­æ˜¯å¦æ˜¯æœç´¢ç»“æœé¡µ
        is_google_search = 'google.com/search' in url or 'www.google.' in url
        is_bing_search = 'bing.com/search' in url
        is_search_results = is_google_search or is_bing_search

        # é¢„å¤„ç†ï¼šç§»é™¤ç¿»è¯‘æŒ‰é’®ï¼ˆå­—ç¬¦ä¸²æ›¿æ¢æ–¹å¼ï¼‰
        if is_search_results:
            if is_google_search:
                # Google: <a><span>ç¿»è¯‘æ­¤é¡µ</span></a> - åˆ é™¤æ•´ä¸ªaæ ‡ç­¾
                raw_html = re.sub(
                    r'<a\s+[^>]*><span[^>]*>\s*ç¿»è¯‘æ­¤é¡µ\s*</span></a>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE
                )
                raw_html = re.sub(
                    r'<a\s+[^>]*href="https://translate\.google\.com[^"]*"[^>]*>.*?</a>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE | re.DOTALL
                )
                # ä¹Ÿå¤„ç†å•å¼•å·çš„æƒ…å†µ
                raw_html = re.sub(
                    r"<a\s+[^>]*href='https://translate\.google\.com[^']*'[^>]*>.*?</a>",
                    '',
                    raw_html,
                    flags=re.IGNORECASE | re.DOTALL
                )
                self.logger.debug("âœ“ Removed Google translate buttons via string replacement")
            elif is_bing_search:
                # Bing: <span>ç¿»è¯‘æ­¤ç»“æœ</span> - åªåˆ é™¤span
                raw_html = re.sub(
                    r'<span[^>]*>\s*ç¿»è¯‘æ­¤ç»“æœ\s*</span>',
                    '',
                    raw_html,
                    flags=re.IGNORECASE
                )
                self.logger.debug("âœ“ Removed Bing translate buttons via string replacement")

            # æœç´¢ç»“æœé¡µç›´æ¥ä½¿ç”¨ html2text
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            converter.ignore_images = True
            converter.body_width = 0
            converter.ignore_emphasis = False

            markdown = converter.handle(raw_html)
            self.logger.info(f"âœ“ Search results page converted with html2text: {len(markdown)} chars")
            return markdown or ""

        
            

        # å°è¯• trafilatura æå–æ­£æ–‡
        markdown = trafilatura.extract(
            raw_html,
            include_links=True,
            include_formatting=True,
            output_format='markdown',
            url=url
        )
        self.logger.debug(f"\n\n {markdown} \n\n")

        

        return markdown or ""

    async def _pdf_to_full_markdown(self, tab, ctx, original_url: str = None) -> str:
        """
        å°† PDF è½¬æ¢ä¸ºå®Œæ•´ Markdown

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰
            original_url: åŸå§‹URLï¼ˆå¯é€‰ï¼Œç”¨äºå¤„ç†Chrome PDF viewerç­‰è½¬æ¢åçš„URLï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„ PDF å†…å®¹
        """
        # ä¸‹è½½ PDF åˆ°æœ¬åœ°
        pdf_path = await self.browser.save_static_asset(tab, original_url)

        # æ£€æŸ¥tabæ˜¯å¦å¡åœ¨PDF vieweré¡µé¢ï¼ˆchrome-extension://ï¼‰
        # åœ¨å¼€å§‹é•¿æ—¶é—´PDFè½¬æ¢å‰æ£€æŸ¥ï¼Œé¿å…è½¬æ¢å®Œæˆåtabå·²ç»å¼‚å¸¸
        current_url = tab.url if hasattr(tab, 'url') else ""
        if current_url.startswith('chrome-extension://'):
            self.logger.warning(f"âš ï¸ Tabå¡åœ¨PDF vieweré¡µé¢ ({current_url})ï¼Œæ­£åœ¨ä¿®å¤...")
            # Navigateåˆ°about:blankè®©tabè„±ç¦»PDF viewerçŠ¶æ€
            await self.browser.navigate(tab, "about:blank")
            await asyncio.sleep(0.5)
            self.logger.info("âœ“ Tabå·²ä¿®å¤ï¼Œè„±ç¦»PDF viewerçŠ¶æ€")

        # è°ƒç”¨ PDF è½¬æ¢ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ï¼‰
        markdown = await asyncio.to_thread(
            self._convert_pdf_to_markdown_text,
            pdf_path
        )

        # å¯é€‰ï¼šä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœ ctx æ”¯æŒï¼‰
        if hasattr(ctx, 'temp_file_dir') and ctx.temp_file_dir:
            self._save_temp_markdown(markdown, pdf_path, ctx.temp_file_dir)

        return markdown

    def _convert_pdf_to_markdown_text(self, pdf_path: str) -> str:
        """
        å°† PDF æ–‡ä»¶è½¬æ¢ä¸º Markdown æ–‡æœ¬ï¼ˆæ ¸å¿ƒå®ç°ï¼‰

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            str: Markdown æ ¼å¼çš„æ–‡æœ¬

        Raises:
            Exception: PDF è½¬æ¢å¤±è´¥
        """
        from marker.converters.pdf import PdfConverter
        from marker.models import create_model_dict
        from marker.output import text_from_rendered
        import fitz

        try:
            # è·å– PDF æ€»é¡µæ•°
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)

            self.logger.debug(f"PDFæ€»é¡µæ•°: {total_pages}")

            # åˆå§‹åŒ– marker æ¨¡å‹
            self.logger.debug("åŠ è½½ marker æ¨¡å‹...")
            converter = PdfConverter(
                artifact_dict=create_model_dict(),
            )

            # æ‰§è¡Œè½¬æ¢
            self.logger.debug("æ­£åœ¨è½¬æ¢ PDF åˆ° Markdown...")
            rendered = converter(pdf_path)

            # ä»æ¸²æŸ“ç»“æœä¸­æå–æ–‡æœ¬
            text, _, images = text_from_rendered(rendered)

            self.logger.debug(f"è½¬æ¢å®Œæˆ! å…± {len(text)} ä¸ªå­—ç¬¦")

            return text

        except Exception as e:
            self.logger.error(f"PDF è½¬æ¢å¤±è´¥: {e}")
            raise

    def _save_temp_markdown(self, markdown: str, pdf_path: str, temp_dir: str):
        """
        ä¿å­˜ä¸´æ—¶ Markdown æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            markdown: Markdown æ–‡æœ¬
            pdf_path: åŸå§‹ PDF è·¯å¾„
            temp_dir: ä¸´æ—¶ç›®å½•
        """
        import os
        from slugify import slugify

        os.makedirs(temp_dir, exist_ok=True)
        filename = slugify(f"pdf_{os.path.basename(pdf_path)}") + ".md"
        temp_path = os.path.join(temp_dir, filename)

        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(markdown)

        self.logger.info(f"ğŸ“„ ä¿å­˜ä¸´æ—¶ Markdown: {temp_path}")


    async def is_navigation_page(self, tab, url: str) -> tuple:
        """
        åˆ¤æ–­é¡µé¢æ˜¯å¦ä¸ºå¯¼èˆªé¡µï¼ˆé¦–é¡µ/é¢‘é“é¡µ/ç›®å½•é¡µï¼‰

        åˆ¤æ–­æ ‡å‡†ï¼š
        1. URLæ·±åº¦æ£€æŸ¥ï¼šé¦–é¡µ(pathé•¿åº¦ <= 1)å¾ˆå¯èƒ½æ˜¯å¯¼èˆªé¡µ
        2. Open Graphæ£€æŸ¥ï¼šog:type="website" é€šå¸¸æ˜¯é¦–é¡µæˆ–é¢‘é“é¡µ
        3. é“¾æ¥å¯†åº¦æ£€æŸ¥ï¼šå¦‚æœ30%ä»¥ä¸Šçš„æ–‡å­—éƒ½æ˜¯é“¾æ¥ï¼Œæˆ–æ€»å­—æ•°å¾ˆå°‘ï¼Œæå¤§æ¦‚ç‡æ˜¯å¯¼èˆªé¡µ

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µ
            url: å½“å‰é¡µé¢URL

        Returns:
            (is_navigation, reason): æ˜¯å¦ä¸ºå¯¼èˆªé¡µåŠåˆ¤æ–­åŸå› 
        """
        from urllib.parse import urlparse

        parsed_url = urlparse(url)
        path = parsed_url.path

        # æ ‡å‡†1: URLæ·±åº¦æ£€æŸ¥ - é¦–é¡µé€šå¸¸æ˜¯å¯¼èˆªé¡µ
        if len(path) <= 1 or path == "/":
            return True, "URL is homepage (root path)"

        # æ ‡å‡†2: Open Graph æ£€æŸ¥
        try:
            og_type_tag = tab.ele('css:meta[property="og:type"]', timeout=1)
            if og_type_tag:
                og_content = og_type_tag.attr('content')
                if og_content == 'website':
                    return True, f"Open Graph type is 'website' (not 'article')"
        except:
            pass  # å¦‚æœæ‰¾ä¸åˆ°æˆ–å‡ºé”™ï¼Œç»§ç»­å…¶ä»–æ£€æŸ¥

        # æ ‡å‡†3: é“¾æ¥å¯†åº¦æ£€æŸ¥
        try:
            # è·å–æ‰€æœ‰æ–‡æœ¬ - DrissionPage éœ€è¦ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•
            # å…ˆè·å– body å…ƒç´ ï¼Œå†è·å–æ–‡æœ¬
            try:
                body = tab.ele('tag:body', timeout=2)
                if body:
                    all_text = body.text.strip()
                else:
                    all_text = ""
            except:
                all_text = ""

            total_length = len(all_text)

            if total_length > 0:
                # è·å–æ‰€æœ‰é“¾æ¥æ–‡æœ¬
                link_elements = tab.eles('tag:a')
                link_text_length = 0

                for link in link_elements:
                    try:
                        link_text = link.text.strip()
                        link_text_length += len(link_text)
                    except:
                        continue

                # è®¡ç®—é“¾æ¥å¯†åº¦
                link_density = link_text_length / total_length

                self.logger.debug(f"Link density check: {link_density:.1%}, total length: {total_length}")

                # åˆ¤æ–­ï¼šé“¾æ¥å¯†åº¦ > 30% æˆ–æ€»å­—æ•°å¾ˆå°‘ (<500å­—ç¬¦)
                if link_density > 0.3:
                    return True, f"Link density {link_density:.1%} > 30%"
                elif total_length < 500:
                    return True, f"Total text length ({total_length}) is very short"

        except Exception as e:
            self.logger.debug(f"Link density check failed: {e}")
            pass  # é“¾æ¥å¯†åº¦æ£€æŸ¥å¤±è´¥ï¼Œä¸åˆ¤å®š

        # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œåˆ¤å®šä¸ºå†…å®¹é¡µ
        return False, "Does not meet navigation page criteria"


    async def get_markdown_via_jina(self, url: str, timeout: int = 30) -> str:
        """
        é€šè¿‡ r.jina.ai APIè·å–é¡µé¢çš„ Markdown æ ¼å¼
        æ”¯æŒä½¿ç”¨ JINA_API_KEY ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœé…ç½®äº†åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨å…è´¹APIï¼‰

        Args:
            url: ç›®æ ‡é¡µé¢URL
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        import aiohttp
        import os

        jina_url = f"https://r.jina.ai/{url}"

        # è·å– API Keyï¼ˆå¯é€‰ï¼‰
        api_key = os.getenv("JINA_API_KEY")

        # è®¾ç½®è¯·æ±‚å¤´
        headers = {}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
            self.logger.debug(f"Using Jina API key for request")

        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                    jina_url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=timeout)
                ) as response:
                    response.raise_for_status()
                    markdown = await response.text()
                    return markdown
            except asyncio.TimeoutError:
                raise Exception(f"Timeout fetching markdown from jina.ai for URL: {url}")
            except Exception as e:
                raise Exception(f"Failed to fetch markdown from jina.ai: {e}")


    def extract_navigation_links(self, markdown: str) -> list:
        """
        ä» Markdown æ–‡æœ¬ä¸­æå–æ‰€æœ‰é“¾æ¥

        è¿‡æ»¤è§„åˆ™ï¼š
        1. å¿½ç•¥çº¯å›¾ç‰‡é“¾æ¥ï¼ˆtext ä»¥ "![", ".png", ".jpg", ".jpeg", ".gif", ".svg" ç­‰å¼€å¤´ï¼‰
        2. å›¾ç‰‡+æ–‡æœ¬é“¾æ¥ï¼šåªä¿ç•™æ–‡æœ¬éƒ¨åˆ†ï¼ˆç§»é™¤å›¾ç‰‡æ ‡è®°ï¼‰
        3. å»é‡

        Args:
            markdown: Markdown æ ¼å¼çš„æ–‡æœ¬

        Returns:
            é“¾æ¥åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¥åŒ…å« {"url": str, "title": str}
        """
        from dataclasses import dataclass

        @dataclass
        class NavigationLink:
            url: str
            title: str

        # Markdown é“¾æ¥æ ¼å¼: [text](url)
        link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        matches = re.findall(link_pattern, markdown)

        # å»é‡å¹¶è¿‡æ»¤
        seen_urls = set()
        unique_links = []

        for raw_text, url in matches:
            if url in seen_urls:
                continue

            # æ¸…ç†é“¾æ¥æ–‡æœ¬
            text = raw_text.strip()

            # è§„åˆ™1: å¿½ç•¥çº¯å›¾ç‰‡é“¾æ¥
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ ‡è®°ï¼ˆ![...] æˆ–ä»¥å›¾ç‰‡æ‰©å±•åç»“å°¾ï¼‰
            if text.startswith('!['):
                # çº¯å›¾ç‰‡é“¾æ¥ï¼Œè·³è¿‡
                continue

            # æ£€æŸ¥æ˜¯å¦åªåŒ…å«å›¾ç‰‡æ‰©å±•åï¼ˆå¸¸è§çš„å›¾ç‰‡æ–‡ä»¶åï¼‰
            image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico', '.bmp']
            if any(text.lower().endswith(ext) for ext in image_extensions):
                # çœ‹èµ·æ¥åƒçº¯å›¾ç‰‡æ–‡ä»¶åï¼Œè·³è¿‡
                continue

            # è§„åˆ™2: å›¾ç‰‡+æ–‡æœ¬é“¾æ¥ï¼Œç§»é™¤å›¾ç‰‡æ ‡è®°
            # ç§»é™¤åµŒå¥—çš„å›¾ç‰‡æ ‡è®° ![alt](url)
            text = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', r'\1', text)

            # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
            text = ' '.join(text.split())

            # å¦‚æœæ¸…ç†åæ–‡æœ¬ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè·³è¿‡
            if not text or len(text) < 2:
                continue

            seen_urls.add(url)
            unique_links.append(NavigationLink(url=url, title=text))

        return unique_links

    def format_navigation_links_as_markdown(self, links: list, page_url: str = "") -> str:
        """
        å°†å¯¼èˆªé¡µé“¾æ¥æ ¼å¼åŒ–ä¸º Markdownï¼ˆç±»ä¼¼æœç´¢ç»“æœæ ¼å¼ï¼‰

        Args:
            links: extract_navigation_links() è¿”å›çš„é“¾æ¥åˆ—è¡¨
            page_url: å½“å‰é¡µé¢URLï¼ˆç”¨äºæå–åŸŸåï¼‰

        Returns:
            æ ¼å¼åŒ–çš„ Markdown æ–‡æœ¬ï¼Œé“¾æ¥æ ¼å¼ä¸º [ğŸ”—Link1 To: example.com]
        """
        from urllib.parse import urlparse

        lines = []

        # ä»URLæå–åŸŸå
        domain = ""
        if page_url:
            try:
                parsed = urlparse(page_url)
                domain = parsed.netloc
            except:
                pass

        # æ ‡é¢˜
        if domain:
            lines.append(f"# å¯¼èˆªé¡µé“¾æ¥ - {domain}")
        else:
            lines.append("# å¯¼èˆªé¡µé“¾æ¥")
        lines.append("")
        lines.append("")

        # æ ¼å¼åŒ–æ¯ä¸ªé“¾æ¥
        for idx, link in enumerate(links, start=1):
            # æå–åŸŸåç”¨äº link_id
            try:
                link_domain = urlparse(link.url).netloc
            except:
                link_domain = "unknown"

            # ç”Ÿæˆé“¾æ¥IDï¼ˆä¸æœç´¢ç»“æœæ ¼å¼ä¸€è‡´ï¼‰
            link_id = f"Link{idx} To: {link_domain}"

            # æ ¼å¼ï¼š[ğŸ”—Link1 To: example.com]
            lines.append(f"[ğŸ”—{link_id}]")
            lines.append(f"**{link.title}**")
            lines.append("")  # ç©ºè¡Œåˆ†éš”

        return "\n".join(lines)

    def build_navigation_link_mapping(self, links: list) -> dict:
        """
        æ„å»ºå¯¼èˆªé¡µé“¾æ¥IDåˆ°URLçš„æ˜ å°„ï¼ˆä¸æœç´¢ç»“æœæ ¼å¼ä¸€è‡´ï¼‰

        Args:
            links: extract_navigation_links() è¿”å›çš„é“¾æ¥åˆ—è¡¨

        Returns:
            {"Link1 To: example.com": "https://example.com/..."}
        """
        from urllib.parse import urlparse

        mapping = {}
        for idx, link in enumerate(links, start=1):
            # æå–åŸŸåç”¨äº link_id
            try:
                link_domain = urlparse(link.url).netloc
            except:
                link_domain = "unknown"

            # ç”Ÿæˆé“¾æ¥IDï¼ˆä¸æœç´¢ç»“æœæ ¼å¼ä¸€è‡´ï¼‰
            link_id = f"Link{idx} To: {link_domain}"
            mapping[link_id] = link.url

        return mapping

