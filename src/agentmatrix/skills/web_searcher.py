import asyncio
import time
import os
import json
import textwrap
import re
from typing import List, Set, Dict, Optional, Any, Deque
from collections import deque
from dataclasses import dataclass, field
from ..skills.utils import sanitize_filename

from ..core.browser.google import search_google
from ..core.browser.bing import search_bing
from ..core.browser.browser_adapter import (
    BrowserAdapter, TabHandle, PageElement, PageSnapshot, PageType
)
from ..core.browser.browser_common import TabSession, BaseCrawlerContext
from ..skills.crawler_helpers import CrawlerHelperMixin
from ..core.browser.drission_page_adapter import DrissionPageAdapter
from ..core.action import register_action

search_func = search_google

# ==========================================
# Prompt é›†ä¸­ç®¡ç†
# ==========================================

class WebSearcherPrompts:
    """Web Searcher Prompt é›†ä¸­ç®¡ç†"""

    # ==========================================
    # 1. ç« èŠ‚é€‰æ‹©
    # ==========================================

    CHAPTER_SELECTION = """You are searching for information to answer: "{question}"

Below is the table of contents for a document:

{toc_list}

[Task]
Select the chapters that are MOST LIKELY to contain information relevant to answering the question.

[Rules]
1. You can select multiple chapters
2. Be conservative - only select chapters that seem directly relevant
3. If unsure, you can select multiple chapters to be safe

[Output Format]

First, explain your reasoning (why you selected these chapters).

Then, output your selections using following format:

====ç« èŠ‚é€‰æ‹©====
ä½ é€‰æ‹©çš„ç« èŠ‚åç§°1(replace with your choice)
ä½ é€‰æ‹©çš„ç« èŠ‚åç§°2(replace with your choice)
...
====ç« èŠ‚é€‰æ‹©ç»“æŸ====

One chapter name per line. The chapter names must EXACTLY match the names shown in the TOC above."""

    CHAPTER_ERROR_HALLUCINATION = """Your selection contains chapters that don't exist in the TOC:

Invalid chapters:
{invalid_chapters}

Please select ONLY from the available chapters listed in the TOC. Try again."""

    CHAPTER_ERROR_FORMAT = """Your output format is incorrect.

Please use this EXACT format:

====ç« èŠ‚é€‰æ‹©====
ç« èŠ‚åç§°1
ç« èŠ‚åç§°2
====ç« èŠ‚é€‰æ‹©ç»“æŸ====

Make sure:
1. The markers are EXACTLY '====ç« èŠ‚é€‰æ‹©====' and '====ç« èŠ‚é€‰æ‹©ç»“æŸ===='
2. One chapter name per line
3. Chapter names EXACTLY match the TOC

Try again."""

    # ==========================================
    # 2. æ‰¹å¤„ç†
    # ==========================================

    BATCH_PROCESSING = """You are reading a document to answer: "{question}"

[Document Info]
- Title: {doc_title}
- Source URL: {url}
- Progress: Page {current_batch} of {total_batches} ({progress_pct}% complete)

[Notebook - What We Already Know]
{notebook}

[Current Page Content - Page {current_batch}]
{batch_text}

[Task]
Based on the Notebook, Current Page, AND your reading progress, provide a brief summary.

Consider your progress:
- If you're early in the document (first 20%), keep exploring even if this page is weak
- If you're late in the document (last 30%) and found nothing useful, consider skipping
- If you're in the middle, continue unless the content is completely irrelevant

Your response MUST start with ONE of these four headings:

##å¯¹é—®é¢˜çš„å›ç­”
If you can provide a clear, complete answer based on the Notebook and Current Page:
- Use this heading
- Provide your answer below
- Keep it concise but complete
- Keep key references (urls) for key information

##å€¼å¾—è®°å½•çš„ç¬”è®°
If you cannot answer yet, but found NEW and USEFUL information:
- Use this heading
- Provide a concise summary (2-5 sentences)
- Focus on facts, data, definitions, explanations
- Only extract information NOT already in Notebook
- Always include the source URL

##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
If the page doesn't contain new or useful information, but the document still shows promise:
- Use this heading
- Briefly explain why (1 sentence)
- Consider: If you're late in the document (>70%), you might want to skip

##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ
If the page is completely irrelevant to the question (navigation, ads, unrelated topics):
- Use this heading
- Explain why (1 sentence)
- Skip the rest of this document
- Especially consider this if you're already deep into the document (>50%) and found nothing useful

[Output Format]

##å¯¹é—®é¢˜çš„å›ç­” (or one of the other three headings)

Your content here...

[Important]
- Start with ONE of the four headings above (EXACTLY as shown)
- Provide your content below the heading
- Consider your reading progress when deciding whether to continue or skip"""

    BATCH_ERROR_FORMAT = """Your output format is incorrect.

Please start your response with ONE of these four headings (EXACTLY as shown):

##å¯¹é—®é¢˜çš„å›ç­”
##å€¼å¾—è®°å½•çš„ç¬”è®°
##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ

Then provide your content below the heading.

Examples:

Example 1 (can answer):
##å¯¹é—®é¢˜çš„å›ç­”
Pythonè£…é¥°å™¨æ˜¯ä¸€ç§...

Example 2 (useful info):
##å€¼å¾—è®°å½•çš„ç¬”è®°
è£…é¥°å™¨ä½¿ç”¨@ç¬¦å·è¯­æ³•...

Example 3 (no new info):
##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
è¿™æ®µå†…å®¹ä»‹ç»äº†ç½‘ç«™å¯¼èˆªï¼Œä½†æ²¡æœ‰æ–°çš„æœ‰ç”¨ä¿¡æ¯ã€‚

Example 4 (irrelevant):
##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ
è¿™æ˜¯ä¸€æ®µè´­ç‰©ç½‘ç«™çš„å¹¿å‘Šå†…å®¹ï¼Œå®Œå…¨ä¸è£…é¥°å™¨æ— å…³ã€‚

Try again."""


# ==========================================
# 1. çŠ¶æ€ä¸ä¸Šä¸‹æ–‡å®šä¹‰
# ==========================================

class WebSearcherContext(BaseCrawlerContext):
    """
    Web æœç´¢ä»»åŠ¡ä¸Šä¸‹æ–‡
    ç”¨äºå›ç­”é—®é¢˜çš„æœç´¢ä»»åŠ¡ï¼Œå¸¦æœ‰"å°æœ¬æœ¬"æœºåˆ¶è®°å½•æœ‰ç”¨ä¿¡æ¯
    """

    def __init__(self, purpose: str, deadline: float, chunk_threshold: int = 5000,
                 temp_file_dir: Optional[str] = None):
        super().__init__(deadline)
        self.purpose = purpose  # æ”¹åï¼šquestion -> purpose
        self.notebook = ""
        self.chunk_threshold = chunk_threshold
        self.temp_file_dir = temp_file_dir

    def add_to_notebook(self, info: str):
        """æ·»åŠ ä¿¡æ¯åˆ°å°æœ¬æœ¬"""
        if info:
            timestamp = time.strftime("%H:%M:%S")
            self.notebook += f"\n\n[{timestamp}] {info}\n"


# ==========================================
# 2. Web Searcher æ ¸å¿ƒé€»è¾‘
# ==========================================

class WebSearcherMixin(CrawlerHelperMixin):
    """
    Web æœç´¢å™¨æŠ€èƒ½
    ç”¨äºå›ç­”é—®é¢˜çš„ç½‘ç»œæœç´¢
    """

    @register_action(
        "é’ˆå¯¹ä¸€ä¸ªé—®é¢˜ä¸Šç½‘æœç´¢ç­”æ¡ˆï¼Œæä¾›è¦è§£å†³çš„é—®é¢˜å’Œï¼ˆå¯é€‰ä½†å»ºè®®æä¾›çš„ï¼‰æœç´¢å…³é”®å­—è¯",
        param_infos={
            "purpose": "è¦å›ç­”çš„é—®é¢˜ï¼ˆæˆ–ç ”ç©¶ç›®æ ‡ï¼‰",
            "search_phrase": "å¯é€‰ï¼Œåˆå§‹æœç´¢å…³é”®è¯",
            "max_time": "å¯é€‰ï¼Œæœ€å¤§æœç´¢åˆ†é’Ÿï¼Œé»˜è®¤20",
            "max_search_pages": "å¯é€‰ï¼Œæœ€å¤§æœç´¢é¡µæ•°ï¼ˆé»˜è®¤5ï¼‰",

        }
    )
    async def web_search(
        self,
        purpose: str,
        search_phrase: str = None,
        max_time: int = 20,
        max_search_pages: int = 5,
        temp_file_dir: Optional[str] = None
    ):
        """
        [Entry Point] ä¸Šç½‘æœç´¢å›ç­”é—®é¢˜ï¼ˆæµå¼å¤„ç†ç‰ˆæœ¬ï¼‰

        Args:
            purpose: è¦å›ç­”çš„é—®é¢˜ï¼ˆæˆ–ç ”ç©¶ç›®æ ‡ï¼‰
            search_phrase: åˆå§‹æœç´¢å…³é”®è¯
            max_time: æœ€å¤§æœç´¢æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            max_search_pages: æœ€å¤§æœç´¢é¡µæ•°ï¼ˆé»˜è®¤5ï¼‰
            chunk_threshold: åˆ†æ®µé˜ˆå€¼ï¼ˆå­—ç¬¦æ•°ï¼‰
            temp_file_dir: ä¸´æ—¶æ–‡ä»¶ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        """
        # 1. å‡†å¤‡ç¯å¢ƒ
        profile_path = os.path.join(self.workspace_root, ".matrix", "browser_profile", self.name)
        download_path = os.path.join(self.current_workspace, "downloads")
        chunk_threshold = 5000

        if not search_phrase:
            resp = await self.brain.think(f"""
            ç°åœ¨æˆ‘ä»¬è¦ç ”ç©¶ä¸ªæ–°é—®é¢˜ï¼š{purpose}ï¼Œæ‰“ç®—ä¸Šç½‘æœç´¢ä¸€ä¸‹ï¼Œéœ€è¦ä½ è®¾è®¡ä¸€ä¸‹æœ€åˆé€‚çš„å…³é”®è¯æˆ–è€…å…³é”®å­—ç»„åˆã€‚è¾“å‡ºçš„æ—¶å€™å¯ä»¥å…ˆç®€å•è§£é‡Šä¸€ä¸‹è¿™ä¹ˆè®¾è®¡çš„ç†ç”±ï¼Œä½†æ˜¯æœ€åä¸€è¡Œå¿…é¡»æ˜¯ä¹Ÿåªèƒ½æ˜¯è¦æœç´¢çš„å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯è¾“å…¥åˆ°æœç´¢å¼•æ“æœç´¢æ çš„å†…å®¹ï¼‰ã€‚ä¾‹å¦‚ä½ è®¤ä¸ºåº”è¯¥æœç´¢"Keyword"ï¼Œé‚£ä¹ˆæœ€åä¸€è¡Œå°±åªèƒ½æ˜¯"Keyword"
            """)
            reply = resp['reply']
            #get last line of reply
            if '\n' in reply:
                search_phrase = reply.split('\n')[-1].strip()
        #å¦‚æœè¿˜æ˜¯æœ‰é—®é¢˜,æˆ‘ä»¬ç›´æ¥æœç´¢é—®é¢˜ï¼š
        if not search_phrase:
            search_phrase = purpose
        self.logger.info(f"ğŸ” å‡†å¤‡æœç´¢: {search_phrase}")

        self.browser = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=download_path
        )

        ctx = WebSearcherContext(
            purpose=purpose,
            deadline=time.time() + int(max_time) * 60,
            chunk_threshold=chunk_threshold,
            temp_file_dir=temp_file_dir
        )

        self.logger.info(f"ğŸ” Web Search Start: {purpose}")
        self.logger.info(f"ğŸ” Initial search phrase: {search_phrase}")
        self.logger.info(f"ğŸ” Max search pages: {max_search_pages}")

        # 2. å¯åŠ¨æµè§ˆå™¨
        await self.browser.start(headless=False)

        try:
            # 3. åˆ›å»º Tab å’Œ Session
            tab = await self.browser.get_tab()
            session = TabSession(handle=tab, current_url="")

            # 4. å¤–å±‚å¾ªç¯ï¼šé€é¡µå¤„ç†æœç´¢ç»“æœ
            for page_num in range(1, max_search_pages + 1):
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ” Fetching search results page {page_num}/{max_search_pages}")
                self.logger.info(f"{'='*60}\n")

                # 4.1 è·å–ç¬¬ page_num é¡µçš„æœç´¢ç»“æœ
                search_result = await search_func(
                    self.browser,
                    tab,
                    search_phrase,
                    max_pages=max_search_pages,
                    page=page_num  # æŒ‡å®šåªè·å–ç¬¬ page_num é¡µ
                )

                if not search_result:
                    self.logger.warning(f"âš ï¸ No results found on page {page_num}")
                    break

                # 4.2 å°† URL æ·»åŠ åˆ° pending_link_queue
                added_count = 0
                for result in search_result:
                    url = result['url']
                    if not ctx.has_visited(url):
                        session.pending_link_queue.append(url)
                        added_count += 1

                self.logger.info(f"âœ“ Added {added_count} URLs from page {page_num} to queue")

                # 4.3 è¿è¡Œ _run_search_lifecycle å¤„ç†è¿™äº› URL
                self.logger.info(f"\nğŸŒ Processing URLs from page {page_num}...")
                answer = await self._run_search_lifecycle(session, ctx)

                # 4.4 å¦‚æœæ‰¾åˆ°ç­”æ¡ˆï¼Œæå‰è¿”å›
                if answer:
                    self.logger.info(f"âœ… Found answer on page {page_num}!")
                    return f"Answer: {answer}\n\n---\nNotebook:\n{ctx.notebook}"

                # 4.5 æ£€æŸ¥æ—¶é—´å’Œèµ„æºé™åˆ¶
                if ctx.is_time_up():
                    self.logger.info("â° Time up!")
                    break

                self.logger.info(f"âœ“ Completed page {page_num}, continuing to next page...")

            # 5. æœªæ‰¾åˆ°ç­”æ¡ˆï¼Œè¿”å› notebook
            self.logger.info("â¸ Exhausted all search pages without finding complete answer")
            return f"Could not find a complete answer.\n\nHere's what I found:\n{ctx.notebook}"

        except Exception as e:
            self.logger.exception("Web searcher crashed")
            return f"Search failed with error: {e}"
        finally:
            self.logger.info("ğŸ›‘ Closing browser...")
            await self.browser.close()

    @register_action(
        "è®¿é—®ä¸€ä¸ªç½‘é¡µå¹¶æŸ¥çœ‹ç½‘é¡µå†…å®¹ï¼Œå¦‚æœæ˜¯pdfæ–‡ä»¶å°±ä¸‹è½½",
        param_infos={
            "url": "è¦è®¿é—®çš„ç½‘é¡µ URL"
        }
    )
    async def visit_url(self,url: str):
        # 1. å‡†å¤‡ç¯å¢ƒ
        profile_path = os.path.join(self.workspace_root, ".matrix", "browser_profile", self.name)
        download_path = os.path.join(self.current_workspace, "downloads")


        
        self.logger.info(f"ğŸ” å‡†å¤‡è®¿é—®: {url}")

        self.browser = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=download_path
        )
        await self.browser.start(headless=False)

        tab = await self.browser.get_tab()
        

        nav_report = await self.browser.navigate(tab, url)
        final_url = self.browser.get_tab_url(tab)
        

        # === Phase 2: Identify Page Type ===
        page_type = await self.browser.analyze_page_type(tab)

        if page_type == PageType.ERRO_PAGE:
            self.logger.warning(f"ğŸš« Error Page: {final_url}")
            return f"Error Accessing Page: {url}"

        # === åˆ†æ”¯ A: é™æ€èµ„æº ===
        if page_type == PageType.STATIC_ASSET:
            self.logger.info(f"ğŸ“„ Static Asset: {final_url}")

            download_file = await self.browser.save_static_asset(tab)
            return f"æ–‡ä»¶å·²ä¸‹è½½åˆ°ï¼š {download_file}"

  

        # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ ===
        elif page_type == PageType.NAVIGABLE:
            await self.browser.stabilize(tab)
            markdown = await self._html_to_full_markdown(tab)
            
            #ç”¨markdownç¬¬ä¸€è¡Œä½œä¸ºæ–‡ä»¶åå­—
            filename = markdown.split('\n')[0].strip().replace("#","")
            filename = sanitize_filename(filename) + ".md"

            #æŠŠmarkdownä¿å­˜ä¸ºæ–‡ä»¶
            with open(os.path.join(self.current_workspace, filename), "w", encoding="utf-8") as f:
                f.write(markdown)
            return f"ç½‘é¡µæ‘˜è¦å·²ä¿å­˜åˆ°ï¼š {filename}"


    # ==========================================
    # 2. è·å–å®Œæ•´é¡µé¢å†…å®¹
    # ==========================================

    async def _get_full_page_markdown(self, tab: TabHandle, ctx: WebSearcherContext) -> str:
        """
        è·å–å®Œæ•´é¡µé¢çš„ Markdownï¼Œæ— å­—ç¬¦é™åˆ¶
        - HTML: ä½¿ç”¨ trafilatura æå–å®Œæ•´ Markdown
        - PDF: ä½¿ç”¨ pdf_to_markdown è½¬æ¢å®Œæ•´æ–‡æ¡£
        """
        content_type = await self.browser.analyze_page_type(tab)

        if content_type == PageType.STATIC_ASSET:
            return await self._pdf_to_full_markdown(tab, ctx)
        else:
            return await self._html_to_full_markdown(tab)

    async def _html_to_full_markdown(self, tab: TabHandle) -> str:
        """å°† HTML é¡µé¢è½¬æ¢ä¸ºå®Œæ•´ Markdown"""
        import trafilatura

        raw_html = tab.html
        url = self.browser.get_tab_url(tab)

        # ä½¿ç”¨ trafilatura æå–å®Œæ•´ Markdown
        markdown = trafilatura.extract(
            raw_html,
            include_links=True,
            include_formatting=True,
            output_format='markdown',
            url=url
        )

        # å¤‡é€‰æ–¹æ¡ˆ
        if not markdown or len(markdown) < 50:
            markdown = tab.text

        return markdown or ""

    async def _pdf_to_full_markdown(self, tab: TabHandle, ctx: WebSearcherContext) -> str:
        """å°† PDF è½¬æ¢ä¸ºå®Œæ•´ Markdownï¼ˆç‹¬ç«‹å®ç°ï¼Œä¾¿äºåç»­ä¼˜åŒ–ï¼‰"""
        from skills.report_writer_utils import pdf_to_markdown

        # ä¸‹è½½ PDF åˆ°æœ¬åœ°
        pdf_path = await self.browser.save_static_asset(tab)

        # è½¬æ¢å®Œæ•´ PDF ä¸º Markdown
        markdown = pdf_to_markdown(pdf_path)

        # å¯é€‰ï¼šä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if ctx.temp_file_dir:
            import os
            from slugify import slugify
            os.makedirs(ctx.temp_file_dir, exist_ok=True)
            filename = slugify(f"pdf_{os.path.basename(pdf_path)}") + ".md"
            temp_path = os.path.join(ctx.temp_file_dir, filename)
            with open(temp_path, "w", encoding="utf-8") as f:
                f.write(markdown)
            self.logger.info(f"ğŸ“„ Saved markdown to: {temp_path}")

        return markdown

    # ==========================================
    # 3. è¾…åŠ©æ–¹æ³•ï¼ˆç›®å½•ã€é€‰æ‹©ç« èŠ‚ã€åˆ†æ®µï¼‰
    # ==========================================

    def _generate_document_toc(self, markdown: str) -> List[Dict[str, Any]]:
        """
        ä» Markdown ä¸­æå–ç›®å½•ç»“æ„
        è¿”å›: [
            {"level": 1, "title": "ç¬¬ä¸€ç« ", "start": 0, "end": 1234},
            {"level": 2, "title": "1.1 ç®€ä»‹", "start": 1235, "end": 2345},
            ...
        ]
        """
        toc = []
        lines = markdown.split("\n")
        current_pos = 0

        for line in lines:
            # åŒ¹é… Markdown æ ‡é¢˜
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                toc.append({
                    "level": level,
                    "title": title,
                    "start": current_pos,
                    "line": line
                })

            current_pos += len(line) + 1  # +1 for newline

        # è®¡ç®—æ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
        for i in range(len(toc) - 1):
            toc[i]["end"] = toc[i + 1]["start"]
        if toc:
            toc[-1]["end"] = len(markdown)

        return toc

    async def _let_llm_select_chapters(
        self,
        toc: List[Dict],
        ctx: WebSearcherContext
    ) -> List[int]:
        """
        è®© LLM æ ¹æ®é—®é¢˜é€‰æ‹©ç›¸å…³ç« èŠ‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        è¿”å›: é€‰ä¸­çš„ç« èŠ‚ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰
        """
        # æ„é€  TOC åˆ—è¡¨ï¼ˆä¸ç”¨æ•°å­—ç¼–å·ï¼Œä¿ç•™ç¼©è¿›ï¼‰
        toc_lines = []
        for chapter in toc:
            indent = "  " * (chapter["level"] - 1)
            toc_lines.append(f"{indent}{chapter['title']}")
        toc_list = "\n".join(toc_lines)

        # æ„é€ ç« èŠ‚åå­—åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºéªŒè¯ï¼‰
        chapter_name_to_index = {
            chapter["title"]: i
            for i, chapter in enumerate(toc)
        }

        # ä½¿ç”¨ prompt æ¨¡æ¿
        initial_prompt = WebSearcherPrompts.CHAPTER_SELECTION.format(
            question=ctx.purpose,
            toc_list=toc_list
        )

        # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "user", "content": initial_prompt}]

        # æœ€å¤§é‡è¯•æ¬¡æ•°
        MAX_RETRIES = 5

        for attempt in range(MAX_RETRIES):
            try:
                # è°ƒç”¨ LLM
                response = await self.cerebellum.backend.think(messages=messages)
                reply = response.get('reply', '').strip()

                self.logger.debug(f"Chapter selection attempt {attempt + 1}:\n{reply}")

                # å°† LLM çš„å›å¤ä½œä¸º assistant æ¶ˆæ¯åŠ å…¥å†å²
                messages.append({"role": "assistant", "content": reply})

                # è§£æè¾“å‡º
                result = self._parse_chapter_selection(reply, chapter_name_to_index)

                if result["status"] == "success":
                    # æƒ…å†µ (1): è§£ææˆåŠŸï¼Œæ‰€æœ‰ç« èŠ‚éƒ½æ˜¯çœŸçš„
                    selected_indices = result["selected_indices"]
                    self.logger.info(f"âœ… Successfully selected {len(selected_indices)} chapters: {selected_indices}")
                    return selected_indices

                elif result["status"] == "hallucination":
                    # æƒ…å†µ (2): è§£ææˆåŠŸï¼Œä½†æœ‰äº›ç« èŠ‚æ˜¯å‡çš„ï¼ˆå¹»è§‰ï¼‰
                    invalid_chapters = result["invalid_chapters"]
                    self.logger.warning(f"âš ï¸ LLM hallucinated chapters: {invalid_chapters}")

                    # ä½¿ç”¨é”™è¯¯æç¤ºæ¨¡æ¿
                    invalid_chapters_str = "\n".join(f"- {ch}" for ch in invalid_chapters)
                    error_msg = WebSearcherPrompts.CHAPTER_ERROR_HALLUCINATION.format(
                        invalid_chapters=invalid_chapters_str
                    )

                    messages.append({"role": "user", "content": error_msg})
                    continue

                else:  # result["status"] == "parse_error"
                    # æƒ…å†µ (3): è§£æå¤±è´¥ï¼ˆæ ¼å¼ä¸å¯¹ï¼‰
                    self.logger.warning(f"âš ï¸ LLM output format incorrect")

                    # ä½¿ç”¨é”™è¯¯æç¤ºæ¨¡æ¿
                    error_msg = WebSearcherPrompts.CHAPTER_ERROR_FORMAT

                    messages.append({"role": "user", "content": error_msg})
                    continue

            except Exception as e:
                self.logger.error(f"Chapter selection failed: {e}")

                if attempt < MAX_RETRIES - 1:
                    messages.append({
                        "role": "user",
                        "content": "An error occurred. Please try again. Make sure to follow the output format exactly."
                    })
                    continue
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                    return []

        # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆä¼šè§¦å‘å…¨æ–‡å¤„ç†ï¼‰
        self.logger.error(f"âŒ Max retries ({MAX_RETRIES}) exceeded. Falling back to full text processing.")
        return []

    def _parse_chapter_selection(
        self,
        llm_output: str,
        chapter_name_to_index: Dict[str, int]
    ) -> Dict[str, Any]:
        """
        è§£æ LLM çš„ç« èŠ‚é€‰æ‹©è¾“å‡º

        è¿”å›:
        {
            "status": "success" | "hallucination" | "parse_error",
            "selected_indices": List[int],      # å¦‚æœæˆåŠŸ
            "invalid_chapters": List[str]       # å¦‚æœæœ‰å¹»è§‰
        }
        """
        # æŸ¥æ‰¾åˆ†éš”ç¬¦
        start_marker = "====ç« èŠ‚é€‰æ‹©===="
        end_marker = "====ç« èŠ‚é€‰æ‹©ç»“æŸ===="

        start_idx = llm_output.find(start_marker)
        end_idx = llm_output.find(end_marker)

        # æ£€æŸ¥åˆ†éš”ç¬¦æ˜¯å¦å­˜åœ¨
        if start_idx == -1 or end_idx == -1:
            return {"status": "parse_error", "selected_indices": [], "invalid_chapters": []}

        # æå–ç« èŠ‚åˆ—è¡¨éƒ¨åˆ†
        start_idx += len(start_marker)
        chapter_section = llm_output[start_idx:end_idx].strip()

        # æŒ‰è¡Œåˆ†å‰²
        chapter_lines = [
            line.strip()
            for line in chapter_section.split('\n')
            if line.strip()
        ]

        if not chapter_lines:
            return {"status": "parse_error", "selected_indices": [], "invalid_chapters": []}

        # éªŒè¯ç« èŠ‚æ˜¯å¦å­˜åœ¨äº TOC ä¸­
        selected_indices = []
        invalid_chapters = []

        for chapter_name in chapter_lines:
            if chapter_name in chapter_name_to_index:
                selected_indices.append(chapter_name_to_index[chapter_name])
            else:
                invalid_chapters.append(chapter_name)

        # åˆ¤æ–­ç»“æœ
        if invalid_chapters:
            # æœ‰å¹»è§‰
            return {
                "status": "hallucination",
                "selected_indices": selected_indices,
                "invalid_chapters": invalid_chapters
            }
        elif selected_indices:
            # æˆåŠŸ
            return {
                "status": "success",
                "selected_indices": selected_indices,
                "invalid_chapters": []
            }
        else:
            # æ²¡æœ‰é€‰ä¸­ä»»ä½•ç« èŠ‚ï¼ˆä¹Ÿå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯ï¼‰
            return {
                "status": "parse_error",
                "selected_indices": [],
                "invalid_chapters": []
            }

    def _split_by_paragraph_boundaries(
        self,
        text: str,
        threshold: int
    ) -> List[str]:
        """
        æŒ‰æ®µè½è¾¹ç•Œå°†æ–‡æœ¬åˆ†æ®µï¼Œæ¯æ®µ â‰¤ threshold

        ç­–ç•¥ï¼š
        1. æŒ‰ \\n\\n åˆ†å‰²æ®µè½
        2. é€æ­¥æ·»åŠ æ®µè½ï¼Œç›´åˆ°æ¥è¿‘é˜ˆå€¼
        3. åœ¨æœ€è¿‘çš„åŒæ¢è¡Œå¤„æ–­å¼€
        4. è¶…é•¿æ®µè½æŒ‰å¥å­ï¼ˆã€‚ï¼‰ç»†åˆ†
        """
        if len(text) <= threshold:
            return [text]

        # æŒ‰åŒæ¢è¡Œåˆ†æ®µ
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            test_chunk = current_chunk + ("\n\n" if current_chunk else "") + para

            if len(test_chunk) <= threshold:
                current_chunk = test_chunk
            else:
                # å½“å‰æ®µè½ä¼šè¶…å‡ºé˜ˆå€¼
                if current_chunk:
                    chunks.append(current_chunk)

                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é˜ˆå€¼ï¼Œå¼ºåˆ¶åœ¨ä¸­é—´æ–­å¼€
                if len(para) > threshold:
                    # æŒ‰å¥å­åˆ†å‰²
                    sentences = para.split('ã€‚')
                    temp_chunk = ""
                    for sent in sentences:
                        test_sent = temp_chunk + ('ã€‚' if temp_chunk else '') + sent
                        if len(test_sent) <= threshold:
                            temp_chunk = test_sent
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk)
                            temp_chunk = sent
                    current_chunk = temp_chunk
                else:
                    current_chunk = para

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    # ==========================================
    # 4. æ ¸å¿ƒæµå¼å¤„ç†
    # ==========================================

    async def _process_batch(
        self,
        batch_text: str,
        ctx: WebSearcherContext,
        doc_title: str,
        current_batch: int,
        total_batches: int,
        url
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ‰¹å¤„ç†å‡½æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        å‚æ•°:
            batch_text: å½“å‰æ‰¹æ¬¡æ–‡æœ¬
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            doc_title: æ–‡æ¡£åç§°
            current_batch: å½“å‰æ‰¹æ¬¡ï¼ˆé¡µç ï¼Œä» 1 å¼€å§‹ï¼‰
            total_batches: æ€»æ‰¹æ¬¡æ•°ï¼ˆæ€»é¡µæ•°ï¼‰

        è¿”å›:
        {
            "heading_type": "answer" | "note" | "continue" | "skip_doc",
            "content": str
        }
        """
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_pct = int((current_batch / total_batches) * 100)

        # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
        messages = [
            {
                "role": "user",
                "content": WebSearcherPrompts.BATCH_PROCESSING.format(
                    question=ctx.purpose,
                    doc_title=doc_title,
                    current_batch=current_batch,
                    total_batches=total_batches,
                    progress_pct=progress_pct,
                    notebook=ctx.notebook,
                    batch_text=batch_text,
                    url=url
                )
            }
        ]

        # æœ€å¤§é‡è¯•æ¬¡æ•°
        MAX_RETRIES = 5

        for attempt in range(MAX_RETRIES):
            try:
                # è°ƒç”¨ LLM
                response = await self.cerebellum.backend.think(messages=messages)
                reply = response.get('reply', '').strip()

                self.logger.debug(f"Batch processing attempt {attempt + 1}:\n{reply}")

                # å°† LLM çš„å›å¤ä½œä¸º assistant æ¶ˆæ¯åŠ å…¥å†å²
                messages.append({"role": "assistant", "content": reply})

                # è§£æè¾“å‡º
                result = self._parse_batch_output(reply)

                if result["status"] == "success":
                    # æˆåŠŸ
                    heading_type = result["heading_type"]
                    content = result["content"]

                    # æ ¹æ®ç±»å‹è®°å½•æ—¥å¿—
                    if heading_type == "answer":
                        self.logger.info(f"âœ… Found answer in batch")
                    elif heading_type == "note":
                        self.logger.info(f"ğŸ“ Found useful info in batch")
                    elif heading_type == "continue":
                        self.logger.debug(f"ğŸ‘€ No new info, continuing")
                    else:  # skip_doc
                        self.logger.warning(f"ğŸš« Document irrelevant, skipping")

                    return {
                        "heading_type": heading_type,
                        "content": content
                    }

                else:  # result["status"] == "parse_error"
                    # æ ¼å¼é”™è¯¯
                    self.logger.warning(f"âš ï¸ LLM output format incorrect")

                    error_msg = WebSearcherPrompts.BATCH_ERROR_FORMAT
                    messages.append({"role": "user", "content": error_msg})
                    continue

            except Exception as e:
                self.logger.error(f"Batch processing failed: {e}")

                if attempt < MAX_RETRIES - 1:
                    messages.append({
                        "role": "user",
                        "content": "An error occurred. Please try again. Make sure to start with one of the four headings."
                    })
                    continue
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
                    return {"heading_type": "continue", "content": ""}

        # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆç»§ç»­é˜…è¯»ï¼‰
        self.logger.error(f"âŒ Max retries ({MAX_RETRIES}) exceeded. Defaulting to 'continue'.")
        return {"heading_type": "continue", "content": ""}

    def _parse_batch_output(self, llm_output: str) -> Dict[str, Any]:
        """
        è§£æ LLM çš„æ‰¹å¤„ç†è¾“å‡º

        è¿”å›:
        {
            "status": "success" | "parse_error",
            "heading_type": "answer" | "note" | "continue" | "skip_doc",
            "content": str
        }
        """
        # å®šä¹‰å››ç§æ ‡é¢˜
        HEADINGS = {
            "##å¯¹é—®é¢˜çš„å›ç­”": "answer",
            "##å€¼å¾—è®°å½•çš„ç¬”è®°": "note",
            "##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»": "continue",
            "##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ": "skip_doc"
        }

        # æ£€æŸ¥è¾“å‡ºä»¥å“ªä¸ªæ ‡é¢˜å¼€å¤´
        heading_type = None
        heading_used = None

        for heading, htype in HEADINGS.items():
            if llm_output.startswith(heading):
                heading_type = htype
                heading_used = heading
                break

        if heading_type is None:
            # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡é¢˜
            return {"status": "parse_error", "heading_type": None, "content": ""}

        # æå–æ ‡é¢˜ä¸‹é¢çš„å†…å®¹
        content_start = len(heading_used)
        content = llm_output[content_start:].strip()

        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œä¹Ÿç®—è§£æé”™è¯¯
        if not content:
            return {"status": "parse_error", "heading_type": None, "content": ""}

        # æˆåŠŸ
        return {
            "status": "success",
            "heading_type": heading_type,
            "content": content
        }

    def _extract_document_title(self, markdown: str) -> str:
        """
        ä» Markdown ä¸­æå–æ–‡æ¡£æ ‡é¢˜
        ä¼˜å…ˆçº§ï¼šç¬¬ä¸€ä¸ª # æ ‡é¢˜ > å‰ 50 å­—ç¬¦ > "æœªå‘½åæ–‡æ¡£"
        """
        # 1. å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª # æ ‡é¢˜
        lines = markdown.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()

        # 2. å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨å‰ 50 å­—ç¬¦ä½œä¸ºæ ‡é¢˜
        if len(markdown) > 50:
            return markdown[:50].strip()

        # 3. é»˜è®¤æ ‡é¢˜
        return "æœªå‘½åæ–‡æ¡£"

    async def _stream_process_markdown(
        self,
        markdown: str,
        ctx: WebSearcherContext,
        url: str
    ) -> Optional[str]:
        """
        æµå¼å¤„ç† Markdown æ–‡æ¡£ï¼ˆç»Ÿä¸€å…¥å£ï¼‰

        æµç¨‹ï¼š
        1. åˆ¤æ–­é•¿åº¦ â†’ å†³å®šæ˜¯å¦éœ€è¦é€‰ç« èŠ‚
        2. å‡†å¤‡å¾…å¤„ç†å†…å®¹ï¼ˆå…¨æ–‡ OR é€‰ä¸­ç« èŠ‚ï¼‰
        3. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        4. é€æ‰¹æµå¼å¤„ç†
        """
        # 1. åˆ¤æ–­é•¿åº¦
        is_long = len(markdown) > ctx.chunk_threshold

        # 2. å‡†å¤‡å¾…å¤„ç†å†…å®¹
        if not is_long:
            # çŸ­æ–‡æ¡£ï¼šå…¨æ–‡å¤„ç†
            self.logger.info(f"ğŸ“„ Short document ({len(markdown)} chars). Processing full text.")
            content_to_process = markdown
        else:
            # é•¿æ–‡æ¡£ï¼šç”Ÿæˆç›®å½• â†’ é€‰æ‹©ç« èŠ‚
            self.logger.info(f"ğŸ“š Long document ({len(markdown)} chars). Generating TOC...")
            toc = self._generate_document_toc(markdown)

            if not toc or len(toc)<2:
                # æ— æ ‡é¢˜ç»“æ„æˆ–è€…åªæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå…¨æ–‡å¤„ç†
                self.logger.info("ğŸ“‹ No headers found. Processing full text.")
                content_to_process = markdown
            else:
                # è®© LLM é€‰æ‹©ç« èŠ‚
                self.logger.info(f"ğŸ“‘ Found {len(toc)} chapters. Asking LLM to select...")
                selected_indices = await self._let_llm_select_chapters(toc, ctx)

                if not selected_indices:
                    self.logger.warning("âš ï¸ No chapters selected. Processing full text.")
                    content_to_process = markdown
                else:
                    self.logger.info(f"âœ… Selected {len(selected_indices)} chapters")
                    # æå–é€‰ä¸­ç« èŠ‚
                    selected_parts = []
                    for idx in selected_indices:
                        chapter = toc[idx]
                        content = markdown[chapter["start"]:chapter["end"]]
                        selected_parts.append(f"# {chapter['title']}\n\n{content}")
                    content_to_process = "\n\n".join(selected_parts)

        # 3. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        self.logger.info(f"ğŸ”ª Splitting content into batches (max {ctx.chunk_threshold} chars each)...")
        batches = self._split_by_paragraph_boundaries(content_to_process, ctx.chunk_threshold)
        total_batches = len(batches)
        self.logger.info(f"ğŸ“Š Split into {total_batches} batches")

        # 4. è·å–æ–‡æ¡£æ ‡é¢˜ï¼ˆç”¨äº LLM ä¸Šä¸‹æ–‡ï¼‰
        doc_title = self._extract_document_title(content_to_process)

        # 5. é€æ‰¹æµå¼å¤„ç†
        for i, batch in enumerate(batches, start=1):  # ä» 1 å¼€å§‹è®¡æ•°
            current_batch = i
            progress_pct = int((current_batch / total_batches) * 100)
            self.logger.info(
                f"ğŸ”„ Processing batch {current_batch}/{total_batches} "
                f"({progress_pct}%, {len(batch)} chars)..."
            )

            # ç»Ÿä¸€çš„æ‰¹å¤„ç†ï¼ˆä¼ å…¥è¿›åº¦ä¿¡æ¯ï¼‰
            result = await self._process_batch(
                batch,
                ctx,
                doc_title=doc_title,
                current_batch=current_batch,
                total_batches=total_batches,
                url=url
            )

            # å¤„ç†ç»“æœ
            if result["heading_type"] == "answer":
                # æ‰¾åˆ°ç­”æ¡ˆï¼Œç«‹å³è¿”å›
                self.logger.info(f"âœ… Answer found in batch {current_batch}!")
                return result["content"]

            elif result["heading_type"] == "note":
                # æœ‰ç”¨ä¿¡æ¯ï¼Œæ·»åŠ åˆ°å°æœ¬æœ¬
                ctx.add_to_notebook(f"[Batch {current_batch}] {result['content']}")
                self.logger.info(f"ğŸ“ Added useful info from batch {current_batch}")

            elif result["heading_type"] == "skip_doc":
                # æ–‡æ¡£ä¸ç›¸å…³ï¼Œæ”¾å¼ƒæ•´ä¸ªæ–‡æ¡£
                self.logger.warning(f"ğŸš« Document irrelevant. Skipping rest of document.")
                break

            # heading_type == "continue": ä»€ä¹ˆéƒ½ä¸åšï¼Œç»§ç»­ä¸‹ä¸€æ‰¹

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    async def _run_search_lifecycle(self, session: TabSession, ctx: WebSearcherContext) -> Optional[str]:
        """
        [The Core Loop] æœç´¢ç”Ÿå‘½å‘¨æœŸ
        æ ¸å¿ƒé€»è¾‘ï¼šè®¿é—®é¡µé¢ â†’ å°è¯•å›ç­”é—®é¢˜ â†’ ä¸èƒ½å›ç­”åˆ™è®°å½•ä¿¡æ¯ â†’ ç»§ç»­æ¢ç´¢
        """
        while not ctx.is_time_up():
            # --- Phase 1: Navigation ---
            if not session.pending_link_queue:
                self.logger.info("Queue empty. Ending search.")
                break

            next_url = session.pending_link_queue.popleft()
            self.logger.info(f"ğŸ”— Navigating to: {next_url}")

            # 1.1 é—¨ç¦æ£€æŸ¥
            if ctx.has_visited(next_url) or any(bl in next_url for bl in ctx.blacklist):
                continue

            # 1.2 å¯¼èˆªåˆ°é¡µé¢
            nav_report = await self.browser.navigate(session.handle, next_url)
            final_url = self.browser.get_tab_url(session.handle)
            session.current_url = final_url

            ctx.mark_visited(next_url)
            ctx.mark_visited(final_url)

            # 1.3 äºŒæ¬¡é»‘åå•æ£€æŸ¥
            if any(bl in final_url for bl in ctx.blacklist):
                self.logger.warning(f"ğŸš« Redirected to blacklisted URL: {final_url}")
                continue

            # === Phase 2: Identify Page Type ===
            page_type = await self.browser.analyze_page_type(session.handle)

            if page_type == PageType.ERRO_PAGE:
                self.logger.warning(f"ğŸš« Error Page: {final_url}")
                continue

            # === åˆ†æ”¯ A: é™æ€èµ„æº ===
            if page_type == PageType.STATIC_ASSET:
                self.logger.info(f"ğŸ“„ Static Asset: {final_url}")

                # è·å–å®Œæ•´ Markdown å¹¶æµå¼å¤„ç†
                markdown = await self._get_full_page_markdown(session.handle, ctx)
                answer = await self._stream_process_markdown(markdown, ctx,final_url)

                if answer:
                    return answer  # æ‰¾åˆ°ç­”æ¡ˆï¼Œç›´æ¥è¿”å›

                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª URL

            # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ ===
            elif page_type == PageType.NAVIGABLE:
                self.logger.debug("ğŸŒ Navigable Page. Entering processing loop.")
                page_active = True
                page_changed = True

                while page_active and not ctx.is_time_up():
                    # 1. Stabilize (æ»šåŠ¨åŠ è½½)
                    if page_changed:
                        await self.browser.stabilize(session.handle)

                        # 2. è·å–å®Œæ•´ Markdown å¹¶æµå¼å¤„ç†
                        markdown = await self._get_full_page_markdown(session.handle, ctx)
                        answer = await self._stream_process_markdown(markdown, ctx,final_url)

                        # 3. å¦‚æœæ‰¾åˆ°ç­”æ¡ˆï¼Œç›´æ¥è¿”å›
                        if answer:
                            return answer

                        # 4. ç”Ÿæˆé¡µé¢æ‘˜è¦ï¼ˆç”¨äºåç»­é“¾æ¥ç­›é€‰ï¼‰
                        page_summary = markdown[:500] if markdown else ""

                    # === Phase 4: Scouting ===
                    links, buttons = await self.browser.scan_elements(session.handle)
                    self.logger.debug(f"ğŸ” Found {len(links)} links and {len(buttons)} buttons")

                    # 4.1 å¤„ç† Links
                    if page_changed:
                        filtered_links = {}
                        for link in links:
                            if ctx.has_link_assessed(link):
                                continue
                            if ctx.has_visited(link):
                                continue
                            if link in session.pending_link_queue:
                                continue
                            if any(bl in link for bl in ctx.blacklist):
                                continue
                            filtered_links[link] = links[link]

                        # è¯„ä¼°é“¾æ¥ç›¸å…³æ€§
                        selected_links = await self._filter_relevant_links(filtered_links, page_summary, ctx)

                        # æ ‡è®°å·²è¯„ä¼°
                        for link in filtered_links:
                            ctx.mark_link_assessed(link)

                        # æ·»åŠ åˆ°é˜Ÿåˆ—
                        new_links_count = 0
                        for link in selected_links:
                            session.pending_link_queue.append(link)
                            new_links_count += 1
                        self.logger.info(f"ğŸ‘€ Added {new_links_count} links to queue")

                    # 4.2 å¤„ç† Buttons
                    candidate_buttons = []
                    for button_text in buttons:
                        if not ctx.has_button_assessed(session.current_url, button_text):
                            candidate_buttons.append({button_text: buttons[button_text]})

                    # === Phase 5: Execution ===
                    if not candidate_buttons:
                        self.logger.info("ğŸ¤” No worthy buttons. Moving to next page.")
                        page_active = False
                        continue

                    chosen_button = await self._choose_best_interaction(candidate_buttons, page_summary, ctx)

                    # æ ‡è®°å·²è¯„ä¼°
                    assessed_button_texts = [list(btn.keys())[0] for btn in candidate_buttons]
                    ctx.mark_buttons_assessed(session.current_url, assessed_button_texts)

                    if not chosen_button:
                        self.logger.info("ğŸ¤” No worthy buttons. Moving to next page.")
                        page_active = False
                        continue

                    # æ‰§è¡Œç‚¹å‡»
                    self.logger.info(f"ğŸ–±ï¸ Clicking: [{chosen_button.get_text()}]")
                    ctx.mark_interacted(session.current_url, chosen_button.get_text())

                    report = await self.browser.click_and_observe(session.handle, chosen_button)

                    # 5.1 å¤„ç†æ–° Tab
                    if report.new_tabs:
                        self.logger.info(f"âœ¨ New Tab(s): {len(report.new_tabs)}")
                        for new_tab_handle in report.new_tabs:
                            new_session = TabSession(handle=new_tab_handle, current_url="", depth=session.depth + 1)
                            answer = await self._run_search_lifecycle(new_session, ctx)
                            if answer:  # å¦‚æœåœ¨é€’å½’ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œå‘ä¸Šä¼ é€’
                                return answer
                            await self.browser.close_tab(new_tab_handle)

                    # 5.2 å¤„ç†é¡µé¢å˜åŠ¨
                    if report.is_dom_changed or report.is_url_changed:
                        self.logger.info("ğŸ”„ Page changed. Re-assessing.")
                        page_changed = True
                        if report.is_url_changed:
                            session.current_url = self.browser.get_tab_url(session.handle)
                        continue

                    # 5.3 æ— å˜åŒ–
                    page_changed = False
                    continue

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    # ==========================================
    # 3. å°è„‘å†³ç­–è¾…åŠ©
    # ==========================================

