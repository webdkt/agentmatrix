import asyncio
import time
import os
import json
import textwrap
import re
from typing import List, Set, Dict, Optional, Any, Deque
from collections import deque
from dataclasses import dataclass, field
from ..skills.utils import sanitize_filename
from ..skills.parser_utils import multi_section_parser

from urllib.parse import quote_plus, urlparse

from ..core.browser.browser_adapter import (
    BrowserAdapter, TabHandle, PageElement, PageSnapshot, PageType
)
from ..core.browser.browser_common import TabSession, BaseCrawlerContext
from ..skills.crawler_helpers import CrawlerHelperMixin
from ..core.browser.drission_page_adapter import DrissionPageAdapter
from ..core.action import register_action

# Search engine configuration
SEARCH_ENGINES = {
    "google": "https://www.google.com/search?q={query}",
    "bing": "https://www.bing.com/search?q={query}"
}
DEFAULT_SEARCH_ENGINE = "bing"  # Can be configured via environment variable or parameter

# ==========================================
# Prompt é›†ä¸­ç®¡ç†
# ==========================================

class WebSearcherPrompts:
    """Web Searcher Prompt é›†ä¸­ç®¡ç†"""

    # ==========================================
    # 1. ç« èŠ‚é€‰æ‹©
    # ==========================================

    CHAPTER_SELECTION = """You are searching for information to answer: "{question}"

Current page URL: {url}

Below is the table of contents for a document:

{toc_list}

[Document Preview - First 500 Characters]
{preview}

[Important Note]
The table of contents above was AUTOMATICALLY EXTRACTED from the web page by analyzing heading levels.
Web pages often have messy or inconsistent heading structures. Some "chapters" may not be real content sections.

Use the preview above to help assess whether this document is worth reading.

[Task]
Select the chapters that are LIKELY to contain information relevant to answering the question.

[Rules]
1. First, assess whether this document is relevant by examining the TOC and preview
2. If the document seems irrelevant, select "SKIP_DOC" to skip the entire document
3. If the TOC looks random or unhelpful, select "SKIP_TOC" to process the full document
4. You can select multiple chapters
5. Be conservative - only select chapters that seem directly relevant
6. If you select a parent chapter, ALL its sub-chapters will be included automatically

[Output Format]

First, explain your reasoning (assess document relevance + TOC quality + why you selected these chapters).

Then, output your selections using following format:

====ç« èŠ‚é€‰æ‹©====
ä½ é€‰æ‹©çš„ç« èŠ‚åç§°1(replace with your choice)
ä½ é€‰æ‹©çš„ç« èŠ‚åç§°2(replace with your choice)
...
====ç« èŠ‚é€‰æ‹©ç»“æŸ====
One chapter name per line. The chapter names must EXACTLY match the names shown in the TOC above.

OR, if you decided è¿™ä¸ªæ–‡æ¡£ä¸ç›¸å…³:
====ç« èŠ‚é€‰æ‹©====
SKIP_DOC
====ç« èŠ‚é€‰æ‹©ç»“æŸ====

OR, if you decided è¿™ä¸ªç›®å½•æ²¡ä»€ä¹ˆç”¨:
====ç« èŠ‚é€‰æ‹©====
SKIP_TOC
====ç« èŠ‚é€‰æ‹©ç»“æŸ====

"""

    

    CHAPTER_ERROR_FORMAT = """Your output format is incorrect.

Please use this EXACT format:

====ç« èŠ‚é€‰æ‹©====
ç« èŠ‚åç§°1(replace with your choice)
ç« èŠ‚åç§°2(replace with your choice)
====ç« èŠ‚é€‰æ‹©ç»“æŸ====

Make sure:
1. The markers are EXACTLY '====ç« èŠ‚é€‰æ‹©====' and '====ç« èŠ‚é€‰æ‹©ç»“æŸ===='
2. One chapter name per line
3. Chapter names EXACTLY match the TOC

Try again."""

    # ==========================================
    # 2. æ‰¹å¤„ç†
    # ==========================================

    BATCH_PROCESSING = """You are reading a document to answer: "{question}"

[Document Info]
- Title: {doc_title}
- Source URL: {url}
- Progress: Page {current_batch} of {total_batches} ({progress_pct}% complete)

{toc_info}

[Notebook - What We Already Know]
{notebook}

[Current Page Content - Page {current_batch}]
(Note: Links are marked as [ğŸ”—LinkText]. If you want to visit a link, copy its text (without ğŸ”—) and list it in the "æ¨èé“¾æ¥" section below.)
====BEGIN OF PAGE====

{batch_text}

====END OF PAGE====

[Task]
Based on the Notebook, Current Page, AND your reading progress, provide a brief summary.

Consider your progress:
- How deep are you in the document?
- Does this document seem relevant to the question?
- Based on document info and toc and current page, is this document likely to contain useful information?

Your response MUST start with ONE of these four headings:

##å¯¹é—®é¢˜çš„å›ç­”
If you can provide a clear, complete answer based on the Notebook and Current Page:
- Use this heading
- Provide your answer below
- Keep it concise but complete
- Keep key references (urls) for key information

##å€¼å¾—è®°å½•çš„ç¬”è®°
If you cannot answer yet, but found NEW and USEFUL information:
- Use this heading
- è®°å½•ä¸‹æœ‰ä»·å€¼çš„ä¿¡æ¯åˆ°å°æœ¬æœ¬ï¼Œä¿æŒç²¾ç®€æ¦‚è¦
- Focus on facts, data, definitions, explanations
- Only extract information NOT already in Notebook
- Always include the source URL

##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
If the page doesn't contain new or useful information, but the document still shows promise:
- Use this heading
- Briefly explain why (1 sentence)


##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ
If the page is completely irrelevant to the question (navigation, ads, unrelated topics) and you believe the document is not worth reading further:
- Use this heading
- Explain why (1 sentence)
- Skip the rest of this document


[Output Format]

##å¯¹é—®é¢˜çš„å›ç­” (or one of the other three headings)

Your content here...

====æ¨èé“¾æ¥====
(Optional. If you see links in the text above that are worth visiting, list the LinkText (without ğŸ”—) of those links here, one per line.)
ç¤ºä¾‹é“¾æ¥æ–‡æœ¬
å¦ä¸€ä¸ªé“¾æ¥æ–‡æœ¬
====æ¨èé“¾æ¥ç»“æŸ====

"""

    BATCH_ERROR_FORMAT = """Your output format is incorrect.

Please start your response with ONE of these four headings (EXACTLY as shown):

##å¯¹é—®é¢˜çš„å›ç­”
##å€¼å¾—è®°å½•çš„ç¬”è®°
##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ

Then provide your content below the heading.

Examples:

Example 1 (can answer):
##å¯¹é—®é¢˜çš„å›ç­”
Pythonè£…é¥°å™¨æ˜¯ä¸€ç§...

Example 2 (useful info):
##å€¼å¾—è®°å½•çš„ç¬”è®°
è£…é¥°å™¨ä½¿ç”¨@ç¬¦å·è¯­æ³•...

Example 3 (no new info):
##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»
è¿™æ®µå†…å®¹ä»‹ç»äº†ç½‘ç«™å¯¼èˆªï¼Œä½†æ²¡æœ‰æ–°çš„æœ‰ç”¨ä¿¡æ¯ã€‚

Example 4 (irrelevant):
##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ
è¿™æ˜¯ä¸€æ®µè´­ç‰©ç½‘ç«™çš„å¹¿å‘Šå†…å®¹ï¼Œå®Œå…¨ä¸è£…é¥°å™¨æ— å…³ã€‚

Try again."""

    # ==========================================
    # 3. æœç´¢ç»“æœé¡µå¤„ç†
    # ==========================================

    SEARCH_RESULT_BATCH = """You are reviewing SEARCH RESULTS from {search_engine} to answer: "{question}"



[Notebook - What We Already Know]
{notebook}

(Note: Links are marked as [ğŸ”—LinkText]. If you want to visit a link, copy its text (without ğŸ”—) and list it in the "##æ¨èé“¾æ¥" section.)
====BEGIN OF SEARCH RESULTS====

{batch_text}

====END OF SEARCH RESULTS====

[Task]
Review these search results and decide which links are worth visiting to help answer the question.

[Important Rules]
1. If NONE of the search results are relevant to the question, select "SKIP_PAGE" to skip this batch
2. Be SELECTIVE - only choose links that have HIGH probability of containing useful information
3. It's OK to skip if the results don't seem helpful

[Output Format]

If you found relevant links:
##æ¨èé“¾æ¥
ç¬¬ä¸€ä¸ªé“¾æ¥(replace with your choice)
å¦ä¸€ä¸ªé“¾æ¥(replace with your choice)

If NONE of the results are relevant:
##æ¨èé“¾æ¥
SKIP_PAGE
"""

    # ==========================================
    # 4. å¯¼èˆªé¡µå¤„ç†
    # ==========================================

    NAVIGATION_PAGE = """You are reviewing a NAVIGATION PAGE to answer: "{question}"


[Notebook - What We Already Know]
{notebook}

(Note: Links are marked as [ğŸ”—LinkText]. If you want to visit a link, copy its text (without ğŸ”—) and list it in the "##æ¨èé“¾æ¥" section.)
====BEGIN OF NAVIGATION PAGE====

{navigation_text}

====END OF NAVIGATION PAGE====

[Task]
This is a navigation/index page containing links to other content. Review these links and select the ones most likely to contain relevant information to answer the question.

[Important Rules]
1. If NONE of the links on this page are relevant to the question, select "SKIP_PAGE" to skip this navigation page
2. Be SELECTIVE - only choose links that have HIGH probability of containing useful information
3. Navigation pages often contain many irrelevant links (home, about, login, etc.) - ignore those
4. It's OK to skip if this navigation page doesn't seem helpful

[Output Format]

If you found relevant links:
##æ¨èé“¾æ¥
ç¬¬ä¸€ä¸ªé“¾æ¥(replace with your choice)
å¦ä¸€ä¸ªé“¾æ¥(replace with your choice)

If NONE of the links are relevant:
##æ¨èé“¾æ¥
SKIP_PAGE
"""


# ==========================================
# ç»“æ„åŒ–é“¾æ¥ç®¡ç†å™¨ï¼ˆç”¨äºæœç´¢ç»“æœå’Œå¯¼èˆªé¡µï¼‰
# ==========================================

class StructuredLinkManager:
    """
    ç»“æ„åŒ–é“¾æ¥ç®¡ç†å™¨

    ç”¨äºæœç´¢ç»“æœé¡µå’Œå¯¼èˆªé¡µçš„é“¾æ¥è§£æï¼Œæ”¯æŒçµæ´»çš„é“¾æ¥æ–‡æœ¬åŒ¹é…ï¼š

    1. å®Œå…¨åŒ¹é… - ä¾‹å¦‚ "[ğŸ”—Link1 To: www.example.com]"
    2. Linkç¼–å·åŒ¹é… - ä¾‹å¦‚ "Link1", "ğŸ”—Link1", "Link 1"
    3. åŸŸååŒ¹é… - ä¾‹å¦‚ "www.example.com"ï¼ˆåŒ¹é…ç¬¬ä¸€ä¸ªï¼‰

    è¿™ä¸ªç±»æ˜¯æœç´¢ç»“æœå’Œå¯¼èˆªé¡µé€šç”¨çš„é“¾æ¥è§£æå·¥å…·ã€‚
    """

    def __init__(self, link_mapping: dict):
        """
        Args:
            link_mapping: é“¾æ¥IDåˆ°URLçš„æ˜ å°„å­—å…¸
                         ä¾‹å¦‚ {"Link1 To: www.example.com": "https://example.com/..."}
        """
        self.link_mapping = link_mapping

    def get_url(self, link_text: str) -> Optional[str]:
        """
        çµæ´»çš„é“¾æ¥æ–‡æœ¬åŒ¹é…ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼

        Args:
            link_text: LLM è¿”å›çš„é“¾æ¥æ–‡æœ¬

        Returns:
            åŒ¹é…çš„ URLï¼Œå¦‚æœæœªåŒ¹é…åˆ™è¿”å› None
        """
        import re

        # æ¨¡å¼1: å®Œå…¨åŒ¹é…ï¼ˆå»æ‰ ğŸ”— å’Œå‰åç©ºæ ¼ï¼‰
        exact_match = link_text.replace("ğŸ”—", "").strip()
        if exact_match in self.link_mapping:
            return self.link_mapping[exact_match]

        # æ¨¡å¼2: Linkç¼–å·åŒ¹é…
        # æå– "Link1", "Link2" ç­‰ç¼–å·
        # æ”¯æŒæ ¼å¼: "Link1", "ğŸ”—Link1", "Link 1", "Link 1 To: ..."
        link_pattern = r"Link\s*(\d+)"
        match = re.search(link_pattern, link_text, re.IGNORECASE)
        if match:
            link_num = match.group(1)
            # åœ¨ link_mapping ä¸­æ‰¾åˆ°åŒ¹é…çš„
            for link_id in self.link_mapping.keys():
                if link_id.startswith(f"Link{link_num} To:"):
                    return self.link_mapping[link_id]

        # æ¨¡å¼3: åŸŸååŒ¹é…
        # æå–åŸŸåå¹¶æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…çš„
        # åŸŸåå¯èƒ½å‡ºç°åœ¨ "To: www.example.com" è¿™æ ·çš„æ ¼å¼ä¸­
        to_pattern = r"To:\s*([^\s\]]+)"
        to_match = re.search(to_pattern, link_text)
        if to_match:
            domain = to_match.group(1)
            # åœ¨ link_mapping ä¸­æŸ¥æ‰¾åŒ…å«è¯¥åŸŸåçš„ç¬¬ä¸€ä¸ª
            for link_id, url in self.link_mapping.items():
                if domain in link_id:
                    return url

        # æ¨¡å¼4: å¦‚æœç›´æ¥æä¾›çš„æ˜¯çº¯åŸŸåï¼ˆæ²¡æœ‰ "To:" å‰ç¼€ï¼‰
        # ä¹Ÿå°è¯•åŒ¹é…
        clean_text = link_text.strip().strip("[]").replace("ğŸ”—", "").strip()
        if clean_text and "." in clean_text and not clean_text.startswith("Link"):
            # çœ‹èµ·æ¥åƒåŸŸå
            for link_id, url in self.link_mapping.items():
                if clean_text in link_id:
                    return url

        # æœªåŒ¹é…
        return None


# ==========================================
# Markdown é“¾æ¥ç®¡ç†å™¨
# ==========================================

class MarkdownLinkManager:
    """
    Markdown é“¾æ¥ç®¡ç†å™¨

    æ ¸å¿ƒèŒè´£ï¼š
    * å»å™ªï¼šè¿‡æ»¤å·²è®¿é—®ã€é»‘åå•é“¾æ¥
    * è¯­ä¹‰åŒ–æ›¿æ¢ï¼šå°† [Text](Long-URL) æ›¿æ¢ä¸º [ğŸ”—Text]
    * é˜²é‡åï¼šå¤„ç†ç›¸åŒ Anchor Text çš„æƒ…å†µ
    * ä¿¡æ¯å¢å¼ºï¼šä¸ºæ— æ„ä¹‰æ–‡æœ¬è¡¥å…… URL ä¿¡æ¯
    * ç›¸å¯¹é“¾æ¥è½¬æ¢ï¼šå°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
    """

    def __init__(self, ctx: 'WebSearcherContext', logger=None):
        self.ctx = ctx
        self.logger = logger  # å¯é€‰çš„ logger
        # æ ¸å¿ƒæ˜ å°„ï¼š{ "æ˜¾ç¤ºç»™LLMçš„å”¯ä¸€æ–‡æœ¬": "çœŸå®URL" }
        self.text_to_url: Dict[str, str] = {}
        # åŒ¹é… Markdown é“¾æ¥ [text](url) - åŒ…æ‹¬ç›¸å¯¹URLå’Œç©ºé”šæ–‡æœ¬
        self.link_pattern = re.compile(r'\[([^\]]*)\]\(([^)]+)\)')
        # æ— æ„ä¹‰æ–‡æœ¬åˆ—è¡¨
        self.generic_terms = {
            "click here", "here", "read more", "more", "link", "details",
            "ç‚¹å‡»è¿™é‡Œ", "ç‚¹å‡»", "æ›´å¤š", "è¯¦æƒ…", "è¿™é‡Œ", "ä¸‹è½½", "download"
        }

    def process(self, markdown_text: str, base_url: str) -> str:
        """
        å¤„ç† Markdownï¼šæ¸…æ´—é“¾æ¥ï¼Œç”Ÿæˆå”¯ä¸€æ–‡æœ¬ï¼Œå»ºç«‹æ˜ å°„

        Args:
            markdown_text: åŸå§‹ Markdown æ–‡æœ¬
            base_url: å½“å‰é¡µé¢çš„URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹é“¾æ¥

        Returns:
            å¤„ç†åçš„ Markdown æ–‡æœ¬ï¼ˆé“¾æ¥å·²æ›¿æ¢ä¸º [ğŸ”—Text] æ ¼å¼ï¼‰
        """
        link_count = 0

        def replace_match(match):
            nonlocal link_count
            text = match.group(1).strip()
            url = match.group(2).strip()

            # æ¸…ç†é“¾æ¥æ–‡æœ¬ï¼šç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºç™½
            text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
            text = text.strip()

            # 0. è¿‡æ»¤éHTTPé“¾æ¥ï¼ˆmailto:, javascript: ç­‰ï¼‰
            if url.startswith(('mailto:', 'javascript:', '#', 'tel:')):
                # è¿”å›çº¯æ–‡æœ¬ï¼Œç§»é™¤é“¾æ¥
                return text

            # 1. å¤„ç†ç©ºé”šæ–‡æœ¬ï¼šä»URLç”Ÿæˆæè¿°æ€§æ–‡æœ¬
            if not text:
                try:
                    parsed_url = urlparse(url)
                    # å°è¯•ä»URLæå–æœ‰æ„ä¹‰çš„æ–‡æœ¬
                    if parsed_url.netloc:
                        # ä½¿ç”¨åŸŸå
                        text = parsed_url.netloc
                    else:
                        # ä½¿ç”¨URLè·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†
                        path_parts = parsed_url.path.split('/')
                        text = next((p for p in reversed(path_parts) if p), "é“¾æ¥")
                except Exception:
                    text = "é“¾æ¥"

            # 2. å¤„ç†ç›¸å¯¹URLï¼Œè½¬æ¢ä¸ºç»å¯¹URL
            if url.startswith('/'):
                # ç›¸å¯¹URLï¼Œæ‹¼æ¥base URL
                try:
                    parsed_base = urlparse(base_url)
                    absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
                    url = absolute_url
                except Exception:
                    # è§£æå¤±è´¥ï¼Œè¿”å›çº¯æ–‡æœ¬
                    return text
            elif not url.startswith('http'):
                # å…¶ä»–æ ¼å¼ï¼ˆå¦‚ //example.com æˆ–å…¶ä»–åè®®ï¼‰ï¼Œè¿”å›çº¯æ–‡æœ¬
                return text

            # 3. è¿‡æ»¤é€»è¾‘ï¼šè·³è¿‡å·²è®¿é—®æˆ–é»‘åå•é“¾æ¥
            if not self.ctx.should_process_url(url):
                return text  # ä»…ä¿ç•™çº¯æ–‡æœ¬ï¼Œç§»é™¤é“¾æ¥å±æ€§

            # 4. ç”Ÿæˆå”¯ä¸€æ–‡æœ¬é”®
            unique_text = self._generate_unique_text(text, url)

            # 5. è®°å½•æ˜ å°„
            self.text_to_url[unique_text] = url

            # 6. æ›¿æ¢ä¸º [ğŸ”—Text] æ ¼å¼
            link_count += 1
            return f"[ğŸ”—{unique_text}]"

        result = self.link_pattern.sub(replace_match, markdown_text)
        if self.logger:
            self.logger.info(f"ğŸ”— Processed {link_count} links in Markdown")
        return result

    def _generate_unique_text(self, text: str, url: str) -> str:
        """
        ç”Ÿæˆå”¯ä¸€çš„æ–‡æœ¬æ ‡è¯†

        Args:
            text: é“¾æ¥çš„ Anchor Text
            url: é“¾æ¥çš„çœŸå® URL

        Returns:
            å”¯ä¸€çš„æ–‡æœ¬æ ‡è¯†
        """
        # A. å¤„ç†æ— æ„ä¹‰æ–‡æœ¬ (Generic Terms)
        # æ–‡æœ¬è¿‡çŸ­æˆ–æ— æ„ä¹‰æ—¶ï¼Œä» URL æå–æç¤º
        if len(text) < 2 or text.lower() in self.generic_terms:
            try:
                path = urlparse(url).path
                hint = path.split('/')[-1]  # å°è¯•æ‹¿æ–‡ä»¶å
                if not hint:
                    hint = urlparse(url).netloc  # æ‹¿ä¸åˆ°æ–‡ä»¶åæ‹¿åŸŸå
                # é™åˆ¶ hint é•¿åº¦é˜²æ­¢è¿‡é•¿
                if len(hint) > 20:
                    hint = hint[:17] + "..."
                text = f"{text}({hint})"
            except Exception:
                pass

        # B. å¤„ç†é‡å (Duplicate Handling)
        # å¦‚æœ URL å®Œå…¨ä¸€è‡´ï¼Œå¤ç”¨åŒä¸€ä¸ªæ–‡æœ¬
        if text in self.text_to_url and self.text_to_url[text] == url:
            return text

        # å¦‚æœæ–‡æœ¬å­˜åœ¨ä½† URL ä¸åŒï¼ŒåŠ åºå·åŒºåˆ†
        base_text = text
        counter = 2
        while text in self.text_to_url:
            text = f"{base_text}({counter})"
            counter += 1

        return text

    def get_url(self, text: str) -> Optional[str]:
        """
        æ ¹æ®æ–‡æœ¬è·å– URL

        Args:
            text: LLM è¾“å‡ºçš„é“¾æ¥æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å« ğŸ”— ç¬¦å·ï¼‰

        Returns:
            å¯¹åº”çš„çœŸå® URLï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        # å®¹é”™ï¼šç§»é™¤å¯èƒ½è¢« LLM å¸¦ä¸Šçš„ ğŸ”— ç¬¦å·å’Œæ ‡ç‚¹
        clean_text = text.replace("ğŸ”—", "").strip()
        clean_text = clean_text.strip("ã€‚ï¼Œã€ï¼›ï¼š""''""ï¼ˆï¼‰")
        return self.text_to_url.get(clean_text)


# ==========================================
# çŠ¶æ€ä¸ä¸Šä¸‹æ–‡å®šä¹‰
# ==========================================

class WebSearcherContext(BaseCrawlerContext):
    """
    Web æœç´¢ä»»åŠ¡ä¸Šä¸‹æ–‡
    ç”¨äºå›ç­”é—®é¢˜çš„æœç´¢ä»»åŠ¡ï¼Œå¸¦æœ‰"å°æœ¬æœ¬"æœºåˆ¶è®°å½•æœ‰ç”¨ä¿¡æ¯
    """

    def __init__(self, purpose: str, deadline: float, chunk_threshold: int = 5000,
                 temp_file_dir: Optional[str] = None):
        super().__init__(deadline)
        self.purpose = purpose  # æ”¹åï¼šquestion -> purpose
        self.notebook = ""
        self.chunk_threshold = chunk_threshold
        self.temp_file_dir = temp_file_dir
        # ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„é“¾æ¥ï¼ˆåœ¨æµå¼é˜…è¯»è¿‡ç¨‹ä¸­å‘ç°ï¼‰
        self.pending_links_from_reading: List[str] = []

    def add_to_notebook(self, info: str):
        """æ·»åŠ ä¿¡æ¯åˆ°å°æœ¬æœ¬"""
        if info:
            timestamp = time.strftime("%H:%M:%S")
            self.notebook += f"\n\n[{timestamp}] {info}\n"

    def add_pending_link(self, url: str):
        """
        æ·»åŠ å¾…å¤„ç†çš„é“¾æ¥ï¼ˆä»æµå¼é˜…è¯»ä¸­å‘ç°ï¼‰

        Args:
            url: å¾…è®¿é—®çš„ URL
        """
        if url and url not in self.pending_links_from_reading:
            self.pending_links_from_reading.append(url)

    def get_pending_links(self) -> List[str]:
        """
        è·å–æ‰€æœ‰å¾…å¤„ç†çš„é“¾æ¥å¹¶æ¸…ç©ºåˆ—è¡¨

        Returns:
            å¾…å¤„ç†çš„é“¾æ¥åˆ—è¡¨
        """
        links = self.pending_links_from_reading.copy()
        self.pending_links_from_reading.clear()
        return links


# ==========================================
# 2. Web Searcher æ ¸å¿ƒé€»è¾‘
# ==========================================

class WebSearcherMixin(CrawlerHelperMixin):
    """
    Web æœç´¢å™¨æŠ€èƒ½
    ç”¨äºå›ç­”é—®é¢˜çš„ç½‘ç»œæœç´¢
    """

    @register_action(
        "é’ˆå¯¹ç‰¹å®šç›®çš„ä¸Šç½‘æœç´¢ï¼Œæä¾›æ˜ç¡®çš„æœç´¢ç›®çš„å’Œï¼ˆå¯é€‰ä½†å»ºè®®æä¾›çš„ï¼‰æœç´¢å…³é”®å­—è¯",
        param_infos={
            "purpose": "æœç´¢çš„ç›®çš„",
            "search_phrase": "å¯é€‰ï¼Œåˆå§‹æœç´¢å…³é”®è¯",
            "max_time": "å¯é€‰ï¼Œæœ€å¤§æœç´¢åˆ†é’Ÿï¼Œé»˜è®¤20",
            "search_engine": "å¯é€‰ï¼Œæœç´¢å¼•æ“ï¼ˆgoogle æˆ– bingï¼‰ï¼Œé»˜è®¤ä½¿ç”¨ agent é…ç½®çš„ default_search_engine æˆ– bing",
        }
    )
    async def web_search(
        self,
        purpose: str,
        search_phrase: str = None,
        max_time: int = 20,
        search_engine: str = None,
        temp_file_dir: Optional[str] = None
    ):
        """
        [Entry Point] ä¸Šç½‘æœç´¢å›ç­”é—®é¢˜ï¼ˆç»Ÿä¸€æµç¨‹ç‰ˆæœ¬ï¼‰

        Args:
            purpose: è¦å›ç­”çš„é—®é¢˜ï¼ˆæˆ–ç ”ç©¶ç›®æ ‡ï¼‰
            search_phrase: åˆå§‹æœç´¢å…³é”®è¯
            max_time: æœ€å¤§æœç´¢æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            search_engine: æœç´¢å¼•æ“ï¼ˆ"google" æˆ– "bing"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é…ç½®çš„é»˜è®¤å¼•æ“
            chunk_threshold: åˆ†æ®µé˜ˆå€¼ï¼ˆå­—ç¬¦æ•°ï¼‰
            temp_file_dir: ä¸´æ—¶æ–‡ä»¶ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        """
        # 0. ç¡®å®š search_engineï¼ˆä¼˜å…ˆçº§ï¼šå‚æ•° > é…ç½® > é»˜è®¤ï¼‰
        if search_engine is None:
            # å°è¯•ä»å®ä¾‹å±æ€§è¯»å–é…ç½®
            search_engine = getattr(self, 'default_search_engine', DEFAULT_SEARCH_ENGINE)
            self.logger.info(f"Using configured default search engine: {search_engine}")
        # 1. å‡†å¤‡ç¯å¢ƒ
        profile_path = os.path.join(self.workspace_root, ".matrix", "browser_profile", self.name)
        download_path = os.path.join(self.current_workspace, "downloads")
        chunk_threshold = 5000

        # 2. ç¡®å®š search_phrase
        if not search_phrase:
            resp = await self.brain.think(f"""
            æˆ‘ä»¬ç°åœ¨çš„ç›®çš„æ˜¯ {purpose}ï¼Œæ‰“ç®—ä¸Šç½‘æœç´¢ä¸€ä¸‹ï¼Œéœ€è¦ä½ è®¾è®¡ä¸€ä¸‹æœ€åˆé€‚çš„å…³é”®è¯æˆ–è€…å…³é”®å­—ç»„åˆã€‚è¾“å‡ºçš„æ—¶å€™å¯ä»¥å…ˆç®€å•è§£é‡Šä¸€ä¸‹è¿™ä¹ˆè®¾è®¡çš„ç†ç”±ï¼Œä½†æ˜¯æœ€åä¸€è¡Œå¿…é¡»æ˜¯ä¹Ÿåªèƒ½æ˜¯è¦æœç´¢çš„å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯è¾“å…¥åˆ°æœç´¢å¼•æ“æœç´¢æ çš„å†…å®¹ï¼‰ã€‚ä¾‹å¦‚ä½ è®¤ä¸ºåº”è¯¥æœç´¢"Keyword"ï¼Œé‚£ä¹ˆæœ€åä¸€è¡Œå°±åªèƒ½æ˜¯"Keyword"
            """)
            reply = resp['reply']
            #get last line of reply
            if '\n' in reply:
                search_phrase = reply.split('\n')[-1].strip()
        #å¦‚æœè¿˜æ˜¯æœ‰é—®é¢˜,æˆ‘ä»¬ç›´æ¥æœç´¢é—®é¢˜ï¼š
        if not search_phrase:
            search_phrase = purpose

        # 3. éªŒè¯æœç´¢å¼•æ“
        if search_engine.lower() not in SEARCH_ENGINES:
            self.logger.warning(f"Unknown search engine '{search_engine}', using default '{DEFAULT_SEARCH_ENGINE}'")
            search_engine = DEFAULT_SEARCH_ENGINE

        self.logger.info(f"ğŸ” å‡†å¤‡æœç´¢: {search_phrase}")
        self.logger.info(f"ğŸ” æœç´¢å¼•æ“: {search_engine}")

        # 4. åˆå§‹åŒ–æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡
        self.browser = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=download_path
        )

        ctx = WebSearcherContext(
            purpose=purpose,
            deadline=time.time() + int(max_time) * 60,
            chunk_threshold=chunk_threshold,
            temp_file_dir=temp_file_dir
        )

        self.logger.info(f"ğŸ” Web Search Start: {purpose}")
        self.logger.info(f"ğŸ” Initial search phrase: {search_phrase}")
        self.logger.info(f"ğŸ” Max search time: {max_time} minutes")

        # 5. å¯åŠ¨æµè§ˆå™¨
        await self.browser.start(headless=False)

        try:
            # 6. åˆ›å»º Tab å’Œ Session
            tab = await self.browser.get_tab()
            session = TabSession(handle=tab, current_url="")

            # 7. ç”Ÿæˆè™šæ‹Ÿæœç´¢URLå¹¶åŠ å…¥é˜Ÿåˆ—
            # æ”¯æŒå¤šä¸ªæœç´¢å…³é”®å­—ç»„åˆï¼šç”¨é€—å·åˆ†éš” search_phraseï¼Œä¸ºæ¯ä¸ªçŸ­è¯­ç”Ÿæˆç‹¬ç«‹çš„æœç´¢URL
            # è™šæ‹ŸURLåè®®æ ¼å¼: "search:{search_engine}:{search_phrase}"
            # ä¾‹å¦‚: "search:bing:2026å¹´1æœˆ24æ—¥ æ”¿æ²» æ–°é—»"

            # åˆ†å‰² search_phraseï¼ˆæ”¯æŒä¸­è‹±æ–‡é€—å·ï¼‰
            search_phrases = [p.strip() for p in search_phrase.replace('ï¼Œ', ',').split(',')]
            # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            search_phrases = [p for p in search_phrases if p]

            self.logger.info(f"âœ“ Parsed {len(search_phrases)} search phrase(s) from input: {search_phrases}")

            # ä¸ºæ¯ä¸ªæœç´¢çŸ­è¯­ç”Ÿæˆè™šæ‹Ÿæœç´¢URLå¹¶åŠ å…¥é˜Ÿåˆ—
            for phrase in search_phrases:
                search_virtual_url = f"search:{search_engine.lower()}:{phrase}"
                session.pending_link_queue.append(search_virtual_url)
                self.logger.info(f"âœ“ Added virtual search URL to queue: {search_virtual_url}")

            # 8. è¿è¡Œç»Ÿä¸€çš„æœç´¢ç”Ÿå‘½å‘¨æœŸ
            # _run_search_lifecycle ä¼šè¯†åˆ«è™šæ‹ŸURLå¹¶è°ƒç”¨ç›¸åº”çš„æœç´¢å‡½æ•°
            answer = await self._run_search_lifecycle(session, ctx)

            # 9. è¿”å›ç»“æœ
            if answer:
                self.logger.info(f"âœ… Found answer!")
                return f"Answer: {answer}\n\n---\nNotebook:\n{ctx.notebook}"
            else:
                self.logger.info("â¸ Search completed without finding complete answer")
                return f"Could not find a complete answer.\n\nHere's what I found:\n{ctx.notebook}"

        except Exception as e:
            self.logger.exception("Web searcher crashed")
            return f"Search failed with error: {e}"
        #finally:
        #    self.logger.info("ğŸ›‘ Closing browser...")
        #    await self.browser.close()

    @register_action(
        "è®¿é—®ä¸€ä¸ªç½‘é¡µå¹¶æŸ¥çœ‹ç½‘é¡µå†…å®¹ï¼Œå¦‚æœæ˜¯pdfæ–‡ä»¶å°±ä¸‹è½½",
        param_infos={
            "url": "è¦è®¿é—®çš„ç½‘é¡µ URL"
        }
    )
    async def visit_url(self,url: str):
        # 1. å‡†å¤‡ç¯å¢ƒ
        profile_path = os.path.join(self.workspace_root, ".matrix", "browser_profile", self.name)
        download_path = os.path.join(self.current_workspace, "downloads")


        
        self.logger.info(f"ğŸ” å‡†å¤‡è®¿é—®: {url}")

        self.browser = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=download_path
        )
        await self.browser.start(headless=False)

        tab = await self.browser.get_tab()
        

        nav_report = await self.browser.navigate(tab, url)
        final_url = self.browser.get_tab_url(tab)
        

        # === Phase 2: Identify Page Type ===
        page_type = await self.browser.analyze_page_type(tab)

        if page_type == PageType.ERRO_PAGE:
            self.logger.warning(f"ğŸš« Error Page: {final_url}")
            return f"Error Accessing Page: {url}"

        # === åˆ†æ”¯ A: é™æ€èµ„æº ===
        if page_type == PageType.STATIC_ASSET:
            self.logger.info(f"ğŸ“„ Static Asset: {final_url}")

            download_file = await self.browser.save_static_asset(tab)
            return f"æ–‡ä»¶å·²ä¸‹è½½åˆ°ï¼š {download_file}"

  

        # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ ===
        elif page_type == PageType.NAVIGABLE:
            await self.browser.stabilize(tab)
            markdown = await self._html_to_full_markdown(tab)
            
            #ç”¨markdownç¬¬ä¸€è¡Œä½œä¸ºæ–‡ä»¶åå­—
            filename = markdown.split('\n')[0].strip().replace("#","")
            filename = sanitize_filename(filename) + ".md"

            #æŠŠmarkdownä¿å­˜ä¸ºæ–‡ä»¶
            with open(os.path.join(self.current_workspace, filename), "w", encoding="utf-8") as f:
                f.write(markdown)
            return f"ç½‘é¡µæ‘˜è¦å·²ä¿å­˜åˆ°ï¼š {filename}"

    # ==========================================
    # 2. è¾…åŠ©æ–¹æ³•ï¼ˆç›®å½•ã€é€‰æ‹©ç« èŠ‚ã€åˆ†æ®µï¼‰
    # ==========================================

    def _generate_document_toc(self, markdown: str) -> List[Dict[str, Any]]:
        """
        ä» Markdown ä¸­æå–ç›®å½•ç»“æ„
        è¿”å›: [
            {"level": 1, "title": "ç¬¬ä¸€ç« ", "start": 0, "end": 1234},
            {"level": 2, "title": "1.1 ç®€ä»‹", "start": 1235, "end": 2345},
            ...
        ]
        """
        toc = []
        lines = markdown.split("\n")
        current_pos = 0

        for line in lines:
            # åŒ¹é… Markdown æ ‡é¢˜
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                toc.append({
                    "level": level,
                    "title": title,
                    "start": current_pos,
                    "line": line
                })

            current_pos += len(line) + 1  # +1 for newline

        # è®¡ç®—æ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
        for i in range(len(toc) - 1):
            toc[i]["end"] = toc[i + 1]["start"]
        if toc:
            toc[-1]["end"] = len(markdown)

        return toc

    async def _let_llm_select_chapters(
        self,
        toc: List[Dict],
        ctx: WebSearcherContext,
        markdown_preview: str = "",
        url: str = ""
    ) -> List[int]:
        """
        è®© LLM æ ¹æ®é—®é¢˜é€‰æ‹©ç›¸å…³ç« èŠ‚ï¼ˆä½¿ç”¨ think_with_retry è‡ªåŠ¨é‡è¯•ï¼‰

        Args:
            toc: ç›®å½•åˆ—è¡¨
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            markdown_preview: Markdownå¼€å¤´500å­—é¢„è§ˆï¼ˆå¸®åŠ©åˆ¤æ–­æ–‡æ¡£æ˜¯å¦æœ‰ç”¨ï¼‰
            url: å½“å‰é¡µé¢URLï¼ˆå¸®åŠ©LLMç†è§£é¡µé¢æ¥æºï¼‰

        Returns:
            é€‰ä¸­çš„ç« èŠ‚ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ï¼Œæˆ–ç‰¹æ®Šå€¼ï¼š
            - None: è·³è¿‡ç« èŠ‚é€‰æ‹©ï¼ˆä½¿ç”¨å…¨æ–‡ï¼‰
            - "SKIP_DOC": è·³è¿‡æ•´ä¸ªæ–‡æ¡£ï¼ˆæ–‡æ¡£ä¸ç›¸å…³ï¼‰
        """
        # æ„é€  TOC åˆ—è¡¨ï¼ˆä¸ç”¨æ•°å­—ç¼–å·ï¼Œä¿ç•™ç¼©è¿›ï¼‰
        toc_lines = []
        for chapter in toc:
            indent = "  " * (chapter["level"] - 1)
            toc_lines.append(f"{indent}{chapter['title']}")
        toc_list = "\n".join(toc_lines)

        # æ„é€ ç« èŠ‚åå­—åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºéªŒè¯ï¼‰
        chapter_name_to_index = {
            chapter["title"]: i
            for i, chapter in enumerate(toc)
        }

        # ä½¿ç”¨ prompt æ¨¡æ¿
        prompt = WebSearcherPrompts.CHAPTER_SELECTION.format(
            question=ctx.purpose,
            toc_list=toc_list,
            url=url,
            preview=markdown_preview
        )

        self.logger.debug(f"ã€Chapter Selection Promptã€‘:\n{prompt}")

        try:
            # ä½¿ç”¨ think_with_retry è‡ªåŠ¨å¤„ç†é‡è¯•
            result = await self.cerebellum.backend.think_with_retry(
                initial_messages=prompt,
                parser=self._chapter_parser,
                max_retries=5,
                chapter_name_to_index = chapter_name_to_index
            )

            # result å¯èƒ½æ˜¯ï¼š
            # - None: è·³è¿‡ç« èŠ‚é€‰æ‹©ï¼ˆä½¿ç”¨å…¨æ–‡ï¼‰
            # - "SKIP_DOC": è·³è¿‡æ•´ä¸ªæ–‡æ¡£
            # - ç« èŠ‚ç´¢å¼•åˆ—è¡¨
            if result is None:
                self.logger.info("ğŸ“‹ Skipping chapter selection (TOC not helpful)")
                return None  # None è¡¨ç¤ºè·³è¿‡ç« èŠ‚é€‰æ‹©ï¼Œä½¿ç”¨å…¨æ–‡

            if result == "SKIP_DOC":
                self.logger.info("ğŸš« LLM decided to skip entire document (not relevant)")
                return "SKIP_DOC"

            self.logger.info(f"âœ… Successfully selected {len(result)} chapters: {result}")
            return result

        except ValueError as e:
            # think_with_retry åœ¨æ‰€æœ‰é‡è¯•å¤±è´¥åä¼šæŠ›å‡º ValueError
            self.logger.error(f"âŒ Chapter selection failed after all retries: {e}")
            # è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¼šè§¦å‘å…¨æ–‡å¤„ç†ï¼‰
            return []

    def _chapter_parser(self, llm_output: str, chapter_name_to_index: Dict[str, int]) -> Dict[str, Any]:
        """
        Parser for think_with_retry - éµå¾ª Parser Contract
        è§£æ LLM çš„ç« èŠ‚é€‰æ‹©è¾“å‡º

        Args:
            llm_output: LLM çš„åŸå§‹è¾“å‡º
            chapter_name_to_index: ç« èŠ‚ååˆ°ç´¢å¼•çš„æ˜ å°„

        Returns:
        {
            "status": "success" | "error",
            "content": List[int],              # å½“ status="success" æ—¶ï¼Œè¿”å›é€‰ä¸­çš„ç« èŠ‚ç´¢å¼•
            "feedback": str                 # å½“ status="error" æ—¶ï¼Œè¿”å›åé¦ˆä¿¡æ¯
        }
        """
        # å®½æ¾å¤„ç†ï¼šå…ˆæ£€æŸ¥æ•´ä¸ªè¾“å‡ºæ˜¯å¦åŒ…å« SKIP_DOC æˆ– SKIP_TOC
        output_upper = llm_output.upper()
        if "SKIP_DOC" in output_upper:
            self.logger.info("ğŸš« LLM decided to skip entire document (not relevant)")
            return {
                "status": "success",
                "content": "SKIP_DOC"  # ç‰¹æ®Šå€¼ï¼šè·³è¿‡æ•´ä¸ªæ–‡æ¡£
            }

        if "SKIP_TOC" in output_upper:
            self.logger.info("ğŸ“‹ LLM chose to skip chapter selection (TOC not helpful)")
            return {
                "status": "success",
                "content": None  # None è¡¨ç¤ºè·³è¿‡ç« èŠ‚é€‰æ‹©ï¼Œä½¿ç”¨å…¨æ–‡
            }

        # ä½¿ç”¨ multi_section_parser æå–ç« èŠ‚é€‰æ‹©éƒ¨åˆ†
        result = multi_section_parser(
            llm_output,
            section_headers=["====ç« èŠ‚é€‰æ‹©===="],
            match_mode="ANY"
        )

        if result["status"] == "error":
            return {
                "status": "error",
                "feedback": WebSearcherPrompts.CHAPTER_ERROR_FORMAT
            }

        # æå–ç« èŠ‚é€‰æ‹©å†…å®¹
        chapter_section = result["content"]["====ç« èŠ‚é€‰æ‹©===="]

        # å¦‚æœæœ‰ç»“æŸæ ‡è®°ï¼Œåªå–ç»“æŸæ ‡è®°ä¹‹å‰çš„éƒ¨åˆ†
        if "====ç« èŠ‚é€‰æ‹©ç»“æŸ====" in chapter_section:
            chapter_section = chapter_section.split("====ç« èŠ‚é€‰æ‹©ç»“æŸ====")[0].strip()

        # æŒ‰è¡Œåˆ†å‰²
        chapter_lines = [
            line.strip()
            for line in chapter_section.split('\n')
            if line.strip()
        ]

        if not chapter_lines:
            return {
                "status": "error",
                "feedback": "æ²¡æœ‰é€‰æ‹©ä»»ä½•ç« èŠ‚ã€‚è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç›¸å…³ç« èŠ‚ã€‚"
            }

        # éªŒè¯ç« èŠ‚æ˜¯å¦å­˜åœ¨äº TOC ä¸­
        selected_indices = []
        invalid_chapters = []

        for chapter_name in chapter_lines:
            # ç‰¹æ®Šå¤„ç†ï¼šSKIP_TOC è¡¨ç¤ºè·³è¿‡ç« èŠ‚é€‰æ‹©ï¼Œä½¿ç”¨å…¨æ–‡
            if chapter_name == "SKIP_TOC":
                self.logger.info("ğŸ“‹ LLM chose to skip chapter selection (TOC not helpful)")
                return {
                    "status": "success",
                    "content": None  # None è¡¨ç¤ºè·³è¿‡ç« èŠ‚é€‰æ‹©
                }

            if chapter_name in chapter_name_to_index:
                selected_indices.append(chapter_name_to_index[chapter_name])
            else:
                invalid_chapters.append(chapter_name)

        # åˆ¤æ–­ç»“æœ
        if invalid_chapters:
            # æœ‰å¹»è§‰ - è¿”å›å…·ä½“åé¦ˆ

            return {
                "status": "error",
                "feedback": f"é€‰æ‹©é”™è¯¯ï¼Œä»¥ä¸‹ç« èŠ‚ä¸å­˜åœ¨äºTOCä¸­ï¼š{', '.join(invalid_chapters)}ã€‚\n\nå¦‚æœTOCä¸­æ²¡æœ‰åˆé€‚çš„ç« èŠ‚ï¼Œå¯ä»¥ç›´æ¥è¾“å‡º SKIP_TOC è·³è¿‡ç« èŠ‚é€‰æ‹©ã€‚"
            }

        if not selected_indices:
            return {
                "status": "error",
                "feedback": "æ²¡æœ‰æœ‰æ•ˆçš„ç« èŠ‚é€‰æ‹©ã€‚\n\nå¦‚æœTOCä¸­æ²¡æœ‰åˆé€‚çš„ç« èŠ‚ï¼Œå¯ä»¥ç›´æ¥è¾“å‡º SKIP_TOC è·³è¿‡ç« èŠ‚é€‰æ‹©ã€‚"
            }

        # æˆåŠŸ - è¿”å›ç« èŠ‚ç´¢å¼•åˆ—è¡¨
        return {
            "status": "success",
            "content": selected_indices
        }

    
    def _split_by_paragraph_boundaries(
        self,
        text: str,
        threshold: int
    ) -> List[str]:
        """
        æŒ‰æ®µè½è¾¹ç•Œå°†æ–‡æœ¬åˆ†æ®µï¼Œæ¯æ®µ â‰¤ threshold

        ç­–ç•¥ï¼š
        1. æŒ‰ \\n\\n åˆ†å‰²æ®µè½
        2. é€æ­¥æ·»åŠ æ®µè½ï¼Œç›´åˆ°æ¥è¿‘é˜ˆå€¼
        3. åœ¨æœ€è¿‘çš„åŒæ¢è¡Œå¤„æ–­å¼€
        4. è¶…é•¿æ®µè½æŒ‰å¥å­ï¼ˆã€‚ï¼‰ç»†åˆ†
        """
        if len(text) <= threshold:
            return [text]

        # æŒ‰åŒæ¢è¡Œåˆ†æ®µ
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            test_chunk = current_chunk + ("\n\n" if current_chunk else "") + para

            if len(test_chunk) <= threshold:
                current_chunk = test_chunk
            else:
                # å½“å‰æ®µè½ä¼šè¶…å‡ºé˜ˆå€¼
                if current_chunk:
                    chunks.append(current_chunk)

                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é˜ˆå€¼ï¼Œå¼ºåˆ¶åœ¨ä¸­é—´æ–­å¼€
                if len(para) > threshold:
                    # æŒ‰å¥å­åˆ†å‰²
                    sentences = para.split('ã€‚')
                    temp_chunk = ""
                    for sent in sentences:
                        test_sent = temp_chunk + ('ã€‚' if temp_chunk else '') + sent
                        if len(test_sent) <= threshold:
                            temp_chunk = test_sent
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk)
                            temp_chunk = sent
                    current_chunk = temp_chunk
                else:
                    current_chunk = para

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    # ==========================================
    # 4. æ ¸å¿ƒæµå¼å¤„ç†
    # ==========================================

    def _find_chapters_in_batch(
        self,
        toc: List[Dict[str, Any]],
        batch_start: int,
        batch_end: int,
        content_start: int = 0
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾ batch ä¸­æ¶‰åŠçš„æ‰€æœ‰ç« èŠ‚ï¼ˆå¯èƒ½è·¨å¤šä¸ªç« èŠ‚ï¼‰

        Args:
            toc: ç›®å½•ç»“æ„
            batch_start: batch åœ¨å†…å®¹ä¸­çš„èµ·å§‹ä½ç½®
            batch_end: batch åœ¨å†…å®¹ä¸­çš„ç»“æŸä½ç½®
            content_start: å†…å®¹çš„èµ·å§‹ä½ç½®ï¼ˆç”¨äºå¤„ç†é€‰ä¸­ç« èŠ‚çš„æƒ…å†µï¼‰

        Returns:
            batch æ¶‰åŠçš„æ‰€æœ‰ç« èŠ‚åˆ—è¡¨ï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰
        """
        adjusted_start = batch_start + content_start
        adjusted_end = batch_end + content_start

        # æ‰¾åˆ°æ‰€æœ‰ä¸ batch æœ‰äº¤é›†çš„ç« èŠ‚
        chapters_in_batch = []
        for chapter in toc:
            # æ£€æŸ¥ç« èŠ‚æ˜¯å¦ä¸ batch æœ‰äº¤é›†
            # æœ‰äº¤é›†çš„æ¡ä»¶ï¼šç« èŠ‚çš„ç»“æŸä½ç½® > batchèµ·å§‹ ä¸” ç« èŠ‚çš„èµ·å§‹ä½ç½® < batchç»“æŸ
            if chapter["end"] > adjusted_start and chapter["start"] < adjusted_end:
                chapters_in_batch.append(chapter)

        return chapters_in_batch

    async def _process_batch(
        self,
        batch_text: str,
        ctx: WebSearcherContext,
        doc_title: str,
        current_batch: int,
        total_batches: int,
        url,
        toc: List[Dict[str, Any]] = None,
        current_chapters: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ‰¹å¤„ç†å‡½æ•°ï¼ˆä½¿ç”¨ think_with_retry è‡ªåŠ¨é‡è¯•ï¼‰

        å‚æ•°:
            batch_text: å½“å‰æ‰¹æ¬¡æ–‡æœ¬
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            doc_title: æ–‡æ¡£åç§°
            current_batch: å½“å‰æ‰¹æ¬¡ï¼ˆé¡µç ï¼Œä» 1 å¼€å§‹ï¼‰
            total_batches: æ€»æ‰¹æ¬¡æ•°ï¼ˆæ€»é¡µæ•°ï¼‰
            url: å½“å‰é¡µé¢ URL
            toc: å¯é€‰çš„æ–‡æ¡£ç›®å½•ç»“æ„
            current_chapters: å¯é€‰çš„å½“å‰é¡µæ¶‰åŠçš„æ‰€æœ‰ç« èŠ‚ï¼ˆå¯èƒ½è·¨å¤šä¸ªç« èŠ‚ï¼‰

        è¿”å›:
        {
            "heading_type": "answer" | "note" | "continue" | "skip_doc",
            "content": str,
            "found_links": List[str]  # LLM æ¨èçš„é“¾æ¥æ–‡æœ¬åˆ—è¡¨
        }
        """
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress_pct = int((current_batch / total_batches) * 100)

        # æ„é€  TOC ä¿¡æ¯
        toc_info = ""
        if toc and current_chapters:
            # è·å–å½“å‰æ¶‰åŠçš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç« èŠ‚
            first_chapter = current_chapters[0]
            last_chapter = current_chapters[-1]

            first_index = toc.index(first_chapter)
            last_index = toc.index(last_chapter)

            toc_info = f"\n[Document Structure - Table of Contents]\n"

            # æ˜¾ç¤ºå‰ 2 ä¸ªç« èŠ‚ï¼ˆåœ¨ç¬¬ä¸€ä¸ªå½“å‰ç« èŠ‚ä¹‹å‰ï¼‰
            if first_index > 0:
                toc_info += "- Previous chapters:\n"
                for ch in toc[max(0, first_index - 2):first_index]:
                    toc_info += f"  - {ch['title']}\n"

            # æ˜¾ç¤ºå½“å‰æ¶‰åŠçš„ç« èŠ‚ï¼ˆé«˜äº®ï¼Œå¯èƒ½å¤šä¸ªï¼‰
            toc_info += "- >>> CURRENT SECTION (this page contains): <<<\n"
            for ch in current_chapters:
                # æ ¹æ® level ç¼©è¿›
                indent = "  " * (ch.get("level", 1) - 1)
                toc_info += f"{indent}â†’ {ch['title']}\n"

            # æ˜¾ç¤ºå 2 ä¸ªç« èŠ‚ï¼ˆåœ¨æœ€åä¸€ä¸ªå½“å‰ç« èŠ‚ä¹‹åï¼‰
            if last_index < len(toc) - 1:
                toc_info += "- Next chapters:\n"
                for ch in toc[last_index + 1:min(len(toc), last_index + 3)]:
                    toc_info += f"  - {ch['title']}\n"

            toc_info += "\n\n"

        # æ„é€ åˆå§‹ prompt
        prompt = WebSearcherPrompts.BATCH_PROCESSING.format(
            question=ctx.purpose,
            doc_title=doc_title,
            current_batch=current_batch,
            total_batches=total_batches,
            progress_pct=progress_pct,
            toc_info=toc_info,
            notebook=ctx.notebook,
            batch_text=batch_text,
            url=url
        )
        self.logger.debug(f"ã€Batch Processing Promptã€‘:\n{prompt}")
        try:
            # ä½¿ç”¨ think_with_retry è‡ªåŠ¨å¤„ç†é‡è¯•
            # parser è¿”å›: {"status": "success", "content": {"heading_type": ..., "content": ..., "found_links": [...]}}
            result_data = await self.cerebellum.backend.think_with_retry(
                initial_messages=prompt,
                parser=self._batch_parser,
                max_retries=5
            )

            # æ ¹æ®ç±»å‹è®°å½•æ—¥å¿—
            heading_type = result_data["heading_type"]
            if heading_type == "answer":
                self.logger.info(f"âœ… Found answer in batch")
            elif heading_type == "note":
                self.logger.info(f"ğŸ“ Found useful info in batch")
            elif heading_type == "continue":
                self.logger.debug(f"ğŸ‘€ No new info, continuing")
            else:  # skip_doc
                self.logger.warning(f"ğŸš« Document irrelevant, skipping")

            return result_data

        except ValueError as e:
            # think_with_retry åœ¨æ‰€æœ‰é‡è¯•å¤±è´¥åä¼šæŠ›å‡º ValueError
            self.logger.error(f"âŒ Batch processing failed after all retries: {e}")
            # è¿”å›é»˜è®¤å€¼ï¼ˆç»§ç»­é˜…è¯»ï¼‰
            return {"heading_type": "continue", "content": "", "found_links": []}

    def _batch_parser(self, llm_output: str) -> Dict[str, Any]:
        """
        Parser for think_with_retry - éµå¾ª Parser Contract
        ä½¿ç”¨ multi_section_parser å®ç°ï¼Œæ›´å¥å£®ä¸”ä¸ä¾èµ–æ ‡é¢˜ä½ç½®

        Args:
            llm_output: LLM çš„åŸå§‹è¾“å‡º

        Returns:
        {
            "status": "success" | "error",
            "content": {
                "heading_type": "answer" | "note" | "continue" | "skip_doc",
                "content": str,
                "found_links": List[str]
            },
            "feedback": str  # å½“ status="error" æ—¶è¿”å›åé¦ˆä¿¡æ¯
        }
        """
        # 1. å®šä¹‰4ä¸ªä¸»æ ‡é¢˜
        main_headings = [
            "##å¯¹é—®é¢˜çš„å›ç­”",
            "##å€¼å¾—è®°å½•çš„ç¬”è®°",
            "##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»",
            "##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ"
        ]

        # 2. ä½¿ç”¨ ANY æ¨¡å¼è°ƒç”¨ multi_section_parserï¼ˆåªéœ€è¦ä¸€ä¸ªä¸»æ ‡é¢˜ï¼‰
        result = multi_section_parser(
            llm_output,
            section_headers=main_headings,
            match_mode="ANY"
        )

        if result["status"] == "error":
            return {"status": "error", "feedback": result["feedback"]}

        sections = result["content"]

        # 3. è·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ sectionï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
        heading_map = {
            "##å¯¹é—®é¢˜çš„å›ç­”": "answer",
            "##å€¼å¾—è®°å½•çš„ç¬”è®°": "note",
            "##æ²¡æœ‰å€¼å¾—è®°å½•çš„ç¬”è®°ç»§ç»­é˜…è¯»": "continue",
            "##å®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£åº”è¯¥æ”¾å¼ƒ": "skip_doc"
        }

        chosen_heading = None
        content = ""
        for heading in main_headings:
            if heading in sections:
                chosen_heading = heading
                content = sections[heading].strip()
                break

        # 4. ä» content ä¸­åˆ†ç¦»å‡ºæ¨èé“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        found_links = []
        if "====æ¨èé“¾æ¥====" in content:
            parts = content.split("====æ¨èé“¾æ¥====")
            content = parts[0].strip()

            if len(parts) > 1:
                # æå–é“¾æ¥éƒ¨åˆ†
                links_section = parts[1]
                if "====æ¨èé“¾æ¥ç»“æŸ====" in links_section:
                    links_section = links_section.split("====æ¨èé“¾æ¥ç»“æŸ====")[0]

                link_lines = [line.strip() for line in links_section.split('\n') if line.strip()]
                # è¿‡æ»¤æ‰ç¤ºä¾‹æ–‡æœ¬å’Œç©ºè¡Œ
                found_links = [line for line in link_lines
                               if line not in ["ç¤ºä¾‹é“¾æ¥æ–‡æœ¬", "å¦ä¸€ä¸ªé“¾æ¥æ–‡æœ¬"]]

        # 5. éªŒè¯å†…å®¹éç©º
        if not content:
            return {
                "status": "error",
                "feedback": "å†…å®¹ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨æ ‡é¢˜ä¸‹æä¾›å®é™…å†…å®¹"
            }

        # 6. è¿”å›ç¬¦åˆ Parser Contract çš„æ ¼å¼
        return {
            "status": "success",
            "content": {
                "heading_type": heading_map[chosen_heading],
                "content": content,
                "found_links": found_links
            }
        }

    

    def _extract_document_title(self, markdown: str) -> str:
        """
        ä» Markdown ä¸­æå–æ–‡æ¡£æ ‡é¢˜
        ä¼˜å…ˆçº§ï¼šç¬¬ä¸€ä¸ª # æ ‡é¢˜ > å‰ 50 å­—ç¬¦ > "æœªå‘½åæ–‡æ¡£"
        """
        # 1. å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª # æ ‡é¢˜
        lines = markdown.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()

        # 2. å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨å‰ 50 å­—ç¬¦ä½œä¸ºæ ‡é¢˜
        if len(markdown) > 50:
            return markdown[:50].strip()

        # 3. é»˜è®¤æ ‡é¢˜
        return "æœªå‘½åæ–‡æ¡£"

    async def _stream_process_markdown(
        self,
        markdown: str,
        ctx: WebSearcherContext,
        url: str,
        session: TabSession = None
    ) -> Optional[str]:
        """
        æµå¼å¤„ç† Markdown æ–‡æ¡£ï¼ˆç»Ÿä¸€å…¥å£ï¼‰

        æµç¨‹ï¼š
        1. åˆ¤æ–­é•¿åº¦ â†’ å†³å®šæ˜¯å¦éœ€è¦é€‰ç« èŠ‚
        2. é“¾æ¥é¢„å¤„ç†ï¼šä½¿ç”¨ MarkdownLinkManager æ¸…æ´—é“¾æ¥
        3. å‡†å¤‡å¾…å¤„ç†å†…å®¹ï¼ˆå…¨æ–‡ OR é€‰ä¸­ç« èŠ‚ï¼‰
        4. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        5. é€æ‰¹æµå¼å¤„ç†ï¼Œå¹¶å¤„ç†å‘ç°çš„é“¾æ¥

        Args:
            markdown: åŸå§‹ Markdown æ–‡æœ¬
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            url: å½“å‰é¡µé¢ URL
            session: å¯é€‰çš„ TabSessionï¼Œç”¨äºå°†å‘ç°çš„é“¾æ¥åŠ å…¥é˜Ÿåˆ—

        Returns:
            æ‰¾åˆ°çš„ç­”æ¡ˆï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        # 1. åˆå§‹åŒ– Link Manager å¹¶é¢„å¤„ç† Markdown
        link_manager = MarkdownLinkManager(ctx, logger=self.logger)
        clean_markdown = link_manager.process(markdown, url)

        # 2. åˆ¤æ–­é•¿åº¦
        is_long = len(clean_markdown) > ctx.chunk_threshold

        # 3. å‡†å¤‡å¾…å¤„ç†å†…å®¹
        if not is_long:
            # çŸ­æ–‡æ¡£ï¼šå…¨æ–‡å¤„ç†
            self.logger.info(f"ğŸ“„ Short document ({len(clean_markdown)} chars). Processing full text.")
            content_to_process = clean_markdown
        else:
            # é•¿æ–‡æ¡£ï¼šç”Ÿæˆç›®å½• â†’ é€‰æ‹©ç« èŠ‚
            self.logger.info(f"ğŸ“š Long document ({len(clean_markdown)} chars). Generating TOC...")
            toc = self._generate_document_toc(clean_markdown)

            if not toc or len(toc)<2:
                # æ— æ ‡é¢˜ç»“æ„æˆ–è€…åªæœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå…¨æ–‡å¤„ç†
                self.logger.info("ğŸ“‹ No headers found. Processing full text.")
                content_to_process = clean_markdown
            else:
                # è®© LLM é€‰æ‹©ç« èŠ‚ï¼ˆä¼ å…¥é¢„è§ˆå’ŒURLï¼‰
                self.logger.info(f"ğŸ“‘ Found {len(toc)} chapters. Asking LLM to select...")
                self.logger.debug(f"ã€Document TOCã€‘: {toc}")

                # æå–å‰500å­—ç¬¦ä½œä¸ºé¢„è§ˆ
                markdown_preview = clean_markdown[:500] if len(clean_markdown) > 500 else clean_markdown

                selected_indices = await self._let_llm_select_chapters(
                    toc,
                    ctx,
                    markdown_preview=markdown_preview,
                    url=url
                )

                # å¤„ç† LLM çš„é€‰æ‹©ç»“æœ
                if selected_indices == "SKIP_DOC":
                    # LLM è®¤ä¸ºæ•´ä¸ªæ–‡æ¡£ä¸ç›¸å…³ï¼Œè·³è¿‡
                    self.logger.warning("ğŸš« LLM decided to skip entire document (not relevant)")
                    # æ ‡è®°æ‰€æœ‰é“¾æ¥ä¸ºå·²è¯„ä¼°
                    for link_url in link_manager.text_to_url.values():
                        ctx.mark_evaluated(link_url)
                        #self.logger.debug(f"âœ“ Marked as evaluated: {link_url}")
                    return None  # ç›¸å½“äº skip_doc

                elif selected_indices is None or not selected_indices:
                    # None è¡¨ç¤º LLM è®¤ä¸ºTOCæ²¡ç”¨ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæ²¡é€‰ä»»ä½•ç« èŠ‚
                    self.logger.warning("âš ï¸ No chapters selected or TOC skipped. Processing full text.")
                    content_to_process = clean_markdown
                else:
                    self.logger.info(f"âœ… Selected {len(selected_indices)} chapters")
                    # æå–é€‰ä¸­ç« èŠ‚ï¼ˆåŒ…å«æ‰€æœ‰å­ç« èŠ‚ï¼‰
                    selected_parts = []

                    for idx in selected_indices:
                        chapter = toc[idx]
                        chapter_level = chapter["level"]

                        # æŸ¥æ‰¾è¯¥ç« èŠ‚çš„æ‰€æœ‰å­ç« èŠ‚ï¼ˆlevel > current_levelï¼Œç›´åˆ°é‡åˆ°åŒçº§æˆ–ä¸Šçº§ç« èŠ‚ï¼‰
                        chapter_start = chapter["start"]
                        chapter_end = chapter["end"]

                        # å‘åæŸ¥æ‰¾ï¼Œæ‰¾åˆ°è¯¥ç« èŠ‚çš„ç»“æŸä½ç½®
                        for j in range(idx + 1, len(toc)):
                            next_chapter = toc[j]
                            if next_chapter["level"] <= chapter_level:
                                # é‡åˆ°åŒçº§æˆ–ä¸Šçº§ç« èŠ‚ï¼Œåœæ­¢
                                chapter_end = next_chapter["start"]
                                break
                            # å¦åˆ™æ˜¯å­ç« èŠ‚ï¼Œç»§ç»­å‘åæŸ¥æ‰¾

                        # æå–å†…å®¹ï¼ˆåŒ…å«æ‰€æœ‰å­ç« èŠ‚ï¼‰
                        content = clean_markdown[chapter_start:chapter_end]
                        selected_parts.append(f"# {chapter['title']}\n\n{content}")

                        self.logger.debug(
                            f"Extracted chapter '{chapter['title']}' "
                            f"(level {chapter_level}, {len(content)} chars, "
                            f"positions {chapter_start}-{chapter_end})"
                        )

                    content_to_process = "\n\n".join(selected_parts)

        # 4. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        self.logger.info(f"ğŸ”ª Splitting content into batches (max {ctx.chunk_threshold} chars each)...")
        batches = self._split_by_paragraph_boundaries(content_to_process, ctx.chunk_threshold)
        total_batches = len(batches)
        self.logger.info(f"ğŸ“Š Split into {total_batches} batches")

        # 5. è·å–æ–‡æ¡£æ ‡é¢˜å’Œ TOCï¼ˆç”¨äº LLM ä¸Šä¸‹æ–‡ï¼‰
        doc_title = self._extract_document_title(content_to_process)

        # ç”Ÿæˆæˆ–å¤ç”¨ TOC
        toc = None
        if is_long:
            # é•¿æ–‡æ¡£ï¼Œå·²ç»ç”Ÿæˆè¿‡ TOCï¼ˆç¬¬ 1064 è¡Œï¼‰
            toc = self._generate_document_toc(clean_markdown)
        else:
            # çŸ­æ–‡æ¡£ï¼Œä¹Ÿç”Ÿæˆ TOCï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            toc = self._generate_document_toc(content_to_process)

        # 6. é€æ‰¹æµå¼å¤„ç†ï¼Œè¿½è¸ªä½ç½®
        current_position = 0  # å½“å‰ batch åœ¨ content_to_process ä¸­çš„èµ·å§‹ä½ç½®

        for i, batch in enumerate(batches, start=1):  # ä» 1 å¼€å§‹è®¡æ•°
            current_batch = i
            batch_start = current_position
            batch_end = current_position + len(batch)
            current_position = batch_end  # æ›´æ–°ä½ç½®

            progress_pct = int((current_batch / total_batches) * 100)
            self.logger.info(
                f"ğŸ”„ Processing batch {current_batch}/{total_batches} "
                f"({progress_pct}%, {len(batch)} chars)..."
            )

            # æŸ¥æ‰¾å½“å‰ batch æ¶‰åŠçš„ç« èŠ‚
            current_chapters = []
            if toc:
                current_chapters = self._find_chapters_in_batch(
                    toc,
                    batch_start,
                    batch_end,
                    content_start=0  # content_to_process æ˜¯ç‹¬ç«‹çš„ï¼Œä» 0 å¼€å§‹
                )

            # ç»Ÿä¸€çš„æ‰¹å¤„ç†ï¼ˆä¼ å…¥ TOC å’Œç« èŠ‚ä¿¡æ¯ï¼‰
            result = await self._process_batch(
                batch,
                ctx,
                doc_title=doc_title,
                current_batch=current_batch,
                total_batches=total_batches,
                url=url,
                toc=toc if toc else None,
                current_chapters=current_chapters if current_chapters else None
            )

            # å¤„ç†ç»“æœ
            if result["heading_type"] == "answer":
                # æ‰¾åˆ°ç­”æ¡ˆï¼Œç«‹å³è¿”å›
                self.logger.info(f"âœ… Answer found in batch {current_batch}!")
                return result["content"]

            elif result["heading_type"] == "note":
                # æœ‰ç”¨ä¿¡æ¯ï¼Œæ·»åŠ åˆ°å°æœ¬æœ¬
                ctx.add_to_notebook(f"[Batch {current_batch}] {result['content']}")
                self.logger.info(f"ğŸ“ Added useful info from batch {current_batch}")

            elif result["heading_type"] == "skip_doc":
                # æ–‡æ¡£ä¸ç›¸å…³ï¼Œæ”¾å¼ƒæ•´ä¸ªæ–‡æ¡£
                self.logger.warning(f"ğŸš« Document irrelevant. Skipping rest of document.")
                break

            # 7. å¤„ç†å‘ç°çš„é“¾æ¥ï¼ˆæ–°å¢é€»è¾‘ï¼‰
            if result.get("found_links"):
                for link_text in result["found_links"]:
                    target_url = link_manager.get_url(link_text)
                    if target_url:
                        self.logger.info(f"ğŸ”— Context-aware link discovered: [{link_text}] -> {target_url}")
                        # å¦‚æœæœ‰ sessionï¼Œç›´æ¥åŠ å…¥é˜Ÿåˆ—ï¼›å¦åˆ™æš‚å­˜åˆ° ctx
                        if session:
                            session.pending_link_queue.append(target_url)
                        else:
                            ctx.add_pending_link(target_url)
                    else:
                        self.logger.warning(f"âš ï¸ LLM hallucinated link text: {link_text}")

            # heading_type == "continue": ä»€ä¹ˆéƒ½ä¸åšï¼Œç»§ç»­ä¸‹ä¸€æ‰¹

        # 8. æ ‡è®°æ‰€æœ‰é“¾æ¥ä¸ºå·²è¯„ä¼°ï¼ˆé‡è¦ï¼ï¼‰
        # _stream_process_markdown æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯„ä¼°è¿‡ç¨‹
        # æ‰€æœ‰ link_manager ä¸­çš„é“¾æ¥éƒ½å·²è¢« LLM è¯„ä¼°è¿‡ï¼ˆè¦ä¹ˆæ¨èï¼Œè¦ä¹ˆæœªæ¨èï¼‰
        for url in link_manager.text_to_url.values():
            ctx.mark_evaluated(url)
            #self.logger.debug(f"âœ“ Marked as evaluated: {url}")

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    def _search_result_links_parser(self, llm_reply, link_manager) -> callable:


        # 1. è°ƒç”¨ multi_section_parser æå– ##æ¨èé“¾æ¥
        result = multi_section_parser(
            llm_reply,
            section_headers=["##æ¨èé“¾æ¥"],
            match_mode="ANY"
        )

        self.logger.debug(f"ã€Search Result Links Parser Outputã€‘:\n{result}")

        if result["status"] == "error":
            return result

        # 2. æå–é“¾æ¥åˆ—è¡¨
        links_section = result["content"].get("##æ¨èé“¾æ¥", "")

        # æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
        link_lines = [
            line.strip()
            for line in links_section.split('\n')
            if line.strip()
        ]

        # 3. æ£€æŸ¥æ˜¯å¦é€‰æ‹©è·³è¿‡
        if "SKIP_PAGE" in link_lines:
            self.logger.info("â­ï¸ LLM decided to skip this search result batch (no relevant results)")
            return {
                "status": "success",
                "content": {
                    "found_links": []  # ç©ºåˆ—è¡¨è¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°ç›¸å…³é“¾æ¥
                }
            }

        # 4. è¿‡æ»¤æ‰ç¤ºä¾‹æ–‡æœ¬
        found_links = [
            line for line in link_lines
            if line not in ["ç¬¬ä¸€ä¸ªé“¾æ¥", "å¦ä¸€ä¸ªé“¾æ¥", "SKIP_PAGE"]
        ]
        self.logger.debug(f"ã€Extracted Links (raw)ã€‘:\n{found_links}")

        # 5. å®½æ¾éªŒè¯ï¼šè¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥ï¼Œä¿ç•™æœ‰æ•ˆé“¾æ¥
        valid_links = []

        for link_text in found_links:
            if link_manager.get_url(link_text):
                # æœ‰æ•ˆé“¾æ¥
                valid_links.append(link_text)

        # 6. åˆ¤æ–­ç»“æœ
        if valid_links:
            # æœ‰æœ‰æ•ˆé“¾æ¥ï¼ŒæˆåŠŸï¼ˆå¿½ç•¥æ— æ•ˆé“¾æ¥ï¼‰

            self.logger.info(f"âœ… Extracted {len(valid_links)} valid links from LLM output")
            return {
                "status": "success",
                "content": {
                    "found_links": valid_links
                }
            }
        else:
            # æ²¡æœ‰ä»»ä½•æœ‰æ•ˆé“¾æ¥ï¼Œä½†LLMä¹Ÿæ²¡æœ‰é€‰æ‹©SKIP_PAGE
            # è§†ä¸º"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³é“¾æ¥"ï¼Œè¿”å›æˆåŠŸä½†ç©ºåˆ—è¡¨
            self.logger.info("â­ï¸ No valid links found (LLM didn't explicitly SKIP, but no relevant links)")
            return {
                "status": "success",
                "content": {
                    "found_links": []
                }
            }

        return parser

    async def _stream_process_search_result(
        self,
        html: str,
        ctx: WebSearcherContext,
        url: str,
        session: TabSession = None,
        search_engine: str = "unknown"
    ) -> Optional[str]:
        """
        æµå¼å¤„ç†æœç´¢å¼•æ“ç»“æœé¡µï¼ˆé‡æ„ç‰ˆï¼‰

        æ–°çš„æµç¨‹ï¼š
        1. ä½¿ç”¨ SearchResultsParser ç›´æ¥è§£æ HTMLï¼Œæå–ç»“æ„åŒ–æ•°æ®
        2. é‡æ–°æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„ Markdown
        3. å»ºç«‹é“¾æ¥IDåˆ°URLçš„æ˜ å°„
        4. åˆ†æ‰¹å¤„ç†ï¼Œè®© LLM é€‰æ‹©å€¼å¾—è®¿é—®çš„é“¾æ¥

        Args:
            html: æœç´¢ç»“æœé¡µçš„åŸå§‹ HTML
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            url: å½“å‰é¡µé¢ URL
            session: å¯é€‰çš„ TabSessionï¼Œç”¨äºå°†å‘ç°çš„é“¾æ¥åŠ å…¥é˜Ÿåˆ—
            search_engine: æœç´¢å¼•æ“åç§° ("google" æˆ– "bing")

        Returns:
            æ‰¾åˆ°çš„ç­”æ¡ˆï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        from .search_results_parser import SearchResultsParser

        # 1. ä½¿ç”¨ä¸“é—¨çš„è§£æå™¨è§£ææœç´¢ç»“æœ
        parser = SearchResultsParser(logger=self.logger)

        # ã€æ–°å¢ã€‘è®¾ç½®URLè¿‡æ»¤å‡½æ•°
        parser.set_url_filter(ctx.should_process_url)

        parsed_data = parser.parse(html, url)

        filtered_count = parsed_data.get('filtered_count', 0)
        self.logger.info(f"âœ“ Parsed {len(parsed_data['results'])} search results from {search_engine}")
        if filtered_count > 0:
            self.logger.info(f"  â†³ {filtered_count} results filtered (already visited/evaluated)")

        # 2. æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤äº†
        if len(parsed_data['results']) == 0:
            self.logger.warning("âš ï¸ All search results have been visited/evaluated. Skipping this page.")
            # æ ‡è®°æ‰€æœ‰é“¾æ¥ä¸ºå·²è¯„ä¼°ï¼ˆè™½ç„¶å·²ç»æ²¡æœ‰äº†ï¼‰
            return None

        # 3. æ ¼å¼åŒ–ä¸º Markdown
        formatted_markdown = parser.format_as_markdown(parsed_data)

        # 4. æ„å»ºé“¾æ¥æ˜ å°„
        link_mapping = parser.build_link_mapping(parsed_data)

        self.logger.debug(f"âœ“ Built link mapping with {len(link_mapping)} entries")
        for link_id, url in list(link_mapping.items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            self.logger.debug(f"  {link_id} -> {url}")

        # 5. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        self.logger.info(f"ğŸ”ª Splitting search results into batches (max {ctx.chunk_threshold} chars each)...")
        batches = self._split_by_paragraph_boundaries(formatted_markdown, ctx.chunk_threshold)
        total_batches = len(batches)
        self.logger.info(f"ğŸ“Š Split into {total_batches} batches")

        # 6. é€æ‰¹æµå¼å¤„ç†
        for i, batch in enumerate(batches, start=1):
            current_batch = i
            progress_pct = int((current_batch / total_batches) * 100)
            self.logger.info(
                f"ğŸ”„ Processing search results batch {current_batch}/{total_batches} "
                f"({progress_pct}%, {len(batch)} chars)..."
            )

            # æ„é€ æœç´¢ç»“æœæ‰¹å¤„ç† prompt
            prompt = WebSearcherPrompts.SEARCH_RESULT_BATCH.format(
                question=ctx.purpose,
                search_engine=search_engine,
                notebook=ctx.notebook,
                batch_text=batch
            )
            self.logger.debug(f"ã€Search Result Batch Promptã€‘:\n{prompt}")

            try:
                # åˆ›å»º Link Manager å®ä¾‹
                link_manager = StructuredLinkManager(link_mapping)

                # ä½¿ç”¨ think_with_retry è‡ªåŠ¨å¤„ç†é‡è¯•
                # ä½¿ç”¨ä¸“é—¨çš„ parserï¼Œä¼šéªŒè¯é“¾æ¥æ˜¯å¦å­˜åœ¨ï¼Œé¿å…å¹»è§‰
                result_data = await self.cerebellum.backend.think_with_retry(
                    initial_messages=prompt,
                    parser=self._search_result_links_parser,
                    link_manager=link_manager,
                    max_retries=5
                )
                self.logger.debug(f"ã€Search Result Batch LLM Outputã€‘:\n{result_data}")

                # æå–æ‰¾åˆ°çš„é“¾æ¥
                found_links = result_data.get("found_links", [])
                self.logger.info(f"âœ… Found {len(found_links)} recommended links from search results")

            except ValueError as e:
                import traceback
                traceback.print_exc()
                self.logger.error(f"âŒ Search results batch processing failed after all retries: {e}")
                found_links = []

            # å¤„ç†å‘ç°çš„é“¾æ¥
            if found_links:
                for link_id in found_links:
                    # ä» link_id ä¸­æå–å®é™…URL
                    target_url = link_manager.get_url(link_id)
                    if target_url:
                        self.logger.info(f"ğŸ”— Search result link discovered: [{link_id}] -> {target_url}")
                        # å¦‚æœæœ‰ sessionï¼Œç›´æ¥åŠ å…¥é˜Ÿåˆ—ï¼›å¦åˆ™æš‚å­˜åˆ° ctx
                        if session:
                            session.pending_link_queue.append(target_url)
                        else:
                            ctx.add_pending_link(target_url)
                    else:
                        self.logger.warning(f"âš ï¸ Could not find URL for link_id: {link_id}")

        # 7. æ ‡è®°æ‰€æœ‰é“¾æ¥ä¸ºå·²è¯„ä¼°ï¼ˆé‡è¦ï¼ï¼‰
        for url in link_mapping.values():
            ctx.mark_evaluated(url)
            #self.logger.debug(f"âœ“ Marked as evaluated: {url}")

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    async def _process_navigation_page(
        self,
        tab,
        ctx: WebSearcherContext,
        url: str,
        session: TabSession = None
    ) -> Optional[str]:
        """
        å¤„ç†å¯¼èˆªé¡µ/ç´¢å¼•é¡µï¼ˆé¦–é¡µã€é¢‘é“é¡µç­‰ï¼‰

        æµç¨‹ï¼š
        1. é€šè¿‡ jina.ai è·å–é¡µé¢çš„ Markdown
        2. æå–æ‰€æœ‰é“¾æ¥ï¼ˆè¿‡æ»¤çº¯å›¾ç‰‡é“¾æ¥ï¼‰
        3. æ ¼å¼åŒ–ä¸ºç±»ä¼¼æœç´¢ç»“æœçš„æ ¼å¼ï¼ˆå¸¦é“¾æ¥IDï¼‰
        4. æ„å»ºé“¾æ¥IDåˆ°URLçš„æ˜ å°„
        5. è®© LLM é€‰æ‹©å€¼å¾—è®¿é—®çš„é“¾æ¥

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µ
            ctx: æœç´¢ä¸Šä¸‹æ–‡
            url: å½“å‰é¡µé¢ URL
            session: TabSessionï¼Œç”¨äºå°†å‘ç°çš„é“¾æ¥åŠ å…¥é˜Ÿåˆ—

        Returns:
            æ‰¾åˆ°çš„ç­”æ¡ˆï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        # 1. é€šè¿‡ jina.ai è·å– markdown
        self.logger.info(f"ğŸ“¥ Fetching navigation page markdown from jina.ai...")
        try:
            markdown = await self.get_markdown_via_jina(url, timeout=30)
            self.logger.info(f"âœ“ Received {len(markdown)} characters from jina.ai")
        except Exception as e:
            self.logger.error(f"âŒ Failed to fetch markdown from jina.ai: {e}")
            return None

        # 2. æå–é“¾æ¥
        self.logger.info(f"ğŸ” Extracting links from navigation page...")
        links = self.extract_navigation_links(markdown)
        self.logger.info(f"âœ“ Extracted {len(links)} links (pure images filtered)")

        if len(links) == 0:
            self.logger.warning("âš ï¸ No links found in navigation page")
            return None

        # 3. è¿‡æ»¤å·²è®¿é—®/å·²è¯„ä¼°çš„é“¾æ¥
        filtered_links = []
        for link in links:
            if ctx.should_process_url(link.url, session.pending_link_queue if session else []):
                filtered_links.append(link)

        filtered_count = len(links) - len(filtered_links)
        if filtered_count > 0:
            self.logger.info(f"  â†³ Filtered {filtered_count} already visited/evaluated links")

        if len(filtered_links) == 0:
            self.logger.warning("âš ï¸ All navigation links have been visited/evaluated. Skipping this page.")
            return None

        # 4. æ ¼å¼åŒ–ä¸º Markdownï¼ˆç±»ä¼¼æœç´¢ç»“æœæ ¼å¼ï¼‰
        formatted_markdown = self.format_navigation_links_as_markdown(filtered_links, url)

        # 5. æ„å»ºé“¾æ¥æ˜ å°„
        link_mapping = self.build_navigation_link_mapping(filtered_links)

        self.logger.debug(f"âœ“ Built link mapping with {len(link_mapping)} entries")
        for link_id, mapped_url in list(link_mapping.items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            self.logger.debug(f"  {link_id} -> {mapped_url}")

        # 6. æŒ‰æ®µè½è¾¹ç•Œåˆ†æˆæ‰¹æ¬¡
        self.logger.info(f"ğŸ”ª Splitting navigation links into batches (max {ctx.chunk_threshold} chars each)...")
        batches = self._split_by_paragraph_boundaries(formatted_markdown, ctx.chunk_threshold)
        total_batches = len(batches)
        self.logger.info(f"ğŸ“Š Split into {total_batches} batches")

        # 7. é€æ‰¹æµå¼å¤„ç†
        for i, batch in enumerate(batches, start=1):
            current_batch = i
            progress_pct = int((current_batch / total_batches) * 100)
            self.logger.info(
                f"ğŸ”„ Processing navigation batch {current_batch}/{total_batches} "
                f"({progress_pct}%, {len(batch)} chars)..."
            )

            # æ„é€ å¯¼èˆªé¡µæ‰¹å¤„ç† prompt
            prompt = WebSearcherPrompts.NAVIGATION_PAGE.format(
                question=ctx.purpose,
                notebook=ctx.notebook,
                navigation_text=batch
            )
            self.logger.debug(f"ã€Navigation Page Batch Promptã€‘:\n{prompt}")

            try:
                # åˆ›å»ºé€šç”¨çš„ StructuredLinkManager å®ä¾‹
                link_manager = StructuredLinkManager(link_mapping)

                # ä½¿ç”¨ think_with_retry è‡ªåŠ¨å¤„ç†é‡è¯•
                result_data = await self.cerebellum.backend.think_with_retry(
                    initial_messages=prompt,
                    parser=self._search_result_links_parser,
                    link_manager=link_manager,
                    max_retries=5
                )
                self.logger.debug(f"ã€Navigation Page Batch LLM Outputã€‘:\n{result_data}")

                # æå–æ‰¾åˆ°çš„é“¾æ¥
                found_links = result_data.get("found_links", [])
                self.logger.info(f"âœ… Found {len(found_links)} recommended links from navigation page")

            except ValueError as e:
                import traceback
                traceback.print_exc()
                self.logger.error(f"âŒ Navigation page batch processing failed after all retries: {e}")
                found_links = []

            # å¤„ç†å‘ç°çš„é“¾æ¥
            if found_links:
                for link_id in found_links:
                    target_url = link_manager.get_url(link_id)
                    if target_url:
                        self.logger.info(f"ğŸ”— Navigation link discovered: [{link_id}] -> {target_url}")
                        session.pending_link_queue.append(target_url)
                    else:
                        self.logger.warning(f"âš ï¸ Could not find URL for link_id: {link_id}")

        # 8. æ ‡è®°æ‰€æœ‰é“¾æ¥ä¸ºå·²è¯„ä¼°ï¼ˆé‡è¦ï¼ï¼‰
        for nav_link in filtered_links:
            ctx.mark_evaluated(nav_link.url)
            #self.logger.debug(f"âœ“ Marked as evaluated: {nav_link.url}")

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    async def _run_search_lifecycle(self, session: TabSession, ctx: WebSearcherContext) -> Optional[str]:
        """
        [NEW VERSION - Simplified]
        [The Core Loop] æœç´¢ç”Ÿå‘½å‘¨æœŸï¼ˆç®€åŒ–ç‰ˆï¼‰

        æ ¸å¿ƒç†å¿µï¼šæ¯ç§é¡µé¢ç±»å‹ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰å†³ç­–ï¼Œä¸éœ€è¦é¢å¤–çš„ Scouting é˜¶æ®µ

        é¡µé¢ç±»å‹å¤„ç†æµç¨‹ï¼š
        1. æœç´¢ç»“æœé¡µ â†’ è§£æç»“æœ â†’ LLMé€‰æ‹©é“¾æ¥ â†’ åŠ å…¥é˜Ÿåˆ— â†’ ç»“æŸ
        2. å¯¼èˆªé¡µ â†’ jina.aiè·å–markdown â†’ æå–é“¾æ¥ â†’ LLMé€‰æ‹© â†’ åŠ å…¥é˜Ÿåˆ— â†’ ç»“æŸ
        3. å†…å®¹é¡µ â†’ LLMé˜…è¯»å†…å®¹ â†’ æ¨èé“¾æ¥(å·²åœ¨é˜…è¯»ä¸­) â†’ å¾—åˆ°answer â†’ ç»“æŸ

        ç›¸æ¯”æ—§ç‰ˆæœ¬çš„ä¼˜åŠ¿ï¼š
        - ä¸å†éœ€è¦Scoutingé˜¶æ®µï¼ˆ1958-2041è¡Œï¼‰
        - æ¯ç§é¡µé¢ç±»å‹éƒ½æœ‰ä¸“é—¨çš„LLMäº¤äº’ï¼Œä¸€æ¬¡æ€§å®Œæˆå†³ç­–
        - å‡å°‘é‡å¤è®©LLMçœ‹é“¾æ¥
        - æµç¨‹æ›´æ¸…æ™°ã€æ›´é«˜æ•ˆ
        """
        while not ctx.is_time_up():
            # --- Phase 1: Navigation ---
            if not session.pending_link_queue:
                self.logger.info("Queue empty. Ending search.")
                break

            next_url = session.pending_link_queue.popleft()
            self.logger.info(f"ğŸ”— Processing: {next_url}")

            # 1.1 é—¨ç¦æ£€æŸ¥ï¼ˆnext_urlï¼‰
            if ctx.has_visited(next_url):
                self.logger.debug(f"âœ“ Already visited: {next_url}")
                continue

            # 1.2 æ£€æŸ¥æ˜¯å¦æ˜¯è™šæ‹Ÿæœç´¢URL
            if next_url.startswith("search:"):
                parts = next_url.split(":", 2)
                if len(parts) != 3:
                    self.logger.error(f"Invalid virtual search URL format: {next_url}")
                    continue

                _, search_engine, search_phrase = parts
                self.logger.info(f"ğŸ” Executing search on {search_engine}: {search_phrase}")

                from agentmatrix.core.browser.google import search_google
                from agentmatrix.core.browser.bing import search_bing

                search_fun = search_google if search_engine.lower() == "google" else search_bing
                await search_fun(adapter=self.browser, tab=session.handle, query=search_phrase)
            else:
                await self.browser.navigate(session.handle, next_url)

            final_url = self.browser.get_tab_url(session.handle)
            session.current_url = final_url

            # 1.3 ä»é˜Ÿåˆ—ä¸­ç§»é™¤ final_urlï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if final_url in session.pending_link_queue:
                temp_list = list(session.pending_link_queue)
                temp_list.remove(final_url)
                session.pending_link_queue = deque(temp_list)

            # 1.4 æ£€æŸ¥ final_url æ˜¯å¦åº”è¯¥è¢«å¤„ç†
            if ctx.has_visited(final_url):
                self.logger.debug(f"âœ“ Already visited after redirect: {final_url}")
                continue

            # 1.5 æ ‡è®°å·²è®¿é—®
            ctx.mark_visited(next_url)
            ctx.mark_visited(final_url)

            # === Phase 2: Identify Page Type ===
            page_type = await self.browser.analyze_page_type(session.handle)

            if page_type == PageType.ERRO_PAGE:
                self.logger.warning(f"ğŸš« Error Page: {final_url}")
                continue

            # === åˆ†æ”¯ A: é™æ€èµ„æº ===
            if page_type == PageType.STATIC_ASSET:
                self.logger.info(f"ğŸ“„ Static Asset: {final_url}")
                markdown = await self._get_full_page_markdown(session.handle, ctx, next_url)
                answer = await self._stream_process_markdown(markdown, ctx, final_url, session)

                if answer:
                    return answer

                # å¤„ç†ä»æµå¼é˜…è¯»ä¸­å‘ç°çš„é“¾æ¥
                pending_links = ctx.get_pending_links()
                for pending_link in pending_links:
                    session.pending_link_queue.append(pending_link)
                    ctx.mark_evaluated(pending_link)

                continue

            # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ ===
            elif page_type == PageType.NAVIGABLE:
                self.logger.debug("ğŸŒ Navigable Page")
                await self.browser.stabilize(session.handle)

                # åˆ¤æ–­é¡µé¢ç±»å‹
                is_google = 'google.com/search' in final_url or 'www.google.' in final_url
                is_bing = 'bing.com/search' in final_url

                if is_google or is_bing:
                    # === ç±»å‹1: æœç´¢ç»“æœé¡µ ===
                    search_engine = "Google" if is_google else "Bing"
                    self.logger.info(f"ğŸ” {search_engine} Search Results")

                    raw_html = session.handle.html
                    answer = await self._stream_process_search_result(raw_html, ctx, final_url, session, search_engine)

                    if answer:
                        return answer

                    # æœç´¢ç»“æœé¡µå¤„ç†å®Œæˆï¼Œç»§ç»­ä¸‹ä¸€ä¸ªURL
                    continue

                else:
                    # === ç»Ÿä¸€å¤„ç†ï¼šå…ˆ extract å†…å®¹ï¼Œæ ¹æ®é•¿åº¦åˆ¤æ–­é¡µé¢ç±»å‹ ===
                    # é»˜è®¤å½“åšå†…å®¹é¡µï¼Œå…ˆå°è¯•æå–å†…å®¹
                    self.logger.info(f"ğŸ“„ Attempting to extract content: {final_url}")
                    markdown = await self._get_full_page_markdown(session.handle, ctx, next_url)

                    # æ ¹æ®æå–çš„å†…å®¹é•¿åº¦åˆ¤æ–­é¡µé¢ç±»å‹
                    content_length = len(markdown.strip()) if markdown else 0
                    self.logger.debug(f"Extracted content length: {content_length} chars")

                    # é˜ˆå€¼ï¼šå¦‚æœæå–çš„å†…å®¹å°‘äº 150 å­—ç¬¦ï¼Œè®¤ä¸ºæ˜¯å¯¼èˆªé¡µ
                    if content_length < 150:
                        # === ç±»å‹2: å¯¼èˆªé¡µï¼ˆextract å†…å®¹å¤ªå°‘ï¼‰ ===
                        self.logger.info(f"ğŸ§­ Navigation Page: {final_url} (extracted only {content_length} chars < 150 threshold)")
                        answer = await self._process_navigation_page(session.handle, ctx, final_url, session)

                        if answer:
                            return answer

                        # å¯¼èˆªé¡µå¤„ç†å®Œæˆï¼Œç»§ç»­ä¸‹ä¸€ä¸ªURL
                        continue

                    else:
                        # === ç±»å‹3: å†…å®¹é¡µï¼ˆextract æˆåŠŸï¼‰ ===
                        self.logger.info(f"ğŸ“– Content Page: {final_url} (extracted {content_length} chars)")
                        answer = await self._stream_process_markdown(markdown, ctx, final_url, session)

                        if answer:
                            return answer

                        # å¤„ç†ä»æµå¼é˜…è¯»ä¸­å‘ç°çš„é“¾æ¥
                        pending_links = ctx.get_pending_links()
                        for pending_link in pending_links:
                            session.pending_link_queue.append(pending_link)
                            ctx.mark_evaluated(pending_link)

                        # å†…å®¹é¡µå¤„ç†å®Œæˆï¼Œç»§ç»­ä¸‹ä¸€ä¸ªURL
                        continue

        # æœªæ‰¾åˆ°ç­”æ¡ˆ
        return None

    

    # ==========================================
    # 3. å°è„‘å†³ç­–è¾…åŠ©
    # ==========================================

