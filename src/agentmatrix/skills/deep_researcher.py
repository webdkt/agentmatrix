import asyncio
import time
import os,json,textwrap
import re
import sqlite3
from typing import List, Set, Dict, Optional, Any, Deque
from collections import deque
from enum import Enum, auto
from dataclasses import dataclass, field
from ..core.browser.google import search_google
from ..core.browser.bing import search_bing

from pathlib import Path


# å¼•å…¥ä¹‹å‰çš„ Adapter å®šä¹‰ (å‡è®¾åœ¨ drission_page_adapter æˆ– browser_adapter ä¸­)
from ..core.browser.browser_adapter import (
    BrowserAdapter, TabHandle, PageElement, InteractionReport, PageSnapshot, PageType
)
# å¼•å…¥å…¬å…±æ•°æ®ç»“æ„
from ..core.browser.browser_common import TabSession, BaseCrawlerContext
# å¼•å…¥çˆ¬è™«è¾…åŠ©æ–¹æ³•
from .crawler_helpers import CrawlerHelperMixin
# å¼•å…¥å…·ä½“çš„ Adapter å®ç°
from ..core.browser.drission_page_adapter import DrissionPageAdapter
from ..core.action import register_action
from .deep_researcher import DeepResearchHelper,ResearchContext
from .utils import sanitize_filename

search_func = search_bing

# ==========================================
# 1. çŠ¶æ€ä¸ä¸Šä¸‹æ–‡å®šä¹‰ (State & Context)
# ==========================================

class ContentVerdict(Enum):
    """Phase 3: é¡µé¢ä»·å€¼åˆ¤æ–­ç»“æœ"""
    TRASH = auto()              # åƒåœ¾/æ— å…³/ç™»å½•å¢™ -> å…³æ‰æˆ–è·³è¿‡
    RELEVANT_INDEX = auto()     # ç´¢å¼•é¡µ/åˆ—è¡¨é¡µ -> ä¸å€¼å¾—æ€»ç»“ï¼Œä½†å€¼å¾—æŒ–æ˜é“¾æ¥
    HIGH_VALUE = auto()         # é«˜ä»·å€¼å†…å®¹ -> æ€»ç»“å¹¶ä¿å­˜



    def _load_assessed_history(self):
        """ä»æ•°æ®åº“åŠ è½½å·²è¯„ä¼°å†å²åˆ°å†…å­˜"""
        # åŠ è½½å·²è¯„ä¼°çš„é“¾æ¥
        cursor = self._db_conn.execute("SELECT url FROM assessed_links")
        self.assessed_links = {row[0] for row in cursor}

        # åŠ è½½å·²è¯„ä¼°çš„æŒ‰é’®
        cursor = self._db_conn.execute("SELECT button_key FROM assessed_buttons")
        self.assessed_buttons = {row[0] for row in cursor}

    def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œå…³é—­æ•°æ®åº“è¿æ¥"""
        if self._db_conn:
            self._db_conn.close()
            self._db_conn = None


# ==========================================
# 2. é€»è¾‘æ ¸å¿ƒ (Logic Mixin)
# ==========================================

class DeepResearcher(DeepResearchHelper):
    """
    Deep Researcher æ ¸å¿ƒé€»è¾‘
    ä¸»æµç¨‹ï¼šç›®æ ‡ç†è§£->äººè®¾ç”Ÿæˆï¼ˆé«˜çº§ç ”ç©¶å‘˜ï¼Œç ”ç©¶å¯¼å¸ˆï¼‰->ç ”ç©¶è®¡åˆ’åˆ¶å®š-> ç ”ç©¶å¾ªç¯ -> å†™æŠ¥å‘Šå¾ªç¯ 

    context: ç ”ç©¶blueprint, ç›®æ ‡å†…å®¹åˆ†ç±»ï¼Œresearch task list (completed)
    å¯¹æ¯ä¸ª reasearch task:
        task å†…ç ”ç©¶å¾ªç¯ï¼š
            - context: bluerpint, å†…å®¹ç»“æ„åˆ†ç±», current task
            - æœç´¢ã€æµè§ˆã€é˜…è¯»å¾ªç¯
                - æ¯ä¸€æ¬¡è¯»å®Œï¼š
                    - è¦ä¸è¦æ›´æ–°ç¬”è®°æœ¬å’Œä¸“æœ‰åè¯
                - å¦‚æœPage Full ï¼ˆè¾¾åˆ°threasholdï¼‰ï¼š
                    - Summarize Page
                    - Any new question or update to blueprint?
                    - å¦‚æœæœ‰ï¼ŒDiscuss with å¯¼å¸ˆ (å½“å‰blueprint, current task, new question, draft 0)
                        - å¦‚æœè®¨è®ºæœ‰æ›´æ”¹å¿…è¦ï¼Œè¿™é‡Œä¼šæ¯”è¾ƒå¤æ‚ï¼Œè¦ä¿®æ”¹å·²æœ‰çš„ç ”ç©¶ï¼Œéœ€è¦ä¿®æ”¹blue / task listï¼Œ ç¬”è®° and Draft 0 (draft 0æ˜¯ä»ç¬”è®°summarizeç”Ÿæˆçš„ï¼‰
                        - å¦‚æœéœ€è¦ä¿®æ”¹ç¬”è®°ï¼Œæ˜¯costæ¯”è¾ƒé«˜çš„å·¥ç¨‹ï¼Œç†è®ºä¸Šéœ€è¦æ¯ä¸€é¡µéƒ½å»å›æº¯ã€‚å› ä¸ºç¬”è®°æ˜¯å®¢è§‚çš„è®°å½•ï¼Œä¸å­˜åœ¨ä¿®æ”¹çš„éœ€è¦ï¼Œåªå­˜åœ¨è¦ä¹ˆæœ‰ç”¨è¦ä¹ˆæ²¡ç”¨ã€‚æ‰€ä»¥å¦‚æœ
                        éœ€è¦æ”¹å˜ç ”ç©¶æ–¹å‘æˆ–è€…é—®é¢˜ï¼Œé€šå¸¸ä¸éœ€è¦æ”¹ç¬”è®°ï¼Œè¦æ”¹ä¹Ÿæ˜¯åˆ æ‰æ²¡ç”¨çš„ã€‚æ‰€ä»¥ä¿®æ”¹å·²æœ‰ç¬”è®°æ˜¯å‰”é™¤æ— ç”¨ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä¿®æ”¹ä¿¡æ¯ã€‚å¯¹äºæ¯ä¸€é¡µï¼Œåº”è¯¥ï¼š
                            - åˆ é™¤æ— ç”¨ç¬”è®°ï¼Œæ ¹æ®ç ”ç©¶blueprintå’Œåˆ†ç±»ï¼Œç”Ÿæˆæ–°çš„page summaryï¼ˆi.e draft 0)
                    - æ¯ä¸€é¡µå®Œæˆï¼Œéƒ½æ€è€ƒï¼Œcurrent task è¦ä¸è¦ç»§ç»­ï¼Œä¸ç»§ç»­å°±æå‰è¿”å›ï¼ˆæŠ¥å‘Štask ç»“æŸï¼‰
                
                ä¼šä¸åœå¾ªç¯ï¼Œç›´åˆ°è¶…æ—¶è¿”å›
            ä¸‹ä¸€æ¬¡task å†…å¾ªç¯ä¼šçŸ¥é“ï¼šå½“å‰taskçŠ¶æ€ï¼šï¼ˆä¸€è½®æµè§ˆé˜…è¯»ç»“æŸã€å®Œæˆã€æå‰ç»“æŸï¼‰ï¼Œåä¸¤è€…ä¼šå¯¼è‡´taskå¾ªç¯é€€å‡ºã€‚å¦‚æœæ˜¯ä¸€è½®é˜…è¯»ç»“æŸï¼Œå¯ä»¥è¯»ä¸€æ¬¡æœ¬è½®çš„æ–°page æ€»ç»“ï¼Œå’Œæœªæ€»ç»“pageï¼Œå†³å®šæ˜¯å¦ç»§ç»­


    ç ”ç©¶å¾ªç¯å®Œæˆåï¼Œè¿›å…¥å†™æŠ¥å‘Šå¾ªç¯
        ç•ªèŒ„é’Ÿæ‹¼æ¥æ³•ï¼šå¯¹æ¯ä¸€ä¸ªç« èŠ‚ï¼šæ‰¾åˆ°æ‰€æœ‰ç›¸å…³é¡µçš„Summaryï¼Œç»„æˆç« èŠ‚è‰ç¨¿ï¼Œç„¶åæ‰©å†™æ¶¦è‰²ï¼Œä¸è¦åœ¨å†™çš„æ—¶å€™æŸ¥æ–‡çŒ®ï¼Œåœ¨æ–‡æ¡£é‡Œæ‰“ä¸‰ä¸ªå¤§å¤§çš„ XXX æˆ–è€… [å¾…æŸ¥]ï¼Œ å†™å®Œäº†ç»Ÿä¸€å›å»è¡¥æ•°æ®ã€‚
        æœ€åæ±‡æ€»æˆæŠ¥å‘Š
    """



    
    @register_action(
        "åœ¨å……åˆ†ç†è§£ç”¨æˆ·çš„éœ€æ±‚åï¼Œå¼€å§‹è¿›è¡Œç ”ç©¶ã€‚ä¸ºè¿™æ¬¡ç ”ç©¶èµ·ä¸€ä¸ªç®€çŸ­çš„åå­—ï¼Œå¹¶å†™æ¸…æ¥šç ”ç©¶ç›®çš„å’Œéœ€æ±‚",
        param_infos={
            "research_title":"ç ”ç©¶çš„åå­—",
            "research_purpose": "ç ”ç©¶çš„å…·ä½“ç›®çš„å’Œéœ€æ±‚"
        }
    )
    async def start_research(self, research_title, research_purpose):
        
        ctx = ResearchContext(research_title=research_title,research_purpose=research_purpose)
        director_persona, researcher_persona = self._generate_personas(ctx)
        ctx.director_persona = director_persona
        ctx.researcher_persona = researcher_persona


    async def _discuss_research_plan(self, ctx: ResearchContext):
        #ç”Ÿæˆåˆæ­¥è®¡åˆ’
        plan_prompt = self._format_prompt(self.START_PLAN_PROMPT, ctx)
        resp = self.brain.think(plan_prompt)
        self.logger.debug(f"ğŸ¤– {resp['reasoning']}")
        plan_draft = resp['reply']
        self.logger.debug(f'{plan_draft}')
        #è®©å¯¼å¸ˆreviewè®¡åˆ’
        director_review_prompt = self._format_prompt(self.DIRECTOR_REVIEW_PROMPT, ctx, plan_draft = plan_draft)
        resp = self.brain.think(director_review_prompt)
        self.logger.debug(f"ğŸ¤– {resp['reasoning']}")
        director_suggestion = resp['reply']
        self.logger.debug(f'{director_suggestion}')
        
        




    



    @register_action(
        "ä¸ºç ”ç©¶åšå‡†å¤‡ï¼Œä¸Šç½‘æœç´¢å¹¶ä¸‹è½½ç›¸å…³èµ„æ–™ï¼Œè¦æä¾›ç ”ç©¶çš„ç›®æ ‡å’Œæœç´¢å…³é”®è¯",
        param_infos={
            "purpose": "ç ”ç©¶çš„å…·ä½“ç›®æ ‡",
            "search_phrase": "åœ¨æœç´¢å¼•æ“è¾“å…¥çš„åˆå§‹å…³é”®è¯",
            "topic": "ä¿å­˜èµ„æ–™çš„æ–‡ä»¶å¤¹åç§°",
            "max_time": "æœ€å¤§è¿è¡Œæ—¶é—´(åˆ†é’Ÿ)"
        }
    )
    async def research_crawler(self, purpose: str, search_phrase: str, topic: str, max_time: int = 30):
        """
        [Entry Point] å¤–éƒ¨è°ƒç”¨çš„å…¥å£
        """
        # 1. å‡†å¤‡ç¯å¢ƒ
        save_dir = os.path.join(self.workspace_root, "downloads", sanitize_filename(topic))

        os.makedirs(save_dir, exist_ok=True)

        profile_path = os.path.join(self.workspace_root ,".matrix", "browser_profile", self.name)
        
        self.browser_adapter = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=save_dir
        )
        
        ctx = MissionContext(
            purpose=purpose,
            save_dir=save_dir,
            deadline=time.time() + int(max_time) * 60
        )
        
        self.logger.info(f"ğŸš€ Mission Start: {purpose}")
        
        # 2. å¯åŠ¨æµè§ˆå™¨
        await self.browser_adapter.start(headless=False) # è°ƒè¯•æ¨¡å¼å…ˆå¼€æœ‰å¤´
        
        try:
            # 3. åˆå§‹é˜¶æ®µï¼šæ‰§è¡Œæœç´¢ (Phase 0)
            # æˆ‘ä»¬æŠŠæœç´¢ç»“æœé¡µå½“åšç¬¬ä¸€ä¸ª Tab çš„åˆå§‹é¡µé¢
            first_tab = await self.browser_adapter.get_tab()
            
            search_result = await search_func(self.browser_adapter, first_tab, search_phrase)
            # åˆ›å»ºåˆå§‹ Session
            initial_session = TabSession(handle=first_tab, current_url="")
            # æŠŠæœç´¢é¡µç›´æ¥æ¨å…¥é˜Ÿåˆ—ï¼Œè®© lifecycle å»å¤„ç† navigate
            for result in search_result:
                initial_session.pending_link_queue.append(result['url'])

            
            
            
            
            
            
            # 4. è¿›å…¥é€’å½’å¾ªç¯
            await self._run_tab_lifecycle(initial_session, ctx)
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            return self._generate_final_report(ctx)

        except Exception as e:
            self.logger.exception("Crawler crashed")
            return f"Mission failed with error: {e}"
        finally:
            self.logger.info("ğŸ›‘ Closing browser...")
            await self.browser_adapter.close()
            ctx.cleanup()  # å…³é—­æ•°æ®åº“è¿æ¥

    async def _run_tab_lifecycle(self, session: TabSession, ctx: MissionContext):
        """
        [The Core Loop] ç‰©ç† Tab çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚
        åªè¦é˜Ÿåˆ—ä¸ç©ºï¼Œæˆ–è€…é¡µé¢ä¸Šæœ‰äº¤äº’è¦åšï¼Œå°±ä¸€ç›´åœ¨è¿™è½¬ã€‚
        """
        
        while not ctx.is_time_up():
            
            # --- Phase 1: Navigation (ä»é˜Ÿåˆ—å–ä»»åŠ¡) ---
            # å¦‚æœå½“å‰æ²¡æœ‰åœ¨æµè§ˆç‰¹å®šé¡µé¢ï¼Œæˆ–è€…å½“å‰é¡µé¢çš„äº¤äº’éƒ½å¤„ç†å®Œäº†ï¼ˆFlagï¼‰ï¼Œåˆ™ä»é˜Ÿåˆ—å–ä¸‹ä¸€ä¸ª
            if not session.pending_link_queue:
                self.logger.info(f"Tab {session.handle} queue empty. Closing tab.")
                break # é˜Ÿåˆ—ç©ºäº†ï¼Œç»“æŸè¿™ä¸ª Tab
                
            next_url = session.pending_link_queue.popleft()
            print(next_url)
            
            # 1.1 é—¨ç¦æ£€æŸ¥
            if ctx.has_visited(next_url) or any(bl in next_url for bl in ctx.blacklist):
                continue
            
            self.logger.info(f"ğŸ”— Navigating to: {next_url}")
            nav_report = await self.browser_adapter.navigate(session.handle, next_url)
            final_url =  self.browser_adapter.get_tab_url(session.handle)
            session.current_url = final_url # æ›´æ–°å½“å‰

            # 1.2 æ ‡è®°å·²è®¿é—®    
            ctx.mark_visited(next_url)
            ctx.mark_visited(final_url)
            self.logger.info(f"ğŸ”— Landed on: {final_url}")
            # 2. äºŒæ¬¡é»‘åå•æ£€æŸ¥ (é˜²æ­¢è·³è½¬åˆ° Facebook/Login é¡µ)
            if any(bl in final_url for bl in ctx.blacklist):
                self.logger.warning(f"ğŸš« Redirected to blacklisted URL: {final_url}. Aborting tab.")
                # è¿™ç§æƒ…å†µä¸‹ï¼Œç›´æ¥ break è¿˜æ˜¯ continue?
                # è¿™æ˜¯ä¸€ä¸ª Dead Endï¼Œæ‰€ä»¥åº”è¯¥ç»“æŸå½“å‰é¡µé¢çš„å¤„ç†ï¼Œå»å¤„ç†é˜Ÿåˆ—é‡Œçš„ä¸‹ä¸€ä¸ª
                continue 

            # === Phase 2: Identify Logic Branch ===
            # å…ˆç¨³ä¸€æ‰‹ï¼Œä¸ç”¨å…¨é¡µé¢ stabilizeï¼Œåªè¦èƒ½æ‹¿åˆ° contentType å°±è¡Œ
            page_type = await self.browser_adapter.analyze_page_type(session.handle)

            if page_type == PageType.ERRO_PAGE:
                self.logger.warning(f"ğŸš« Error Page: {final_url}. Skipping.")
                continue

            # === åˆ†æ”¯ A: é™æ€èµ„æº (Dead End) ===
            if page_type == PageType.STATIC_ASSET:
                self.logger.info(f"ğŸ“„ Detected Static Asset: {session.current_url}")
                
                # 1. å°è¯•è·å–å†…å®¹ (Snapshot)
                #    å¯¹äº PDFï¼Œå¦‚æœæµè§ˆå™¨èƒ½æå–æ–‡å­—æœ€å¥½ï¼Œæå–ä¸åˆ°å°±æ‹¿æ–‡ä»¶åå’Œ URL åšæ‘˜è¦
                snapshot = await self.browser_adapter.get_page_snapshot(session.handle)
                
                # 2. å°è„‘åˆ¤æ–­ (Assess)
                #    "è¿™æ˜¯ä¸€ä¸ª PDFï¼Œæ ‡é¢˜æ˜¯ xxxï¼Œå‰ 500 å­—æ˜¯ xxx... å€¼å¾—å­˜å—ï¼Ÿ"
                #    æ³¨æ„ï¼šå¯¹äºæ— æ³•æå–æ–‡å­—çš„ Image/PDFï¼Œåªèƒ½è®©å°è„‘æ ¹æ® URL/Title ç›²çŒœ
                verdict_dict = await self._assess_page_value(snapshot, ctx)
                verdict = verdict_dict["verdict"]
                
                if verdict == ContentVerdict.HIGH_VALUE:
                    self.logger.info("ğŸ’¾ Saving Asset...")
                    # ä¿å­˜æ–‡ä»¶
                    await self.browser_adapter.save_static_asset(session.handle)
                    # è®°å½•åˆ° Context
                    ctx.knowledge_base.append({"type": "file", "url": session.current_url, "title": snapshot.title})
                
                # 3. ç»“æŸå½“å‰ URL çš„å¤„ç†
                #    å› ä¸ºæ˜¯ Static Assetï¼Œæ²¡æœ‰äº¤äº’ï¼Œæ²¡æœ‰ scoutï¼Œç›´æ¥ break (å¦‚æœæ˜¯å•é¡µ) æˆ– continue (å¦‚æœè¿˜è¦å¤„ç†é˜Ÿåˆ—)
                #    ä½†åœ¨æˆ‘ä»¬çš„é€»è¾‘é‡Œï¼ŒAsset æ˜¯ç»ˆç‚¹ï¼Œå¤„ç†å®Œå°±å¯ä»¥ä» Queue å–ä¸‹ä¸€ä¸ªäº†
                #    ä¸éœ€è¦ break Loop (Loop æ˜¯å¤„ç† Queue çš„)ï¼Œè€Œæ˜¯ continue Outer Loop
                continue 


            # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ (Infinite Possibilities) ===
            elif page_type == PageType.NAVIGABLE:
                
                # è¿›å…¥æˆ‘ä»¬ä¹‹å‰çš„å¤æ‚å¾ªç¯ï¼šStabilize -> Assess -> Scout -> Act
                # è¿™é‡Œå°±æ˜¯åŸæ¥çš„ Inner Loop ä»£ç 
                self.logger.debug("ğŸŒ Detected Navigable Page. Entering complex loop.")
                page_active = True
                page_changed = True
                one_line_summary=""
                while page_active and not ctx.is_time_up():
                    # 1. Stabilize (æ»šåŠ¨åŠ è½½)
                    
                    
                    if page_changed: 
                        #ç¬¬ä¸€æ¬¡æ°¸è¿œæ˜¯page_changedï¼Œä½†å¦‚æœç‚¹è¿‡ä¸€ä¸ªbuttonæ²¡ä»€ä¹ˆå˜åŒ–ï¼Œä»å¤´å†æ¥ä¸€æ¬¡ï¼Œå°±æ˜¯Falseäº†
                        #è¿™æ—¶å€™ä¸éœ€è¦å†å»stablize,ä¹Ÿä¸ç”¨å†çœ‹assessäº†ï¼Œç›´æ¥çœ‹scout
                        await self.browser_adapter.stabilize(session.handle)
                        # 2. Assess (HTML Extract -> Brain)
                        snapshot = await self.browser_adapter.get_page_snapshot(session.handle)
                        verdict_dict = await self._assess_page_value(snapshot, ctx)
                        verdict = verdict_dict["verdict"]
                        one_line_summary = verdict_dict["reason"]
                        
                        if verdict == ContentVerdict.HIGH_VALUE:
                            await self._save_content(snapshot, ctx) # ä¿å­˜ Summary
                        elif verdict == ContentVerdict.TRASH:
                            page_active = False; 
                            continue

                    # === Phase 4: Scouting (Look) ===
                    # æ‰«ææ‰€æœ‰å…ƒç´ 
                    links, buttons = await self.browser_adapter.scan_elements(session.handle)
                    #linksçš„æ ¼å¼ï¼š{url: text}
                    #buttonsçš„æ ¼å¼ï¼š{text: button_element}
                    self.logger.debug(f"ğŸ” Found {len(links)} links and {len(buttons)} buttons.")



                    # 4.1 å¤„ç† Links -> å…¥é˜Ÿ (ä¸ç«‹å³è®¿é—®)
                    # å¦‚æœpage_changed = Falseï¼Œè¿™ä¸ªå¯ä»¥è·³è¿‡äº†

                    if page_changed:
                        filtered_links = {}
                        for link in links:

                            # è¿‡æ»¤æ‰å·²è¯„ä¼°è¿‡çš„é“¾æ¥ï¼ˆé¿å…é‡å¤è°ƒç”¨ LLMï¼‰
                            if ctx.has_link_assessed(link):
                                continue
                            #å…ˆåˆ¤æ–­è¿™ä¸ªlinkæ˜¯å¦å·²ç»è®¿é—®è¿‡ï¼Œæˆ–è€…æ˜¯å¦åœ¨é»‘åå•ä¸­ï¼Œä»¥åŠæ˜¯ä¸æ˜¯å·²ç»åœ¨pending_link_queueé‡Œ
                            if ctx.has_visited(link):
                                continue
                            if link in session.pending_link_queue:
                                continue
                            if any(bl in link for bl in ctx.blacklist):
                                continue
                            
                            
                            filtered_links[link] = links[link]

                        selected_links = await self._filter_relevant_links(filtered_links,one_line_summary, ctx)

                        # è®°å½•æ‰€æœ‰è¯„ä¼°è¿‡çš„é“¾æ¥ï¼ˆæ— è®ºæ˜¯å¦è¢«é€‰ä¸­ï¼‰
                        for link in filtered_links:
                            ctx.mark_link_assessed(link)

                        new_links_count = 0
                        for link in selected_links:

                            session.pending_link_queue.append(link)
                            new_links_count += 1
                        self.logger.info(f"ğŸ‘€ Scouted {new_links_count} relevant links (enqueued).")

                    # 4.2 å¤„ç† Buttons -> å€™é€‰åˆ—è¡¨
                    candidate_buttons=[]
                    #åªä¿ç•™æ²¡è¯„ä¼°è¿‡çš„æŒ‰é’®
                    for button_text in buttons:
                        # è¿‡æ»¤æ‰å·²è¯„ä¼°è¿‡çš„æŒ‰é’®
                        if not ctx.has_button_assessed(session.current_url, button_text):
                            candidate_buttons.append({
                                button_text: buttons[button_text]
                            })
                

                    
                    # === Phase 5: Execution (Act) ===
                    # å°è¯•ç‚¹å‡»æœ€æœ‰ä»·å€¼çš„æŒ‰é’®
                    # å¦‚æœå°è„‘å†³å®šä¸ç‚¹ä»»ä½•æŒ‰é’®ï¼Œæˆ–è€…æ²¡æŒ‰é’®å¯ç‚¹ï¼ŒInner Loop ç»“æŸ
                    if not candidate_buttons:
                        self.logger.info("ğŸ¤” No worthy interactions found. Moving to next page in queue.")
                        page_active = False # ç»“æŸå½“å‰é¡µ
                        continue

                    chosen_button = await self._choose_best_interaction(candidate_buttons,one_line_summary, ctx)

                    # è®°å½•æ‰€æœ‰è¯„ä¼°è¿‡çš„æŒ‰é’®ï¼ˆæ— è®ºæ˜¯å¦è¢«é€‰ä¸­ï¼‰
                    assessed_button_texts = [list(btn.keys())[0] for btn in candidate_buttons]
                    ctx.mark_buttons_assessed(session.current_url, assessed_button_texts)

                    if not chosen_button:
                        self.logger.info("ğŸ¤” No worthy interactions found. Moving to next page in queue.")
                        page_active = False # ç»“æŸå½“å‰é¡µ
                        continue
                    
                    # æ‰§è¡Œç‚¹å‡»
                    self.logger.info(f"point_up: Clicking button: [{chosen_button.get_text()}]")
                    ctx.mark_interacted(session.current_url, chosen_button.get_text())
                    
                    report = await self.browser_adapter.click_and_observe(session.handle, chosen_button)
                    
                    # 5.1 å¤„ç†åæœ: æ–° Tab
                    if report.new_tabs:
                        self.logger.info(f"âœ¨ New Tab(s) detected: {len(report.new_tabs)}")
                        for new_tab_handle in report.new_tabs:
                            # é€’å½’ï¼åˆ›å»ºæ–°çš„ Session
                            new_session = TabSession(handle=new_tab_handle, current_url="", depth=session.depth + 1)
                            # ç­‰å¾…é€’å½’è¿”å›
                            await self._run_tab_lifecycle(new_session, ctx)
                            # é€’å½’å›æ¥åï¼Œå…³é—­é‚£ä¸ª tab (é€šå¸¸ lifecycle ç»“æŸæ—¶ä¼šè‡ªæ€ï¼Œè¿™é‡Œå¯ä»¥åšä¸ªä¿é™©)
                            await self.browser_adapter.close_tab(new_tab_handle)
                    
                    # 5.2 å¤„ç†åæœ: é¡µé¢å˜åŠ¨ (Soft Restart)
                    if report.is_dom_changed or report.is_url_changed:
                        self.logger.info("ğŸ”„ Page mutated. Triggering Soft Restart (Re-assess).")
                        # ä¸è®¾ç½® page_active = Falseï¼Œè€Œæ˜¯ç›´æ¥ continue Inner Loop
                        # è¿™ä¼šå¯¼è‡´é‡æ–° Stabilize -> Assess -> Scout
                        # æ³¨æ„æ›´æ–° URL
                        page_changed = True
                        if report.is_url_changed:
                            session.current_url =  self.browser_adapter.get_tab_url(session.handle) # è·å–æœ€æ–° URL
                        continue 

                    # 5.3 å¤„ç†åæœ: æ— äº‹å‘ç”Ÿæˆ–ä»…ä¸‹è½½
                    # å¦‚æœæ²¡å˜åŠ¨ï¼Œä¹Ÿæ²¡å¼¹çª—ï¼Œæˆ‘ä»¬å‡è®¾è¿™ä¸ªæŒ‰é’®ç‚¹å®Œäº†ã€‚
                    # ç»§ç»­ Inner Loop çš„ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Ÿä¸ï¼Œå› ä¸º DOM æ²¡å˜ï¼Œcandidate_buttons ä¹Ÿæ²¡å˜ã€‚
                    # æˆ‘ä»¬åº”è¯¥ç»§ç»­ä» candidate_buttons é‡Œé€‰ä¸‹ä¸€ä¸ªå—ï¼Ÿ
                    # ä¸ºäº†ç®€å•èµ·è§ï¼Œå¦‚æœç‚¹äº†ä¸€ä¸ªæŒ‰é’®æ²¡ååº”ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºâ€œè¿™é¡µæ²¡å•¥å¥½ç‚¹çš„äº†â€ï¼Œæˆ–è€…è®©å°è„‘åœ¨ä¸‹ä¸€è½®é‡æ–°é€‰ï¼ˆåæ­£å·²ç» mark interacted äº†ï¼‰
                    # è¿™é‡Œé€‰æ‹©ï¼šç»§ç»­å¾ªç¯ï¼Œè®© Assess/Scout å†è·‘ä¸€éï¼ˆæˆæœ¬ä¸é«˜ï¼‰ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
                    page_changed = False
                    continue

            
            # End Inner Loop
        
        # End Outer Loop (Queue Empty or Time Up)
        self.logger.info(f"ğŸ Tab Session ended. visited: {len(ctx.visited_urls)}")
        # è¿™é‡Œçš„ close_tab äº¤ç»™è°ƒç”¨æ–¹å¤„ç†ï¼Œæˆ–è€… adapter.close_tab(session.handle)


    # ==========================================
    # 3. å°è„‘å†³ç­–è¾…åŠ© (Brain Power)
    # ==========================================

    async def _assess_page_value(self, snapshot: PageSnapshot, ctx: MissionContext):
        """
        [Brain] è¯„ä¼°é¡µé¢ä»·å€¼ã€‚
        è¾“å…¥ï¼šé¡µé¢å¿«ç…§ (URL, Title, Text Preview)
        è¾“å‡ºï¼šContentVerdict (TRASH | RELEVANT_INDEX | HIGH_VALUE)
        """
        
        # 1. æç®€å¯å‘å¼è¿‡æ»¤ (Heuristics)
        # å¦‚æœæ˜¯ NAVIGABLE ç±»å‹ï¼Œä¸”å†…å®¹æçŸ­ (ä¾‹å¦‚ < 50 å­—ç¬¦)ï¼Œ
        # å¾€å¾€æ˜¯è„šæœ¬æ²¡åŠ è½½å‡ºæ¥ï¼Œæˆ–è€…ç¡®å®æ˜¯ç©ºé¡µã€‚
        # ä¸ºäº†é˜²æ­¢æ¼æ‰åªæœ‰å›¾ç‰‡çš„é¡µé¢ï¼Œæˆ‘ä»¬ç¨å¾®å®½å®¹ä¸€ç‚¹ï¼Œäº¤ç»™ LLMï¼Œ
        # ä½†å¦‚æœè¿ Title éƒ½æ˜¯ç©ºçš„ï¼Œç›´æ¥æ‰”æ‰ã€‚
        if not snapshot.title and len(snapshot.main_text) < 10:
            self.logger.warning(f"ğŸ—‘ï¸ Empty title and content: {snapshot.url}")
            return {"verdict":ContentVerdict.TRASH, "reason":"Empty title and content"}

        # 2. æ„é€  Prompt
        # æˆªæ–­æ–‡æœ¬ï¼Œé¿å… Token æº¢å‡ºã€‚2000å­—é€šå¸¸è¶³å¤Ÿåˆ¤æ–­ä»·å€¼ã€‚
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œmain_text å¯èƒ½æ˜¯ç©ºçš„æˆ–è€…åªæœ‰å…ƒæ•°æ®ï¼Œæ²¡å…³ç³»ã€‚
        preview_text = snapshot.main_text[:2500]
        
        # é’ˆå¯¹é™æ€èµ„æºå’Œæ™®é€šç½‘é¡µä½¿ç”¨ç•¥å¾®ä¸åŒçš„ Prompt ä¾§é‡
        if snapshot.content_type == PageType.STATIC_ASSET:
            evaluation_guide = textwrap.dedent("""
            Type: STATIC FILE (PDF/Image/Doc).
            Task: Decide if this file is relevant to [Research Goal] and should be DOWNLOADED based on its Title and URL.
            Allowed Verdict Value:
            - TRASH: Completely unrelated.
            - HIGH_VALUE: The file seems relevant to the Research Goal (e.g., specific data, report, paper) or not enough information to judge. (Download anyway.)
            
            (Note: Use HIGH_VALUE if you are not sure.)
            """)
        else:
            evaluation_guide = textwrap.dedent("""
            Type: WEBPAGE.
            Task: Analyze content relevance.
            Allowed Verdict Value:
            - TRASH: 
                * Login/Signup walls, Captchas.
                * 404/Errors, "Site under construction".
                * Pure SEO spam, generic ads, "Buy now" product pages (unless research goal is shopping).
                * Completely off-topic content.
            - RELEVANT_INDEX: 
                * Hub pages, Directories, List of links (e.g., "Top 10 resources").
                * Content is relevant but short/shallow (not worth summarizing, but worth exploring links).
                * If unsure or info is sparse but looks relevant -> Choose THIS.
            - HIGH_VALUE: 
                * Detailed articles, Reports, Data tables, Technical documentation.
                * Directly answers the Research Goal with substance.
            """)

        prompt = textwrap.dedent(f"""
        You are a Research Assistant.
        
        [Research Goal]
        "{ctx.purpose}"

        [Target Info]
        URL: {snapshot.url}
        Title: {snapshot.title}

        [Evaluation Guide]
        {evaluation_guide}

        [Content Preview]
        {preview_text}
        

        [Output Requirement]
        Return JSON ONLY. Format: {{"verdict": "one of allowed verdict values", "reason": "One line summary about the page"}}
        """)

        # 3. è°ƒç”¨å°è„‘
        try:
            # å‡è®¾ self.cerebellum.think è¿”å› dict: {'reply': '...', 'reasoning': '...'}
            # è¿™é‡Œçš„ messages æ ¼å¼å–å†³äºä½ çš„åº•å±‚ LLM æ¥å£ï¼Œè¿™é‡ŒæŒ‰å¸¸è§æ ¼å¼å†™
            response = await self.cerebellum.backend.think(
                messages=[{"role": "user", "content": prompt}]
            )
            raw_reasoning = response.get('reasoning', '').strip()
            raw_reply = response.get('reply', '').strip()
            #self.logger.debug(f"ğŸ§  Brain Reply: {raw_reply} \n\n Reasoning: {raw_reasoning}")
            
            # 4. è§£æç»“æœ
            # ç®€å•çš„ JSON æ¸…æ´—ï¼ˆé˜²æ­¢ LLM åŠ  markdown code blockï¼‰
            json_str = raw_reply.replace("```json", "").replace("```", "").strip()
            result = json.loads(json_str)
            
            verdict_str = result.get("verdict", "RELEVANT_INDEX").upper()
            reason = result.get("reason", "No reason provided")
            
            self.logger.info(f"ğŸ§  Brain Assess [{verdict_str}]: {snapshot.title[:30]}... | Reason: {reason}")

            if verdict_str == "HIGH_VALUE":
                return {"verdict": ContentVerdict.HIGH_VALUE, "reason": reason}
            elif verdict_str == "TRASH":
                return {"verdict": ContentVerdict.TRASH, "reason": reason}
            else:
                return {"verdict": ContentVerdict.RELEVANT_INDEX, "reason": snapshot.main_text[:800]}

        except Exception as e:
            self.logger.error(f"ğŸ§  Brain Assessment Failed: {e}. Defaulting to RELEVANT_INDEX.")
            # å‘ç”Ÿå¼‚å¸¸ï¼ˆå¦‚ JSON è§£æå¤±è´¥ã€ç½‘ç»œè¶…æ—¶ï¼‰æ—¶ï¼Œ
            # éµå¾ªâ€œé»˜è®¤å®½å®¹åŸåˆ™â€ï¼Œåªè¦ä¸æ˜¯é™æ€èµ„æºï¼Œå°±å½“ä½œ INDEX ç»§ç»­æ¢ç´¢ï¼Œé¿å…æ¼æ‰ã€‚
            if snapshot.content_type == PageType.STATIC_ASSET:
                # æ–‡ä»¶å¦‚æœåˆ¤æ–­ä¸å‡ºï¼Œé€šå¸¸ä¸ºäº†ä¿é™©èµ·è§ï¼Œå¯ä»¥è®¾ä¸º TRASH æˆ–è€… HIGH_VALUE
                # è¿™é‡Œä¸ºäº†é˜²æ­¢ä¸‹åƒåœ¾æ–‡ä»¶ï¼Œè®¾ä¸º TRASH (æˆ–è€…ä½ å¯ä»¥æ”¹ä¸º HIGH_VALUE)
                return {"verdict": ContentVerdict.HIGH_VALUE, "reason": "Static Asset"}
            return {"verdict": ContentVerdict.TRASH, "reason": "Possible related info"}

    async def _save_content(self, snapshot: PageSnapshot, ctx: MissionContext):
        """
        [Action] ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ç³»ç»Ÿã€‚
        ç­–ç•¥ï¼š
        1. çŸ­æ–‡ (< 1k chars): ç›´æ¥å­˜åŸæ–‡ï¼Œä¸æ€»ç»“ã€‚
        2. ä¸­æ–‡ (1k - 15k chars): å°è„‘è¿›è¡Œæ·±åº¦æ€»ç»“ (Deep Summary)ã€‚
        3. é•¿æ–‡ (> 15k chars): ç”Ÿæˆç®€ä»‹ (Abstract) + é™„ä¸Šå…¨æ–‡ã€‚
        """
        
        # === 1. æ–‡ä»¶åç”Ÿæˆç­–ç•¥ ===
        # ä½¿ç”¨ slugify ä¿è¯æ–‡ä»¶åå®‰å…¨ï¼Œæˆªæ–­é˜²æ­¢è¿‡é•¿
        safe_title = sanitize_filename(snapshot.title,60)
        # åŠ ä¸ªæ—¶é—´æˆ³é˜²æ­¢é‡åè¦†ç›– (æ¯”å¦‚ä¸¤ä¸ªé¡µé¢æ ‡é¢˜ä¸€æ ·)
        timestamp_suffix = str(int(time.time()))[-4:] 
        filename = f"{safe_title}_{timestamp_suffix}.md"
        save_path = os.path.join(ctx.save_dir, filename)

        text_len = len(snapshot.main_text)
        final_content = ""
        summary_type = ""

        # === 2. åˆ†çº§å¤„ç† ===

        # --- Tier A: çŸ­æ–‡ (ç›´æ¥ä¿å­˜) ---
        if text_len < 1000:
            self.logger.info(f"ğŸ’¾ Saving Short Content ({text_len} chars): {filename}")
            summary_type = "Raw (Short)"
            final_content = self._format_markdown(snapshot, "No summary generated (Content too short).", snapshot.main_text)

        # --- Tier B: ä¸­ç¯‡ (æ·±åº¦æ€»ç»“) ---
        elif text_len < 15000:
            self.logger.info(f"ğŸ“ Summarizing Medium Content ({text_len} chars)...")
            summary_type = "AI Summary"
            
            summary = await self._generate_summary(snapshot.main_text, ctx.purpose, mode="deep")
            final_content = self._format_markdown(snapshot, summary, snapshot.main_text)

        # --- Tier C: é•¿ç¯‡ (ç®€ä»‹ + åŸæ–‡) ---
        else:
            self.logger.info(f"ğŸ“š Archiving Long Content ({text_len} chars)...")
            summary_type = "Abstract + Full Text"
            
            # ç­–ç•¥ï¼šå–å‰ 5000 å­—ï¼ˆåŒ…å«ä»‹ç»ï¼‰å’Œå 2000 å­—ï¼ˆåŒ…å«ç»“è®ºï¼‰ï¼Œè·³è¿‡ä¸­é—´ç»†èŠ‚
            # è¿™æ ·å°è„‘èƒ½è¯»æ‡‚å¤§æ¦‚åœ¨è®²ä»€ä¹ˆï¼Œè€Œä¸ä¼šè¢«ä¸­é—´çš„ç»†èŠ‚æ·¹æ²¡
            partial_text = snapshot.main_text[:5000] + "\n\n...[Middle section omitted for summarization]...\n\n" + snapshot.main_text[-2000:]
            
            abstract = await self._generate_summary(partial_text, ctx.purpose, mode="abstract")
            
            note = f"**Note**: Document is very long ({text_len} chars). Below is an AI generated abstract based on intro/outro, followed by the full raw text."
            final_content = self._format_markdown(snapshot, f"{note}\n\n{abstract}", snapshot.main_text)

        # === 3. å†™å…¥æ–‡ä»¶ ===
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(final_content)
            
            # === 4. æ›´æ–°çŸ¥è¯†åº“ç´¢å¼• ===
            # è¿™æ˜¯ç»™ Brain æœ€åçœ‹çš„ Manifest
            ctx.knowledge_base.append({
                "type": "page",
                "title": snapshot.title,
                "url": snapshot.url,
                "file_path": filename, # ç›¸å¯¹è·¯å¾„
                "summary_type": summary_type,
                "size_kb": round(text_len / 1024, 1)
            })
            
        except Exception as e:
            self.logger.error(f"Failed to write file {save_path}: {e}")

    # --- è¾…åŠ©å‡½æ•° ---

    def _format_markdown(self, snapshot: PageSnapshot, summary_part: str, raw_part: str) -> str:
        """
        ç»Ÿä¸€çš„ Markdown æ–‡ä»¶æ ¼å¼
        """
        return textwrap.dedent(f"""
        # {snapshot.title}
        
        > Source: {snapshot.url}
        > Captured: {time.strftime("%Y-%m-%d %H:%M:%S")}
        
        ## ğŸ¤– AI Summary / Notes
        {summary_part}
        
        ---
        
        ## ğŸ“„ Original Content
        {raw_part}
        """).strip()

    async def _generate_summary(self, text: str, purpose: str, mode: str = "deep") -> str:
        """
        è°ƒç”¨å°è„‘ç”Ÿæˆæ€»ç»“
        """
        if mode == "deep":
            task_desc = "Create a detailed structured summary (Markdown). Focus on facts, data, and answers relevant to the Research Goal."
        else:
            task_desc = "Create a brief Abstract/Overview (1-2 paragraphs). Explain what this document is about and its potential value."

        prompt = f"""
        You are a Research Assistant.
        Research Goal: "{purpose}"
        
        Task: {task_desc}
        
        Content:
        {text}
        
        Output Markdown only.
        """
        
        try:
            resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
            return resp.get('reply', '').strip()
        except Exception:
            return "[Error: AI Summary Generation Failed]"

    import re

    def _generate_final_report(self, ctx: MissionContext) -> str:
        return f"Mission Complete. Found {len(ctx.knowledge_base)} items."