"!!! è¿‡æ—¶å¾…åˆ é™¤æˆ–è€…é‡åš !!!"
import asyncio
import time
import os,json,textwrap
import re
import sqlite3
from typing import List, Set, Dict, Optional, Any, Deque
from collections import deque
from enum import Enum, auto
from dataclasses import dataclass, field
from ..core.browser.google import search_google
from ..core.browser.bing import search_bing
from ..skills.utils import sanitize_filename
from pathlib import Path


# å¼•å…¥ä¹‹å‰çš„ Adapter å®šä¹‰ (å‡è®¾åœ¨ drission_page_adapter æˆ– browser_adapter ä¸­)
from ..core.browser.browser_adapter import (
    BrowserAdapter, TabHandle, PageElement, InteractionReport, PageSnapshot, PageType
)
# å¼•å…¥å…¬å…±æ•°æ®ç»“æ„
from ..core.browser.browser_common import TabSession, BaseCrawlerContext
# å¼•å…¥çˆ¬è™«è¾…åŠ©æ–¹æ³•
from ..skills.crawler_helpers import CrawlerHelperMixin
# å¼•å…¥å…·ä½“çš„ Adapter å®ç°
from ..core.browser.drission_page_adapter import DrissionPageAdapter
from ..core.action import register_action
from slugify import slugify

search_func = search_bing

# ==========================================
# 1. çŠ¶æ€ä¸ä¸Šä¸‹æ–‡å®šä¹‰ (State & Context)
# ==========================================

class ContentVerdict(Enum):
    """Phase 3: é¡µé¢ä»·å€¼åˆ¤æ–­ç»“æœ"""
    TRASH = auto()              # åƒåœ¾/æ— å…³/ç™»å½•å¢™ -> å…³æ‰æˆ–è·³è¿‡
    RELEVANT_INDEX = auto()     # ç´¢å¼•é¡µ/åˆ—è¡¨é¡µ -> ä¸å€¼å¾—æ€»ç»“ï¼Œä½†å€¼å¾—æŒ–æ˜é“¾æ¥
    HIGH_VALUE = auto()         # é«˜ä»·å€¼å†…å®¹ -> æ€»ç»“å¹¶ä¿å­˜

class MissionContext(BaseCrawlerContext):
    """
    å…¨å±€ä»»åŠ¡ä¸Šä¸‹æ–‡ (Global Memory)
    è·¨è¶Šæ‰€æœ‰é€’å½’å±‚çº§å…±äº«çš„æ•°æ®ã€‚
    """

    def __init__(self, purpose: str, save_dir: str, deadline: float):
        super().__init__(deadline)
        self.purpose = purpose
        self.save_dir = save_dir
        self.knowledge_base: List[Dict] = []
        self._db_conn: Optional[sqlite3.Connection] = None
        self._init_database()
        self._load_assessed_history()

    def mark_link_assessed(self, url: str):
        """æ ‡è®°é“¾æ¥ä¸ºå·²è¯„ä¼°ï¼ˆå†…å­˜ + æ•°æ®åº“ï¼‰"""
        super().mark_link_assessed(url)
        if url not in self.assessed_links:
            self._db_conn.execute(
                "INSERT OR IGNORE INTO assessed_links (url) VALUES (?)",
                (url,)
            )
            self._db_conn.commit()

    def mark_buttons_assessed(self, url: str, button_texts: List[str]):
        """æ‰¹é‡æ ‡è®°æŒ‰é’®ä¸ºå·²è¯„ä¼°ï¼ˆå†…å­˜ + æ•°æ®åº“ï¼‰"""
        super().mark_buttons_assessed(url, button_texts)
        for button_text in button_texts:
            key = f"{url}|{button_text}"
            if key not in self.assessed_buttons:
                self._db_conn.execute(
                    "INSERT OR IGNORE INTO assessed_buttons (button_key) VALUES (?)",
                    (key,)
                )
        self._db_conn.commit()

    def _init_database(self):
        """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
        db_path = os.path.join(self.save_dir, ".crawler_assessment.db")
        self._db_conn = sqlite3.connect(db_path)
        self._db_conn.execute("PRAGMA journal_mode=WAL")  # æå‡å¹¶å‘æ€§èƒ½

        # åˆ›å»ºè¡¨
        self._db_conn.execute("""
            CREATE TABLE IF NOT EXISTS assessed_links (
                url TEXT PRIMARY KEY,
                assessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self._db_conn.execute("""
            CREATE TABLE IF NOT EXISTS assessed_buttons (
                button_key TEXT PRIMARY KEY,
                assessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self._db_conn.commit()

    def _load_assessed_history(self):
        """ä»æ•°æ®åº“åŠ è½½å·²è¯„ä¼°å†å²åˆ°å†…å­˜"""
        # åŠ è½½å·²è¯„ä¼°çš„é“¾æ¥
        cursor = self._db_conn.execute("SELECT url FROM assessed_links")
        self.assessed_links = {row[0] for row in cursor}

        # åŠ è½½å·²è¯„ä¼°çš„æŒ‰é’®
        cursor = self._db_conn.execute("SELECT button_key FROM assessed_buttons")
        self.assessed_buttons = {row[0] for row in cursor}

    def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œå…³é—­æ•°æ®åº“è¿æ¥"""
        if self._db_conn:
            self._db_conn.close()
            self._db_conn = None


# ==========================================
# 2. é€»è¾‘æ ¸å¿ƒ (Logic Mixin)
# ==========================================

class DigitalInternCrawlerMixin(CrawlerHelperMixin):
    """
    æ•°å­—å®ä¹ ç”Ÿé€»è¾‘æ ¸å¿ƒã€‚
    å®ç°äº† "Observation -> Thought -> Action" çš„é€’å½’å¾ªç¯ã€‚
    """

    # ä¾èµ–æ³¨å…¥ï¼šå‡è®¾ self.cerebellum å’Œ self.logger å·²ç»ç”±ä¸»ç±»æä¾›
    
    
        

    #def start_browser(self, ctx: MissionContext):
    #    profile_path = os.path.join(self.workspace_root ,".matrix", "browser_profile", self.name)
    #    download_path = os.path.join(self.workspace_root ,"download")
    #    self.browser_adapter = DrissionPageAdapter(
    #        profile_path=profile_path,
    #        download_path=download_path
    #    )

    def _resolve_download_folder(self, folder: str) -> Optional[str]:
        """
        å®‰å…¨è·¯å¾„è§£æï¼Œç¡®ä¿ä¸‹è½½ç›®å½•åœ¨å…è®¸çš„å·¥ä½œç©ºé—´èŒƒå›´å†…

        Args:
            folder: ç”¨æˆ·æä¾›çš„ç›®å½•è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰

        Returns:
            è§£æåçš„ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²ï¼Œå¦‚æœè·¯å¾„ä¸å®‰å…¨åˆ™è¿”å› None
        """
        try:
            workspace_root = Path(self.workspace_root).resolve()
            folder_path = Path(folder)

            self.logger.debug(f"DEBUG _resolve_download_folder: workspace_root={workspace_root}")
            self.logger.debug(f"DEBUG _resolve_download_folder: folder_path={folder_path}")
            self.logger.debug(f"DEBUG _resolve_download_folder: is_absolute={folder_path.is_absolute()}")
            self.logger.debug(f"DEBUG _resolve_download_folder: current_workspace={Path(self.current_workspace)}")

            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäº current_workspace è§£æ
            if not folder_path.is_absolute():
                target_path = (Path(self.current_workspace) / folder_path).resolve()
                self.logger.debug(f"DEBUG _resolve_download_folder: Using relative path logic, target_path={target_path}")
            else:
                # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                target_path = folder_path.resolve()
                self.logger.debug(f"DEBUG _resolve_download_folder: Using absolute path logic, target_path={target_path}")

            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç›®æ ‡è·¯å¾„åœ¨ workspace_root å†…éƒ¨
            try:
                relative = target_path.relative_to(workspace_root)
                self.logger.debug(f"DEBUG _resolve_download_folder: relative_to workspace_root={relative}")
            except ValueError:
                self.logger.error(
                    f"Security Alert: Download folder is outside workspace. "
                    f"Requested: {folder}, Resolved: {target_path}, "
                    f"Allowed root: {workspace_root}"
                )
                return None

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            target_path.mkdir(parents=True, exist_ok=True)

            return str(target_path)

        except Exception as e:
            self.logger.exception(f"Failed to resolve download folder: {e}")
            return None

    @register_action(
        "ä¸‹è½½ä¸€ä¸ªæŒ‡å®šçš„æ–‡ä»¶ï¼Œæä¾›æ–‡ä»¶çš„ URL,å¯é€‰æŒ‡å®šçš„ä¿å­˜ç›®å½•",
        param_infos={
            "url": "æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥",
            "filename": "ä¿å­˜æ–‡ä»¶çš„åç§°",
            "folder": "ï¼ˆå¯é€‰ï¼‰ä¿å­˜ç›®å½•çš„åç§°",
        }
    )
    async def download_file(self, url: str, folder: str=None):
        if folder is None:
            folder = os.path.join(self.current_workspace,"downloads")

        self.logger.debug(f"DEBUG: Initial folder (type {type(folder).__name__}): {folder}")

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ folder è·¯å¾„åœ¨å…è®¸çš„å·¥ä½œç©ºé—´èŒƒå›´å†…
        folder_resolved = self._resolve_download_folder(folder)
        if not folder_resolved:
            return "Security Error: Download folder path is outside allowed workspace"

        self.logger.debug(f"DEBUG: Resolved folder: {folder_resolved}")
        self.logger.debug(f"DEBUG: workspace_root: {self.workspace_root}")

        profile_path = os.path.join(self.workspace_root ,".matrix", "browser_profile", self.name)

        self.browser_adapter = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=folder_resolved
        )

        ctx = MissionContext(
            purpose='download file',
            save_dir=folder_resolved,
            deadline=time.time() +  60
        )

        self.logger.info(f"ğŸš€ Mission Start: Download file {url}")

        # 2. å¯åŠ¨æµè§ˆå™¨
        await self.browser_adapter.start(headless=False) # è°ƒè¯•æ¨¡å¼å…ˆå¼€æœ‰å¤´

        try:
            # è®¿é—®url
            tab = await self.browser_adapter.get_tab()
            # æ³¨æ„ï¼šæµè§ˆå™¨å·²ç»åœ¨ DrissionPageAdapter ä¸­è®¾ç½®äº† download_path
            # æ‰€ä»¥è¿™é‡Œä¸åº”è¯¥å†ä¼ ç¬¬äºŒä¸ªå‚æ•°ï¼Œå¦åˆ™è·¯å¾„ä¼šè¢«æ‹¼æ¥ä¸¤æ¬¡ï¼
            res = await asyncio.to_thread(tab.download, url)
            status, file_path = res
            if status == 'success':
                self.logger.info(f"File downloaded successfully: {file_path}")
                # è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
                full_path = os.path.abspath(file_path)
                return full_path
            else:
                self.logger.error(f"Failed to download file: {file_path}")
                return 'Failed to Download'
        except Exception as e:
            self.logger.exception(e)
            return f'Failed to Download {e}'

    



    @register_action(
        "ä¸ºç ”ç©¶åšå‡†å¤‡ï¼Œä¸Šç½‘æœç´¢å¹¶ä¸‹è½½ç›¸å…³èµ„æ–™ï¼Œè¦æä¾›ç ”ç©¶çš„ç›®æ ‡å’Œæœç´¢å…³é”®è¯",
        param_infos={
            "purpose": "ç ”ç©¶çš„å…·ä½“ç›®æ ‡",
            "search_phrase": "åœ¨æœç´¢å¼•æ“è¾“å…¥çš„åˆå§‹å…³é”®è¯",
            "topic": "ä¿å­˜èµ„æ–™çš„æ–‡ä»¶å¤¹åç§°",
            "max_time": "æœ€å¤§è¿è¡Œæ—¶é—´(åˆ†é’Ÿ)"
        }
    )
    async def research_crawler(self, purpose: str, search_phrase: str, topic: str, max_time: int = 30):
        """
        [Entry Point] å¤–éƒ¨è°ƒç”¨çš„å…¥å£
        """
        # 1. å‡†å¤‡ç¯å¢ƒ
        save_dir = os.path.join(self.workspace_root, "downloads", sanitize_filename(topic))

        os.makedirs(save_dir, exist_ok=True)

        profile_path = os.path.join(self.workspace_root ,".matrix", "browser_profile", self.name)
        
        self.browser_adapter = DrissionPageAdapter(
            profile_path=profile_path,
            download_path=save_dir
        )
        
        ctx = MissionContext(
            purpose=purpose,
            save_dir=save_dir,
            deadline=time.time() + int(max_time) * 60
        )
        
        self.logger.info(f"ğŸš€ Mission Start: {purpose}")
        
        # 2. å¯åŠ¨æµè§ˆå™¨
        await self.browser_adapter.start(headless=False) # è°ƒè¯•æ¨¡å¼å…ˆå¼€æœ‰å¤´
        
        try:
            # 3. åˆå§‹é˜¶æ®µï¼šæ‰§è¡Œæœç´¢ (Phase 0)
            # æˆ‘ä»¬æŠŠæœç´¢ç»“æœé¡µå½“åšç¬¬ä¸€ä¸ª Tab çš„åˆå§‹é¡µé¢
            first_tab = await self.browser_adapter.get_tab()
            
            search_result = await search_func(self.browser_adapter, first_tab, search_phrase)
            # åˆ›å»ºåˆå§‹ Session
            initial_session = TabSession(handle=first_tab, current_url="")
            # æŠŠæœç´¢é¡µç›´æ¥æ¨å…¥é˜Ÿåˆ—ï¼Œè®© lifecycle å»å¤„ç† navigate
            for result in search_result:
                initial_session.pending_link_queue.append(result['url'])

            
            
            
            
            
            
            # 4. è¿›å…¥é€’å½’å¾ªç¯
            await self._run_tab_lifecycle(initial_session, ctx)
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            return self._generate_final_report(ctx)

        except Exception as e:
            self.logger.exception("Crawler crashed")
            return f"Mission failed with error: {e}"
        finally:
            self.logger.info("ğŸ›‘ Closing browser...")
            await self.browser_adapter.close()
            ctx.cleanup()  # å…³é—­æ•°æ®åº“è¿æ¥

    async def _run_tab_lifecycle(self, session: TabSession, ctx: MissionContext):
        """
        [The Core Loop] ç‰©ç† Tab çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚
        åªè¦é˜Ÿåˆ—ä¸ç©ºï¼Œæˆ–è€…é¡µé¢ä¸Šæœ‰äº¤äº’è¦åšï¼Œå°±ä¸€ç›´åœ¨è¿™è½¬ã€‚
        """
        
        while not ctx.is_time_up():
            
            # --- Phase 1: Navigation (ä»é˜Ÿåˆ—å–ä»»åŠ¡) ---
            # å¦‚æœå½“å‰æ²¡æœ‰åœ¨æµè§ˆç‰¹å®šé¡µé¢ï¼Œæˆ–è€…å½“å‰é¡µé¢çš„äº¤äº’éƒ½å¤„ç†å®Œäº†ï¼ˆFlagï¼‰ï¼Œåˆ™ä»é˜Ÿåˆ—å–ä¸‹ä¸€ä¸ª
            if not session.pending_link_queue:
                self.logger.info(f"Tab {session.handle} queue empty. Closing tab.")
                break # é˜Ÿåˆ—ç©ºäº†ï¼Œç»“æŸè¿™ä¸ª Tab
                
            next_url = session.pending_link_queue.popleft()
            print(next_url)
            
            # 1.1 é—¨ç¦æ£€æŸ¥
            if ctx.has_visited(next_url) or any(bl in next_url for bl in ctx.blacklist):
                continue
            
            self.logger.info(f"ğŸ”— Navigating to: {next_url}")
            nav_report = await self.browser_adapter.navigate(session.handle, next_url)
            final_url =  self.browser_adapter.get_tab_url(session.handle)
            session.current_url = final_url # æ›´æ–°å½“å‰

            # 1.2 æ ‡è®°å·²è®¿é—®    
            ctx.mark_visited(next_url)
            ctx.mark_visited(final_url)
            self.logger.info(f"ğŸ”— Landed on: {final_url}")
            # 2. äºŒæ¬¡é»‘åå•æ£€æŸ¥ (é˜²æ­¢è·³è½¬åˆ° Facebook/Login é¡µ)
            if any(bl in final_url for bl in ctx.blacklist):
                self.logger.warning(f"ğŸš« Redirected to blacklisted URL: {final_url}. Aborting tab.")
                # è¿™ç§æƒ…å†µä¸‹ï¼Œç›´æ¥ break è¿˜æ˜¯ continue?
                # è¿™æ˜¯ä¸€ä¸ª Dead Endï¼Œæ‰€ä»¥åº”è¯¥ç»“æŸå½“å‰é¡µé¢çš„å¤„ç†ï¼Œå»å¤„ç†é˜Ÿåˆ—é‡Œçš„ä¸‹ä¸€ä¸ª
                continue 

            # === Phase 2: Identify Logic Branch ===
            # å…ˆç¨³ä¸€æ‰‹ï¼Œä¸ç”¨å…¨é¡µé¢ stabilizeï¼Œåªè¦èƒ½æ‹¿åˆ° contentType å°±è¡Œ
            page_type = await self.browser_adapter.analyze_page_type(session.handle)

            if page_type == PageType.ERRO_PAGE:
                self.logger.warning(f"ğŸš« Error Page: {final_url}. Skipping.")
                continue

            # === åˆ†æ”¯ A: é™æ€èµ„æº (Dead End) ===
            if page_type == PageType.STATIC_ASSET:
                self.logger.info(f"ğŸ“„ Detected Static Asset: {session.current_url}")
                
                # 1. å°è¯•è·å–å†…å®¹ (Snapshot)
                #    å¯¹äº PDFï¼Œå¦‚æœæµè§ˆå™¨èƒ½æå–æ–‡å­—æœ€å¥½ï¼Œæå–ä¸åˆ°å°±æ‹¿æ–‡ä»¶åå’Œ URL åšæ‘˜è¦
                snapshot = await self.browser_adapter.get_page_snapshot(session.handle)
                
                # 2. å°è„‘åˆ¤æ–­ (Assess)
                #    "è¿™æ˜¯ä¸€ä¸ª PDFï¼Œæ ‡é¢˜æ˜¯ xxxï¼Œå‰ 500 å­—æ˜¯ xxx... å€¼å¾—å­˜å—ï¼Ÿ"
                #    æ³¨æ„ï¼šå¯¹äºæ— æ³•æå–æ–‡å­—çš„ Image/PDFï¼Œåªèƒ½è®©å°è„‘æ ¹æ® URL/Title ç›²çŒœ
                verdict_dict = await self._assess_page_value(snapshot, ctx)
                verdict = verdict_dict["verdict"]
                
                if verdict == ContentVerdict.HIGH_VALUE:
                    self.logger.info("ğŸ’¾ Saving Asset...")
                    # ä¿å­˜æ–‡ä»¶
                    await self.browser_adapter.save_static_asset(session.handle)
                    # è®°å½•åˆ° Context
                    ctx.knowledge_base.append({"type": "file", "url": session.current_url, "title": snapshot.title})
                
                # 3. ç»“æŸå½“å‰ URL çš„å¤„ç†
                #    å› ä¸ºæ˜¯ Static Assetï¼Œæ²¡æœ‰äº¤äº’ï¼Œæ²¡æœ‰ scoutï¼Œç›´æ¥ break (å¦‚æœæ˜¯å•é¡µ) æˆ– continue (å¦‚æœè¿˜è¦å¤„ç†é˜Ÿåˆ—)
                #    ä½†åœ¨æˆ‘ä»¬çš„é€»è¾‘é‡Œï¼ŒAsset æ˜¯ç»ˆç‚¹ï¼Œå¤„ç†å®Œå°±å¯ä»¥ä» Queue å–ä¸‹ä¸€ä¸ªäº†
                #    ä¸éœ€è¦ break Loop (Loop æ˜¯å¤„ç† Queue çš„)ï¼Œè€Œæ˜¯ continue Outer Loop
                continue 


            # === åˆ†æ”¯ B: äº¤äº’å¼ç½‘é¡µ (Infinite Possibilities) ===
            elif page_type == PageType.NAVIGABLE:
                
                # è¿›å…¥æˆ‘ä»¬ä¹‹å‰çš„å¤æ‚å¾ªç¯ï¼šStabilize -> Assess -> Scout -> Act
                # è¿™é‡Œå°±æ˜¯åŸæ¥çš„ Inner Loop ä»£ç 
                self.logger.debug("ğŸŒ Detected Navigable Page. Entering complex loop.")
                page_active = True
                page_changed = True
                one_line_summary=""
                while page_active and not ctx.is_time_up():
                    # 1. Stabilize (æ»šåŠ¨åŠ è½½)
                    
                    
                    if page_changed: 
                        #ç¬¬ä¸€æ¬¡æ°¸è¿œæ˜¯page_changedï¼Œä½†å¦‚æœç‚¹è¿‡ä¸€ä¸ªbuttonæ²¡ä»€ä¹ˆå˜åŒ–ï¼Œä»å¤´å†æ¥ä¸€æ¬¡ï¼Œå°±æ˜¯Falseäº†
                        #è¿™æ—¶å€™ä¸éœ€è¦å†å»stablize,ä¹Ÿä¸ç”¨å†çœ‹assessäº†ï¼Œç›´æ¥çœ‹scout
                        await self.browser_adapter.stabilize(session.handle)
                        # 2. Assess (HTML Extract -> Brain)
                        snapshot = await self.browser_adapter.get_page_snapshot(session.handle)
                        verdict_dict = await self._assess_page_value(snapshot, ctx)
                        verdict = verdict_dict["verdict"]
                        one_line_summary = verdict_dict["reason"]
                        
                        if verdict == ContentVerdict.HIGH_VALUE:
                            await self._save_content(snapshot, ctx) # ä¿å­˜ Summary
                        elif verdict == ContentVerdict.TRASH:
                            page_active = False; 
                            continue

                    # === Phase 4: Scouting (Look) ===
                    # æ‰«ææ‰€æœ‰å…ƒç´ 
                    links, buttons = await self.browser_adapter.scan_elements(session.handle)
                    #linksçš„æ ¼å¼ï¼š{url: text}
                    #buttonsçš„æ ¼å¼ï¼š{text: button_element}
                    self.logger.debug(f"ğŸ” Found {len(links)} links and {len(buttons)} buttons.")



                    # 4.1 å¤„ç† Links -> å…¥é˜Ÿ (ä¸ç«‹å³è®¿é—®)
                    # å¦‚æœpage_changed = Falseï¼Œè¿™ä¸ªå¯ä»¥è·³è¿‡äº†

                    if page_changed:
                        filtered_links = {}
                        for link in links:

                            # è¿‡æ»¤æ‰å·²è¯„ä¼°è¿‡çš„é“¾æ¥ï¼ˆé¿å…é‡å¤è°ƒç”¨ LLMï¼‰
                            if ctx.has_link_assessed(link):
                                continue
                            #å…ˆåˆ¤æ–­è¿™ä¸ªlinkæ˜¯å¦å·²ç»è®¿é—®è¿‡ï¼Œæˆ–è€…æ˜¯å¦åœ¨é»‘åå•ä¸­ï¼Œä»¥åŠæ˜¯ä¸æ˜¯å·²ç»åœ¨pending_link_queueé‡Œ
                            if ctx.has_visited(link):
                                continue
                            if link in session.pending_link_queue:
                                continue
                            if any(bl in link for bl in ctx.blacklist):
                                continue
                            
                            
                            filtered_links[link] = links[link]

                        selected_links = await self._filter_relevant_links(filtered_links,one_line_summary, ctx)

                        # è®°å½•æ‰€æœ‰è¯„ä¼°è¿‡çš„é“¾æ¥ï¼ˆæ— è®ºæ˜¯å¦è¢«é€‰ä¸­ï¼‰
                        for link in filtered_links:
                            ctx.mark_link_assessed(link)

                        new_links_count = 0
                        for link in selected_links:

                            session.pending_link_queue.append(link)
                            new_links_count += 1
                        self.logger.info(f"ğŸ‘€ Scouted {new_links_count} relevant links (enqueued).")

                    # 4.2 å¤„ç† Buttons -> å€™é€‰åˆ—è¡¨
                    candidate_buttons=[]
                    #åªä¿ç•™æ²¡è¯„ä¼°è¿‡çš„æŒ‰é’®
                    for button_text in buttons:
                        # è¿‡æ»¤æ‰å·²è¯„ä¼°è¿‡çš„æŒ‰é’®
                        if not ctx.has_button_assessed(session.current_url, button_text):
                            candidate_buttons.append({
                                button_text: buttons[button_text]
                            })
                

                    
                    # === Phase 5: Execution (Act) ===
                    # å°è¯•ç‚¹å‡»æœ€æœ‰ä»·å€¼çš„æŒ‰é’®
                    # å¦‚æœå°è„‘å†³å®šä¸ç‚¹ä»»ä½•æŒ‰é’®ï¼Œæˆ–è€…æ²¡æŒ‰é’®å¯ç‚¹ï¼ŒInner Loop ç»“æŸ
                    if not candidate_buttons:
                        self.logger.info("ğŸ¤” No worthy interactions found. Moving to next page in queue.")
                        page_active = False # ç»“æŸå½“å‰é¡µ
                        continue

                    chosen_button = await self._choose_best_interaction(candidate_buttons,one_line_summary, ctx)

                    # è®°å½•æ‰€æœ‰è¯„ä¼°è¿‡çš„æŒ‰é’®ï¼ˆæ— è®ºæ˜¯å¦è¢«é€‰ä¸­ï¼‰
                    assessed_button_texts = [list(btn.keys())[0] for btn in candidate_buttons]
                    ctx.mark_buttons_assessed(session.current_url, assessed_button_texts)

                    if not chosen_button:
                        self.logger.info("ğŸ¤” No worthy interactions found. Moving to next page in queue.")
                        page_active = False # ç»“æŸå½“å‰é¡µ
                        continue
                    
                    # æ‰§è¡Œç‚¹å‡»
                    self.logger.info(f"point_up: Clicking button: [{chosen_button.get_text()}]")
                    ctx.mark_interacted(session.current_url, chosen_button.get_text())
                    
                    report = await self.browser_adapter.click_and_observe(session.handle, chosen_button)
                    
                    # 5.1 å¤„ç†åæœ: æ–° Tab
                    if report.new_tabs:
                        self.logger.info(f"âœ¨ New Tab(s) detected: {len(report.new_tabs)}")
                        for new_tab_handle in report.new_tabs:
                            # é€’å½’ï¼åˆ›å»ºæ–°çš„ Session
                            new_session = TabSession(handle=new_tab_handle, current_url="", depth=session.depth + 1)
                            # ç­‰å¾…é€’å½’è¿”å›
                            await self._run_tab_lifecycle(new_session, ctx)
                            # é€’å½’å›æ¥åï¼Œå…³é—­é‚£ä¸ª tab (é€šå¸¸ lifecycle ç»“æŸæ—¶ä¼šè‡ªæ€ï¼Œè¿™é‡Œå¯ä»¥åšä¸ªä¿é™©)
                            await self.browser_adapter.close_tab(new_tab_handle)
                    
                    # 5.2 å¤„ç†åæœ: é¡µé¢å˜åŠ¨ (Soft Restart)
                    if report.is_dom_changed or report.is_url_changed:
                        self.logger.info("ğŸ”„ Page mutated. Triggering Soft Restart (Re-assess).")
                        # ä¸è®¾ç½® page_active = Falseï¼Œè€Œæ˜¯ç›´æ¥ continue Inner Loop
                        # è¿™ä¼šå¯¼è‡´é‡æ–° Stabilize -> Assess -> Scout
                        # æ³¨æ„æ›´æ–° URL
                        page_changed = True
                        if report.is_url_changed:
                            session.current_url =  self.browser_adapter.get_tab_url(session.handle) # è·å–æœ€æ–° URL
                        continue 

                    # 5.3 å¤„ç†åæœ: æ— äº‹å‘ç”Ÿæˆ–ä»…ä¸‹è½½
                    # å¦‚æœæ²¡å˜åŠ¨ï¼Œä¹Ÿæ²¡å¼¹çª—ï¼Œæˆ‘ä»¬å‡è®¾è¿™ä¸ªæŒ‰é’®ç‚¹å®Œäº†ã€‚
                    # ç»§ç»­ Inner Loop çš„ä¸‹ä¸€æ¬¡è¿­ä»£ï¼Ÿä¸ï¼Œå› ä¸º DOM æ²¡å˜ï¼Œcandidate_buttons ä¹Ÿæ²¡å˜ã€‚
                    # æˆ‘ä»¬åº”è¯¥ç»§ç»­ä» candidate_buttons é‡Œé€‰ä¸‹ä¸€ä¸ªå—ï¼Ÿ
                    # ä¸ºäº†ç®€å•èµ·è§ï¼Œå¦‚æœç‚¹äº†ä¸€ä¸ªæŒ‰é’®æ²¡ååº”ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºâ€œè¿™é¡µæ²¡å•¥å¥½ç‚¹çš„äº†â€ï¼Œæˆ–è€…è®©å°è„‘åœ¨ä¸‹ä¸€è½®é‡æ–°é€‰ï¼ˆåæ­£å·²ç» mark interacted äº†ï¼‰
                    # è¿™é‡Œé€‰æ‹©ï¼šç»§ç»­å¾ªç¯ï¼Œè®© Assess/Scout å†è·‘ä¸€éï¼ˆæˆæœ¬ä¸é«˜ï¼‰ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
                    page_changed = False
                    continue

            
            # End Inner Loop
        
        # End Outer Loop (Queue Empty or Time Up)
        self.logger.info(f"ğŸ Tab Session ended. visited: {len(ctx.visited_urls)}")
        # è¿™é‡Œçš„ close_tab äº¤ç»™è°ƒç”¨æ–¹å¤„ç†ï¼Œæˆ–è€… adapter.close_tab(session.handle)


    # ==========================================
    # 3. å°è„‘å†³ç­–è¾…åŠ© (Brain Power)
    # ==========================================

    async def _assess_page_value(self, snapshot: PageSnapshot, ctx: MissionContext):
        """
        [Brain] è¯„ä¼°é¡µé¢ä»·å€¼ã€‚
        è¾“å…¥ï¼šé¡µé¢å¿«ç…§ (URL, Title, Text Preview)
        è¾“å‡ºï¼šContentVerdict (TRASH | RELEVANT_INDEX | HIGH_VALUE)
        """
        
        # 1. æç®€å¯å‘å¼è¿‡æ»¤ (Heuristics)
        # å¦‚æœæ˜¯ NAVIGABLE ç±»å‹ï¼Œä¸”å†…å®¹æçŸ­ (ä¾‹å¦‚ < 50 å­—ç¬¦)ï¼Œ
        # å¾€å¾€æ˜¯è„šæœ¬æ²¡åŠ è½½å‡ºæ¥ï¼Œæˆ–è€…ç¡®å®æ˜¯ç©ºé¡µã€‚
        # ä¸ºäº†é˜²æ­¢æ¼æ‰åªæœ‰å›¾ç‰‡çš„é¡µé¢ï¼Œæˆ‘ä»¬ç¨å¾®å®½å®¹ä¸€ç‚¹ï¼Œäº¤ç»™ LLMï¼Œ
        # ä½†å¦‚æœè¿ Title éƒ½æ˜¯ç©ºçš„ï¼Œç›´æ¥æ‰”æ‰ã€‚
        if not snapshot.title and len(snapshot.main_text) < 10:
            self.logger.warning(f"ğŸ—‘ï¸ Empty title and content: {snapshot.url}")
            return {"verdict":ContentVerdict.TRASH, "reason":"Empty title and content"}

        # 2. æ„é€  Prompt
        # æˆªæ–­æ–‡æœ¬ï¼Œé¿å… Token æº¢å‡ºã€‚2000å­—é€šå¸¸è¶³å¤Ÿåˆ¤æ–­ä»·å€¼ã€‚
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œmain_text å¯èƒ½æ˜¯ç©ºçš„æˆ–è€…åªæœ‰å…ƒæ•°æ®ï¼Œæ²¡å…³ç³»ã€‚
        preview_text = snapshot.main_text[:2500]
        
        # é’ˆå¯¹é™æ€èµ„æºå’Œæ™®é€šç½‘é¡µä½¿ç”¨ç•¥å¾®ä¸åŒçš„ Prompt ä¾§é‡
        if snapshot.content_type == PageType.STATIC_ASSET:
            evaluation_guide = textwrap.dedent("""
            Type: STATIC FILE (PDF/Image/Doc).
            Task: Decide if this file is relevant to [Research Goal] and should be DOWNLOADED based on its Title and URL.
            Allowed Verdict Value:
            - TRASH: Completely unrelated.
            - HIGH_VALUE: The file seems relevant to the Research Goal (e.g., specific data, report, paper) or not enough information to judge. (Download anyway.)
            
            (Note: Use HIGH_VALUE if you are not sure.)
            """)
        else:
            evaluation_guide = textwrap.dedent("""
            Type: WEBPAGE.
            Task: Analyze content relevance.
            Allowed Verdict Value:
            - TRASH: 
                * Login/Signup walls, Captchas.
                * 404/Errors, "Site under construction".
                * Pure SEO spam, generic ads, "Buy now" product pages (unless research goal is shopping).
                * Completely off-topic content.
            - RELEVANT_INDEX: 
                * Hub pages, Directories, List of links (e.g., "Top 10 resources").
                * Content is relevant but short/shallow (not worth summarizing, but worth exploring links).
                * If unsure or info is sparse but looks relevant -> Choose THIS.
            - HIGH_VALUE: 
                * Detailed articles, Reports, Data tables, Technical documentation.
                * Directly answers the Research Goal with substance.
            """)

        prompt = textwrap.dedent(f"""
        You are a Research Assistant.
        
        [Research Goal]
        "{ctx.purpose}"

        [Target Info]
        URL: {snapshot.url}
        Title: {snapshot.title}

        [Evaluation Guide]
        {evaluation_guide}

        [Content Preview]
        {preview_text}
        

        [Output Requirement]
        Return JSON ONLY. Format: {{"verdict": "one of allowed verdict values", "reason": "One line summary about the page"}}
        """)

        # 3. è°ƒç”¨å°è„‘
        try:
            # å‡è®¾ self.cerebellum.think è¿”å› dict: {'reply': '...', 'reasoning': '...'}
            # è¿™é‡Œçš„ messages æ ¼å¼å–å†³äºä½ çš„åº•å±‚ LLM æ¥å£ï¼Œè¿™é‡ŒæŒ‰å¸¸è§æ ¼å¼å†™
            response = await self.cerebellum.backend.think(
                messages=[{"role": "user", "content": prompt}]
            )
            raw_reasoning = response.get('reasoning', '').strip()
            raw_reply = response.get('reply', '').strip()
            #self.logger.debug(f"ğŸ§  Brain Reply: {raw_reply} \n\n Reasoning: {raw_reasoning}")
            
            # 4. è§£æç»“æœ
            # ç®€å•çš„ JSON æ¸…æ´—ï¼ˆé˜²æ­¢ LLM åŠ  markdown code blockï¼‰
            json_str = raw_reply.replace("```json", "").replace("```", "").strip()
            result = json.loads(json_str)
            
            verdict_str = result.get("verdict", "RELEVANT_INDEX").upper()
            reason = result.get("reason", "No reason provided")
            
            self.logger.info(f"ğŸ§  Brain Assess [{verdict_str}]: {snapshot.title[:30]}... | Reason: {reason}")

            if verdict_str == "HIGH_VALUE":
                return {"verdict": ContentVerdict.HIGH_VALUE, "reason": reason}
            elif verdict_str == "TRASH":
                return {"verdict": ContentVerdict.TRASH, "reason": reason}
            else:
                return {"verdict": ContentVerdict.RELEVANT_INDEX, "reason": snapshot.main_text[:800]}

        except Exception as e:
            self.logger.error(f"ğŸ§  Brain Assessment Failed: {e}. Defaulting to RELEVANT_INDEX.")
            # å‘ç”Ÿå¼‚å¸¸ï¼ˆå¦‚ JSON è§£æå¤±è´¥ã€ç½‘ç»œè¶…æ—¶ï¼‰æ—¶ï¼Œ
            # éµå¾ªâ€œé»˜è®¤å®½å®¹åŸåˆ™â€ï¼Œåªè¦ä¸æ˜¯é™æ€èµ„æºï¼Œå°±å½“ä½œ INDEX ç»§ç»­æ¢ç´¢ï¼Œé¿å…æ¼æ‰ã€‚
            if snapshot.content_type == PageType.STATIC_ASSET:
                # æ–‡ä»¶å¦‚æœåˆ¤æ–­ä¸å‡ºï¼Œé€šå¸¸ä¸ºäº†ä¿é™©èµ·è§ï¼Œå¯ä»¥è®¾ä¸º TRASH æˆ–è€… HIGH_VALUE
                # è¿™é‡Œä¸ºäº†é˜²æ­¢ä¸‹åƒåœ¾æ–‡ä»¶ï¼Œè®¾ä¸º TRASH (æˆ–è€…ä½ å¯ä»¥æ”¹ä¸º HIGH_VALUE)
                return {"verdict": ContentVerdict.HIGH_VALUE, "reason": "Static Asset"}
            return {"verdict": ContentVerdict.TRASH, "reason": "Possible related info"}

    async def _save_content(self, snapshot: PageSnapshot, ctx: MissionContext):
        """
        [Action] ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ç³»ç»Ÿã€‚
        ç­–ç•¥ï¼š
        1. çŸ­æ–‡ (< 1k chars): ç›´æ¥å­˜åŸæ–‡ï¼Œä¸æ€»ç»“ã€‚
        2. ä¸­æ–‡ (1k - 15k chars): å°è„‘è¿›è¡Œæ·±åº¦æ€»ç»“ (Deep Summary)ã€‚
        3. é•¿æ–‡ (> 15k chars): ç”Ÿæˆç®€ä»‹ (Abstract) + é™„ä¸Šå…¨æ–‡ã€‚
        """
        
        # === 1. æ–‡ä»¶åç”Ÿæˆç­–ç•¥ ===
        # ä½¿ç”¨ slugify ä¿è¯æ–‡ä»¶åå®‰å…¨ï¼Œæˆªæ–­é˜²æ­¢è¿‡é•¿
        safe_title = sanitize_filename(snapshot.title,60)
        # åŠ ä¸ªæ—¶é—´æˆ³é˜²æ­¢é‡åè¦†ç›– (æ¯”å¦‚ä¸¤ä¸ªé¡µé¢æ ‡é¢˜ä¸€æ ·)
        timestamp_suffix = str(int(time.time()))[-4:] 
        filename = f"{safe_title}_{timestamp_suffix}.md"
        save_path = os.path.join(ctx.save_dir, filename)

        text_len = len(snapshot.main_text)
        final_content = ""
        summary_type = ""

        # === 2. åˆ†çº§å¤„ç† ===

        # --- Tier A: çŸ­æ–‡ (ç›´æ¥ä¿å­˜) ---
        if text_len < 1000:
            self.logger.info(f"ğŸ’¾ Saving Short Content ({text_len} chars): {filename}")
            summary_type = "Raw (Short)"
            final_content = self._format_markdown(snapshot, "No summary generated (Content too short).", snapshot.main_text)

        # --- Tier B: ä¸­ç¯‡ (æ·±åº¦æ€»ç»“) ---
        elif text_len < 15000:
            self.logger.info(f"ğŸ“ Summarizing Medium Content ({text_len} chars)...")
            summary_type = "AI Summary"
            
            summary = await self._generate_summary(snapshot.main_text, ctx.purpose, mode="deep")
            final_content = self._format_markdown(snapshot, summary, snapshot.main_text)

        # --- Tier C: é•¿ç¯‡ (ç®€ä»‹ + åŸæ–‡) ---
        else:
            self.logger.info(f"ğŸ“š Archiving Long Content ({text_len} chars)...")
            summary_type = "Abstract + Full Text"
            
            # ç­–ç•¥ï¼šå–å‰ 5000 å­—ï¼ˆåŒ…å«ä»‹ç»ï¼‰å’Œå 2000 å­—ï¼ˆåŒ…å«ç»“è®ºï¼‰ï¼Œè·³è¿‡ä¸­é—´ç»†èŠ‚
            # è¿™æ ·å°è„‘èƒ½è¯»æ‡‚å¤§æ¦‚åœ¨è®²ä»€ä¹ˆï¼Œè€Œä¸ä¼šè¢«ä¸­é—´çš„ç»†èŠ‚æ·¹æ²¡
            partial_text = snapshot.main_text[:5000] + "\n\n...[Middle section omitted for summarization]...\n\n" + snapshot.main_text[-2000:]
            
            abstract = await self._generate_summary(partial_text, ctx.purpose, mode="abstract")
            
            note = f"**Note**: Document is very long ({text_len} chars). Below is an AI generated abstract based on intro/outro, followed by the full raw text."
            final_content = self._format_markdown(snapshot, f"{note}\n\n{abstract}", snapshot.main_text)

        # === 3. å†™å…¥æ–‡ä»¶ ===
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(final_content)
            
            # === 4. æ›´æ–°çŸ¥è¯†åº“ç´¢å¼• ===
            # è¿™æ˜¯ç»™ Brain æœ€åçœ‹çš„ Manifest
            ctx.knowledge_base.append({
                "type": "page",
                "title": snapshot.title,
                "url": snapshot.url,
                "file_path": filename, # ç›¸å¯¹è·¯å¾„
                "summary_type": summary_type,
                "size_kb": round(text_len / 1024, 1)
            })
            
        except Exception as e:
            self.logger.error(f"Failed to write file {save_path}: {e}")

    # --- è¾…åŠ©å‡½æ•° ---

    def _format_markdown(self, snapshot: PageSnapshot, summary_part: str, raw_part: str) -> str:
        """
        ç»Ÿä¸€çš„ Markdown æ–‡ä»¶æ ¼å¼
        """
        return textwrap.dedent(f"""
        # {snapshot.title}
        
        > Source: {snapshot.url}
        > Captured: {time.strftime("%Y-%m-%d %H:%M:%S")}
        
        ## ğŸ¤– AI Summary / Notes
        {summary_part}
        
        ---
        
        ## ğŸ“„ Original Content
        {raw_part}
        """).strip()

    async def _generate_summary(self, text: str, purpose: str, mode: str = "deep") -> str:
        """
        è°ƒç”¨å°è„‘ç”Ÿæˆæ€»ç»“
        """
        if mode == "deep":
            task_desc = "Create a detailed structured summary (Markdown). Focus on facts, data, and answers relevant to the Research Goal."
        else:
            task_desc = "Create a brief Abstract/Overview (1-2 paragraphs). Explain what this document is about and its potential value."

        prompt = f"""
        You are a Research Assistant.
        Research Goal: "{purpose}"
        
        Task: {task_desc}
        
        Content:
        {text}
        
        Output Markdown only.
        """
        
        try:
            resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
            return resp.get('reply', '').strip()
        except Exception:
            return "[Error: AI Summary Generation Failed]"

    import re

    def _generate_final_report(self, ctx: MissionContext) -> str:
        return f"Mission Complete. Found {len(ctx.knowledge_base)} items."