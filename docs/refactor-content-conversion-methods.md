# å†…å®¹è½¬æ¢æ–¹æ³•é‡æ„è®¡åˆ’

**åˆ›å»ºæ—¥æœŸ**: 2025-01-08
**çŠ¶æ€**: ğŸ“ è®¡åˆ’ä¸­

---

## 1. æ¦‚è¿°

### é‡æ„ç›®æ ‡
å°† `web_searcher.py` ä¸­çš„å†…å®¹è½¬æ¢ç±»æ–¹æ³•è¿ç§»åˆ° `crawler_helpers.py`ï¼ŒåŒæ—¶å°† PDF è½¬æ¢å®ç°ä» `report_writer_utils.py` é›†æˆè¿›æ¥ï¼Œè®© `crawler_helpers` æ›´åŠ è‡ªåŒ…å«ã€‚

**æ ¸å¿ƒæ”¶ç›Š**:
- å‡å°‘ 150+ è¡Œé‡å¤ä»£ç 
- ç»Ÿä¸€ HTML/PDF è½¬ Markdown çš„å¤„ç†é€»è¾‘
- ä¸å†ä¾èµ–å¤–éƒ¨æ¨¡å—çš„ `pdf_to_markdown` import

### é‡æ„èŒƒå›´
**è¿ç§»æ–¹æ³•**:
1. `_get_full_page_markdown()` - ç»Ÿä¸€å…¥å£ï¼Œè‡ªåŠ¨è¯†åˆ« HTML/PDF
2. `_html_to_full_markdown()` - HTML è½¬ Markdownï¼ˆä½¿ç”¨ trafilaturaï¼‰
3. `_pdf_to_full_markdown()` - PDF è½¬ Markdownï¼ˆè°ƒç”¨å†…éƒ¨æ–¹æ³•ï¼‰
4. `_convert_pdf_to_markdown_text()` - PDF è½¬æ¢çš„æ ¸å¿ƒå®ç°ï¼ˆä» report_writer_utils è¿ç§»ï¼‰

---

## 2. å½“å‰çŠ¶æ€

### æºæ–‡ä»¶ï¼šweb_searcher.py

**ä½ç½®**: `src/agentmatrix/skills/web_searcher.py:409-465`

```python
async def _get_full_page_markdown(self, tab: TabHandle, ctx: WebSearcherContext) -> str:
    content_type = await self.browser.analyze_page_type(tab)
    if content_type == PageType.STATIC_ASSET:
        return await self._pdf_to_full_markdown(tab, ctx)
    else:
        return await self._html_to_full_markdown(tab)

async def _html_to_full_markdown(self, tab: TabHandle) -> str:
    import trafilatura
    raw_html = tab.html
    url = self.browser.get_tab_url(tab)
    markdown = trafilatura.extract(raw_html, include_links=True,
                                    include_formatting=True,
                                    output_format='markdown', url=url)
    if not markdown or len(markdown) < 50:
        markdown = tab.text
    return markdown or ""

async def _pdf_to_full_markdown(self, tab: TabHandle, ctx: WebSearcherContext) -> str:
    from skills.report_writer_utils import pdf_to_markdown
    pdf_path = await self.browser.save_static_asset(tab)
    markdown = pdf_to_markdown(pdf_path)
    if ctx.temp_file_dir:
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶...
    return markdown
```

**è¢«è°ƒç”¨ä½ç½®**:
- `web_searcher.py:1048` - é™æ€èµ„æºåˆ†æ”¯
- `web_searcher.py:1068` - äº¤äº’å¼ç½‘é¡µåˆ†æ”¯

### PDF è½¬æ¢å®ç°ï¼šreport_writer_utils.py

**ä½ç½®**: `src/agentmatrix/skills/report_writer_utils.py:185-257`

```python
def pdf_to_markdown(pdf_path, start_page=None, end_page=None, lang=None):
    """ä½¿ç”¨ marker åº“å°† PDF è½¬æ¢ä¸º Markdown"""
    from marker.converters.pdf import PdfConverter
    from marker.models import create_model_dict
    from marker.output import text_from_rendered
    import fitz

    # è·å–æ€»é¡µæ•°ï¼Œå¤„ç†é¡µé¢èŒƒå›´...
    converter = PdfConverter(artifact_dict=create_model_dict())
    rendered = converter(pdf_to_convert)
    text, _, images = text_from_rendered(rendered)
    return text
```

### ç›®æ ‡æ–‡ä»¶ï¼šcrawler_helpers.py

**ä½ç½®**: `src/agentmatrix/skills/crawler_helpers.py`

**ç°æœ‰å†…å®¹**: `CrawlerHelperMixin` ç±»ï¼ˆ3 ä¸ªæ–¹æ³•ï¼‰
- `_filter_relevant_links()` - æ‰¹é‡ç­›é€‰é“¾æ¥
- `_choose_best_interaction()` - é€‰æ‹©æœ€ä½³æŒ‰é’®
- `_evaluate_button_batch()` - è¯„ä¼°æŒ‰é’®æ‰¹æ¬¡

---

## 3. é‡æ„æ–¹æ¡ˆ

### å®æ–½æ–¹æ¡ˆ

**æ›´æ–° crawler_helpers.py**:

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥
from ..core.browser.browser_adapter import PageType

class CrawlerHelperMixin:
    """çˆ¬è™«æŠ€èƒ½çš„å…¬å…±è¾…åŠ©æ–¹æ³• Mixin"""

    # ... ç°æœ‰æ–¹æ³•ä¿æŒä¸å˜ ...

    async def _get_full_page_markdown(self, tab, ctx) -> str:
        """
        è·å–å®Œæ•´é¡µé¢çš„ Markdownï¼Œè‡ªåŠ¨è¯†åˆ« HTML/PDF

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        content_type = await self.browser.adapter.analyze_page_type(tab)

        if content_type == PageType.STATIC_ASSET:
            return await self._pdf_to_full_markdown(tab, ctx)
        else:
            return await self._html_to_full_markdown(tab)

    async def _html_to_full_markdown(self, tab) -> str:
        """
        å°† HTML é¡µé¢è½¬æ¢ä¸ºå®Œæ•´ Markdown

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„

        Returns:
            str: Markdown æ ¼å¼çš„é¡µé¢å†…å®¹
        """
        import trafilatura

        raw_html = tab.html
        url = self.browser.adapter.get_tab_url(tab)

        markdown = trafilatura.extract(
            raw_html,
            include_links=True,
            include_formatting=True,
            output_format='markdown',
            url=url
        )

        # é™çº§æ–¹æ¡ˆï¼šå¦‚æœ trafilatura å¤±è´¥ï¼Œä½¿ç”¨çº¯æ–‡æœ¬
        if not markdown or len(markdown) < 50:
            markdown = tab.text

        return markdown or ""

    async def _pdf_to_full_markdown(self, tab, ctx) -> str:
        """
        å°† PDF è½¬æ¢ä¸ºå®Œæ•´ Markdown

        Args:
            tab: æµè§ˆå™¨æ ‡ç­¾é¡µå¥æŸ„
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶ä¿å­˜ï¼‰

        Returns:
            str: Markdown æ ¼å¼çš„ PDF å†…å®¹
        """
        # ä¸‹è½½ PDF åˆ°æœ¬åœ°
        pdf_path = await self.browser.adapter.save_static_asset(tab)

        # è°ƒç”¨ PDF è½¬æ¢ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ï¼‰
        import asyncio
        markdown = await asyncio.to_thread(
            self._convert_pdf_to_markdown_text,
            pdf_path
        )

        # å¯é€‰ï¼šä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœ ctx æ”¯æŒï¼‰
        if hasattr(ctx, 'temp_file_dir') and ctx.temp_file_dir:
            self._save_temp_markdown(markdown, pdf_path, ctx.temp_file_dir)

        return markdown

    def _convert_pdf_to_markdown_text(self, pdf_path: str) -> str:
        """
        å°† PDF æ–‡ä»¶è½¬æ¢ä¸º Markdown æ–‡æœ¬ï¼ˆæ ¸å¿ƒå®ç°ï¼‰

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            str: Markdown æ ¼å¼çš„æ–‡æœ¬

        Raises:
            Exception: PDF è½¬æ¢å¤±è´¥
        """
        from marker.converters.pdf import PdfConverter
        from marker.models import create_model_dict
        from marker.output import text_from_rendered
        import fitz

        try:
            # è·å– PDF æ€»é¡µæ•°
            with fitz.open(pdf_path) as doc:
                total_pages = len(doc)

            self.logger.debug(f"PDFæ€»é¡µæ•°: {total_pages}")

            # åˆå§‹åŒ– marker æ¨¡å‹
            self.logger.debug("åŠ è½½ marker æ¨¡å‹...")
            converter = PdfConverter(
                artifact_dict=create_model_dict(),
            )

            # æ‰§è¡Œè½¬æ¢
            self.logger.debug("æ­£åœ¨è½¬æ¢ PDF åˆ° Markdown...")
            rendered = converter(pdf_path)

            # ä»æ¸²æŸ“ç»“æœä¸­æå–æ–‡æœ¬
            text, _, images = text_from_rendered(rendered)

            self.logger.debug(f"è½¬æ¢å®Œæˆ! å…± {len(text)} ä¸ªå­—ç¬¦")

            return text

        except Exception as e:
            self.logger.error(f"PDF è½¬æ¢å¤±è´¥: {e}")
            raise

    def _save_temp_markdown(self, markdown: str, pdf_path: str, temp_dir: str):
        """
        ä¿å­˜ä¸´æ—¶ Markdown æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            markdown: Markdown æ–‡æœ¬
            pdf_path: åŸå§‹ PDF è·¯å¾„
            temp_dir: ä¸´æ—¶ç›®å½•
        """
        import os
        from slugify import slugify

        os.makedirs(temp_dir, exist_ok=True)
        filename = slugify(f"pdf_{os.path.basename(pdf_path)}") + ".md"
        temp_path = os.path.join(temp_dir, filename)

        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(markdown)

        self.logger.info(f"ğŸ“„ ä¿å­˜ä¸´æ—¶ Markdown: {temp_path}")
```

**æ›´æ–° web_searcher.py**:

åˆ é™¤ `web_searcher.py:409-465` çš„ä¸‰ä¸ªæ–¹æ³•ï¼Œæ‰€æœ‰è°ƒç”¨ä¿æŒä¸å˜ã€‚

---

## 4. å®æ–½æ­¥éª¤

### å‰ç½®è¦æ±‚ï¼šåˆ›å»º Git Tag

```bash
# 1. ç¡®ä¿å·¥ä½œåŒºå¹²å‡€
git status

# 2. å¦‚æœ‰æœªæäº¤æ›´æ”¹ï¼Œå…ˆæäº¤
git add .
git commit -m "WIP: before content conversion refactor"

# 3. åˆ›å»ºæ ‡ç­¾
git tag -a refactor/pre-content-conversion -m "Before refactoring content conversion methods"

# 4. éªŒè¯å¹¶æ¨é€
git show refactor/pre-content-conversion --stat
git push origin refactor/pre-content-conversion
```

### æ­¥éª¤ 1: æ›´æ–° crawler_helpers.py

```bash
# ç¼–è¾‘æ–‡ä»¶ï¼Œæ·»åŠ å››ä¸ªæ–¹æ³•
vim src/agentmatrix/skills/crawler_helpers.py

# è¯­æ³•æ£€æŸ¥
python -m py_compile src/agentmatrix/skills/crawler_helpers.py

# å¯¼å…¥æ£€æŸ¥
python -c "from agentmatrix.skills.crawler_helpers import CrawlerHelperMixin; print('âœ“ Import OK')"
```

### æ­¥éª¤ 2: æ›´æ–° web_searcher.py

```bash
# åˆ é™¤ä¸‰ä¸ªæ–¹æ³•ï¼ˆè¡Œ 409-465ï¼‰
vim src/agentmatrix/skills/web_searcher.py

# è¯­æ³•æ£€æŸ¥
python -m py_compile src/agentmatrix/skills/web_searcher.py

# å¯¼å…¥æ£€æŸ¥
python -c "from agentmatrix.skills.web_searcher import WebSearcherMixin; print('âœ“ Import OK')"
```

### æ­¥éª¤ 3: æäº¤æ›´æ”¹

```bash
# æŸ¥çœ‹æ”¹åŠ¨
git diff

# æäº¤
git add .
git commit -m "Refactor: Move content conversion methods to crawler_helpers

- Move _get_full_page_markdown() from web_searcher to crawler_helpers
- Move _html_to_full_markdown() from web_searcher to crawler_helpers
- Move _pdf_to_full_markdown() from web_searcher to crawler_helpers
- Add _convert_pdf_to_markdown_text() to crawler_helpers (from report_writer_utils)
- Remove duplicate code in web_searcher.py (~150 lines)
- Make crawler_helpers self-contained for PDF conversion

See docs/refactor-content-conversion-methods.md for details"

# åˆ›å»ºåç½®æ ‡ç­¾
git tag -a refactor/post-content-conversion -m "After refactoring content conversion methods"

# æ¨é€
git push origin main
git push origin refactor/post-content-conversion
```

### æ­¥éª¤ 4: éªŒè¯

**æµ‹è¯•ç”¨ä¾‹ 1**: HTML æœç´¢
```python
from agentmatrix.agents.digital_intern import DigitalIntern

agent = DigitalIntern()
result = await agent.web_search(
    purpose="ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ",
    search_phrase="Python programming",
    max_time=2,
    max_search_pages=1
)
```

**æµ‹è¯•ç”¨ä¾‹ 2**: PDF å¤„ç†
```python
result = await agent.web_search(
    purpose="æŸ¥æ‰¾ PDF æ–‡æ¡£",
    search_phrase="example.com filetype:pdf",
    max_time=2
)
```

**éªŒè¯ç‚¹**:
- âœ… HTML è½¬ Markdown æ­£å¸¸
- âœ… PDF è½¬ Markdown æ­£å¸¸
- âœ… æ— å¯¼å…¥é”™è¯¯
- âœ… æ— æ˜æ˜¾æ€§èƒ½ä¸‹é™

---

## 5. ä¾èµ–æ¸…å•

**æ–°å¢ä¾èµ–** (crawler_helpers.py):
- `trafilatura` - HTML è§£æ
- `marker.converters.pdf.PdfConverter` - PDF è½¬æ¢
- `marker.models.create_model_dict` - æ¨¡å‹åŠ è½½
- `marker.output.text_from_rendered` - æ–‡æœ¬æå–
- `fitz` (PyMuPDF) - PDF ä¿¡æ¯è¯»å–

**ç°æœ‰ä¾èµ–**:
- `PageType` (from `core.browser.browser_adapter`)
- `slugify` - æ–‡ä»¶åå¤„ç†

**æ£€æŸ¥ä¾èµ–**:
```bash
pip list | grep -E "trafilatura|marker|PyMuPDF|slugify"
```

---

## 6. åç»­ä¼˜åŒ–å»ºè®®

1. **æ¨¡å‹ç¼“å­˜**: marker æ¨¡å‹å¯ä»¥å…¨å±€ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡è½¬æ¢éƒ½é‡æ–°åŠ è½½
2. **æ›´å¤šé™çº§ç­–ç•¥**: ä¸º PDF æ·»åŠ æ›´å¤šè½¬æ¢æ–¹æ¡ˆï¼ˆå¦‚ pdfplumberï¼‰
3. **é…ç½®åŒ–**: å…è®¸è‡ªå®šä¹‰è½¬æ¢å‚æ•°
4. **æ€§èƒ½ç›‘æ§**: æ·»åŠ è½¬æ¢æ—¶é—´ç»Ÿè®¡
5. **æ‰©å±•åˆ° data_crawler**: è®© data_crawler ä¹Ÿä½¿ç”¨è¿™äº›æ–¹æ³•

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-01-08
