import asyncio
import time
import os
import json
import textwrap
import logging
import random
import re
from urllib.parse import urljoin, urlparse
import trafilatura
from slugify import slugify
from core.action import register_action
from skills.search_tool import SmartSearcherMixin
from core.log_util import AutoLoggerMixin

# DrissionPage ä¾èµ–
from DrissionPage import ChromiumPage, ChromiumOptions

# ==========================================
# 1. é™æ€é…ç½®ä¸å·¥å…·å‡½æ•°
# ==========================================

DOMAIN_BLACKLIST = {
    'taobao.com', 'jd.com', 'tmall.com', 'amazon.com', 'ebay.com', 'temu.com',
    'youtube.com', 'bilibili.com', 'netflix.com', 'iqiyi.com',
    'facebook.com', 'instagram.com', 'twitter.com', 'x.com', 'tiktok.com', 'douyin.com',
    'linkedin.com/login', 'weibo.com'
}

def _is_safe_url(url: str) -> bool:
    """å®‰å…¨æ£€æŸ¥ï¼šé¿å¼€ç”µå•†ã€ç¤¾åª’å’ŒåŠŸèƒ½é¡µ"""
    if not url or len(url) < 5: return False
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    if any(bl in domain for bl in DOMAIN_BLACKLIST): return False
    # ç®€å•çš„æ‰©å±•åè¿‡æ»¤ï¼ˆå›¾ç‰‡ç­‰ï¼‰
    if parsed.path.lower().endswith(('.jpg', '.png', '.gif', '.css', '.js')): return False
    return True

class ClickableCandidate:
    """æ•°æ®ç±»ï¼šå°è£…é¡µé¢ä¸Šçš„å¯äº¤äº’å¯¹è±¡"""
    def __init__(self, element, text, ctype, url=None):
        self.element = element
        self.text = text.strip()
        self.type = ctype  # 'link' or 'interaction'
        self.url = url     # ä»… link æœ‰æ•ˆ

    def __repr__(self):
        return f"[{self.type.upper()}] {self.text[:20]}..."

# ==========================================
# 2. HumanBrowserAgent (æ ¸å¿ƒé€»è¾‘)
# ==========================================

class BrowserState:
    """è®°å½• Agent çš„è®¤çŸ¥çŠ¶æ€ï¼Œé˜²æ­¢æ­»å¾ªç¯"""
    def __init__(self):
        self.visited_urls = set()          # å·²è®¿é—®çš„ URL
        self.clicked_interactions = set()  # å·²ç‚¹å‡»çš„äº¤äº’ (URL + ButtonText)
        self.summarized_hashes = set()     # å·²æ€»ç»“çš„å†…å®¹ Hash

class HumanBrowser(AutoLoggerMixin):
    # å®šä¹‰é˜ˆå€¼ï¼šè¶…è¿‡è¿™ä¸ªé•¿åº¦å°±ä¸è®©å°è„‘åšæ€»ç»“äº†ï¼Œå¤ªç´¯ä¸”å®¹æ˜“å¹»è§‰
    SUMMARY_THRESHOLD = 15000 

    # === é¢„è®¾çš„é¢†åŸŸè¯„ä¼°æ ‡å‡† (Prompt Templates) ===
    PRESET_CRITERIA = {
        "STEM": textwrap.dedent("""
            [DOMAIN: SCIENCE, TECH, ENGINEERING, MATH]
            - **High Value Content**: Architecture diagrams, Mathematical formulas, Code snippets, Benchmark tables, Dataset specifications, Implementation details.
            - **Summary Style**: Structured bullet points with specific numbers and parameters.
            - **Trash**: Marketing fluff ("AI is the future"), Surface-level intros, SEO spam.
        """),

        "HUMANITIES": textwrap.dedent("""
            [DOMAIN: HISTORY, LITERATURE, PHILOSOPHY, ARTS]
            - **High Value Content**: Primary historical sources, Direct quotes, Critical analysis, Novel arguments, Chronology of events, Historiography.
            - **Summary Style**: Narrative summary capturing the core argument + Key Quotes.
            - **Trash**: Wikipedia-style generic summaries, Shallow listicles, Product sales.
        """),

        "BUSINESS": textwrap.dedent("""
            [DOMAIN: BUSINESS, FINANCE, NEWS]
            - **High Value Content**: Financial reports (10-K/10-Q), Official press releases, Executive statements, Market share data, Strategic analysis, Event timelines.
            - **Summary Style**: Executive Summary style. Focus on outcomes, numbers, and dates.
            - **Trash**: Clickbait headlines, Rumors without sources, Generic investment advice.
        """),
        "GENERAL": textwrap.dedent("""
            [DOMAIN: GENERAL KNOWLEDGE / LIFESTYLE / HOW-TO]
            - **High Value Content**: Comprehensive guides (Step-by-step), Verified factual data, Neutral encyclopedic overviews, Deep investigative reporting, High-quality tutorials.
            - **Summary Style**: Clear "How-to" steps, List of key facts, or Structured overview.
            - **Trash**: SEO Spam ("Top 10 products"), Shallow clickbait, User comments/Forum arguments (unless highly technical), Login walls.
        """)
    }
    def __init__(self, browser, context, save_dir, purpose, loop):
        self.browser = browser
        self.ctx = context
        self.save_dir = save_dir
        self.purpose = purpose
        self.state = BrowserState()
        
        # ä¿å­˜ asyncio äº‹ä»¶å¾ªç¯ï¼Œä»¥ä¾¿åœ¨åŒæ­¥çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥çš„å°è„‘
        self.loop = loop 

    def start(self, seed_urls):
        """[å…¥å£] å¯åŠ¨å•çº¿ç¨‹é€’å½’æµè§ˆ"""
        tab = self.browser.new_tab()
        # åˆå§‹å¾…åŠæ ˆ
        initial_stack = [u for u in reversed(seed_urls) if _is_safe_url(u)]
        
        try:
            self._explore_tab_loop(tab, initial_stack)
        finally:
            # æ ¹ Tab å…³é—­ï¼Œä»»åŠ¡ç»“æŸ
            try: tab.close() 
            except: pass

    def _explore_tab_loop(self, tab, pending_urls):
        """
        [é€’å½’æ ¸å¿ƒ] åœ¨ä¸€ä¸ª Tab å†…çš„ç”Ÿå‘½å‘¨æœŸå¾ªç¯
        """
        while self.ctx.is_active():
            
            # --- Phase 1: å¯¼èˆªæ§åˆ¶ (Navigation) ---
            # å¦‚æœå½“å‰æ˜¯ç©ºé¡µï¼Œæˆ–è€…ä¹‹å‰çš„äº¤äº’ç»“æŸäº†éœ€è¦å»æ–°åœ°æ–¹
            if tab.url in ['about:blank', 'data:,'] or not _is_safe_url(tab.url):
                if not pending_urls:
                    self.logger.info("ğŸ’¤ No more pending URLs in this tab.")
                    break
                
                next_url = pending_urls.pop()
                if next_url in self.state.visited_urls: continue
                
                self.logger.info(f"â¡ Navigating to: {next_url}")
                try:
                    tab.get(next_url, timeout=20)
                except:
                    continue

            # é¡µé¢åŠ è½½åçš„åŸºç¡€å¤„ç†ï¼ˆå…³å¼¹çª—ç­‰ï¼‰
            self._optimize_page(tab)
            current_url = tab.url
            self.state.visited_urls.add(current_url)

            # --- Phase 2: è§‚å¯Ÿä¸æ€»ç»“ (Observation) ---
            # è·å–å†…å®¹
            html = tab.html
            text_content = trafilatura.extract(html) or ""
            
            # åªæœ‰å½“å†…å®¹æ˜¯æ–°çš„ï¼Œä¸”é•¿åº¦è¶³å¤Ÿæ—¶ï¼Œæ‰è¿›è¡Œæ€»ç»“
            content_hash = hash(text_content)
            if content_hash not in self.state.summarized_hashes and len(text_content) > 300:
                self.state.summarized_hashes.add(content_hash)
                # è°ƒç”¨å°è„‘æ€»ç»“å¹¶ä¿å­˜
                self._analyze_and_save(current_url, tab.title, text_content)

            # --- Phase 3: å‘ç°å€™é€‰è€… (Discovery) ---
            # è·å–æ‰€æœ‰ Link å’Œ Interaction
            link_candidates, interact_candidates = self._scan_page(tab)

            # --- Phase 4: è§„åˆ’é“¾æ¥ (Planning - Links) ---
            # [å…³é”®é€»è¾‘] åœ¨ç‚¹å‡»ä»»ä½•æŒ‰é’®ä¹‹å‰ï¼Œå…ˆæŠŠæœ¬é¡µæœ‰ä»·å€¼çš„é“¾æ¥å­˜èµ·æ¥ï¼
            
            # è¿‡æ»¤æ‰å·²è®¿é—®çš„é“¾æ¥
            new_links = [c for c in link_candidates if c.url not in self.state.visited_urls and c.url not in pending_urls]
            
            if new_links:
                # é—®å°è„‘ï¼šè¿™äº›é“¾æ¥å“ªäº›å€¼å¾—ä»¥åçœ‹ï¼Ÿ
                chosen_urls = self._call_ai_filter_links(new_links)
                if chosen_urls:
                    self.logger.info(f"ğŸ“Œ Queued {len(chosen_urls)} links for later")
                    # å°†é€‰ä¸­çš„é“¾æ¥åŠ å…¥å¾…åŠæ ˆã€‚
                    # ç­–ç•¥ï¼šextend åˆ°æœ«å°¾ï¼Œæ„å‘³ç€ä½œä¸ºä¸€ä¸ª Stackï¼Œå®ƒä»¬ä¼šè¢«ä¼˜å…ˆè®¿é—®ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰ã€‚
                    # å¦‚æœå¸Œæœ›å¹¿åº¦ä¼˜å…ˆï¼Œå¯ä»¥ç”¨ insert(0, ...) ä½†äººç±»æµè§ˆé€šå¸¸æ˜¯æ·±åº¦çš„ã€‚
                    for u in chosen_urls:
                         if u not in pending_urls: # åŒé‡å»é‡æ£€æŸ¥
                             pending_urls.append(u)

            # --- Phase 5: è§„åˆ’äº¤äº’ (Planning - Interaction) ---
            # é—®å°è„‘ï¼šç°åœ¨æœ‰æ²¡æœ‰å¿…é¡»ç‚¹çš„æŒ‰é’®ï¼Ÿ
            
            # è¿‡æ»¤æ‰å·²ç‚¹å‡»è¿‡çš„æŒ‰é’®
            valid_interactions = []
            for c in interact_candidates:
                key = (current_url, c.text)
                if key not in self.state.clicked_interactions:
                    valid_interactions.append(c)
            
            target_interaction = None
            if valid_interactions:
                target_interaction = self._call_ai_pick_interaction(valid_interactions)

            # --- Phase 6: æ‰§è¡Œäº¤äº’ (Execution) ---
            if target_interaction:
                # è®°å½•ç‚¹å‡»ï¼Œé˜²æ­¢æ­»å¾ªç¯
                key = (current_url, target_interaction.text)
                self.state.clicked_interactions.add(key)
                
                self.logger.info(f"ğŸ‘† Clicking: [{target_interaction.text}]")
                
                # è®°å½•ç‚¹å‡»å‰çš„ Tab çŠ¶æ€
                pre_tab_id = tab.tab_id
                pre_tab_count = self.browser.tabs_count
                
                try:
                    # ç‚¹å‡»!
                    target_interaction.element.click(by_js=False) # ä¼˜å…ˆæ¨¡æ‹ŸçœŸå®ç‚¹å‡»
                    time.sleep(2) # ç­‰å¾…ååº”
                except Exception as e:
                    self.logger.warning(f"Click failed: {e}")
                    continue

                # æ£€æŸ¥ç»“æœï¼š
                # æƒ…å†µ A: å¼¹å‡ºäº†æ–°æ ‡ç­¾é¡µ -> é€’å½’è¿›å»å¤„ç†
                if self.browser.tabs_count > pre_tab_count:
                    self.logger.info("ğŸ”€ New Tab detected, recursing...")
                    new_tab = self.browser.tabs[-1]
                    # é€’å½’è°ƒç”¨ï¼æ–° Tab ä»ç©ºæ ˆå¼€å§‹
                    self._explore_tab_loop(new_tab, []) 
                    # é€’å½’è¿”å›åï¼Œç¡®ä¿æ–° Tab å…³é—­
                    try: new_tab.close()
                    except: pass
                    # å›åˆ°å½“å‰å¾ªç¯ï¼Œç»§ç»­å¤„ç†å½“å‰é¡µï¼ˆå› ä¸ºä¹‹å‰çš„é“¾æ¥å·²ç»å­˜äº†ï¼Œä¸æ€•ä¸¢ï¼‰
                
                # æƒ…å†µ B: è¿˜æ˜¯åŒä¸€ä¸ª Tabï¼Œä½† URL å˜äº†æˆ–è€…å†…å®¹å˜äº†
                else:
                    # ä»€ä¹ˆéƒ½ä¸ç”¨åšï¼Œç›´æ¥ continueã€‚
                    # ä¸‹ä¸€æ¬¡å¾ªç¯å¼€å¤´ä¼šæ£€æµ‹ URL å˜åŒ–ï¼Œæˆ–è€…é‡æ–° hash å†…å®¹ã€‚
                    pass
            
            else:
                # æ²¡æœ‰å€¼å¾—ç‚¹çš„äº¤äº’äº†
                # æ­¤æ—¶åº”è¯¥å»å¤„ç† pending_urls
                if pending_urls:
                    # å¼ºåˆ¶è·³è½¬åˆ°ä¸‹ä¸€ä¸ª URL
                    next_url = pending_urls.pop()
                    if next_url not in self.state.visited_urls:
                        self.logger.info(f"â¡ Page exhausted, going next: {next_url}")
                        try:
                            tab.get(next_url)
                        except: pass
                else:
                    # å½“å‰ Tab æ—¢æ²¡æœ‰äº¤äº’ï¼Œä¹Ÿæ²¡æœ‰å¾…åŠ URL -> ç»“æŸ Tab
                    self.logger.info("âœ… Tab finished.")
                    break

    def _scan_page(self, tab):
        """æ‰«æé¡µé¢å…ƒç´ ï¼Œè¿”å› (links, interactions)"""
        links = []
        interactions = []
        
        # 1. Links
        for a in tab.eles('tag:a@@visibility:visible'):
            try:
                txt = a.text.strip()
                href = a.link
                if not txt or len(txt) < 2: continue
                # å¦‚æœæ˜¯ javascript: æˆ–è€…æ˜¯ç©ºé“¾æ¥ï¼Œè§†ä¸ºäº¤äº’æŒ‰é’®
                if not href or len(href) < 5 or href.startswith('javascript'):
                    interactions.append(ClickableCandidate(a, txt, 'interaction'))
                elif _is_safe_url(href):
                    links.append(ClickableCandidate(a, txt, 'link', href))
            except: continue
            
        # 2. Buttons / Inputs
        for btn in tab.eles('css:button, input[type="submit"], [role="button"]'):
            try:
                if not btn.is_visible: continue
                txt = btn.text.strip() or btn.attr('title') or btn.attr('value')
                if txt and len(txt) > 2:
                    interactions.append(ClickableCandidate(btn, txt, 'interaction'))
            except: continue
            
        return links, interactions

    def _optimize_page(self, tab):
        """æ¸…é™¤å¼¹çª—ï¼Œå±•å¼€å†…å®¹"""
        # ç®€å•å®ç°ï¼Œå¯æ‰©å±•
        try:
            # å…³é®ç½©
            for txt in ['Accept', 'Agree', 'Close', 'å…³é—­', 'åŒæ„']:
                btn = tab.ele(f'tag:button@@text():{txt}@@visibility:visible')
                if btn: btn.click(by_js=True)
            # å±•å¼€
            for txt in ['Read More', 'Show Full', 'å±•å¼€']:
                btn = tab.ele(f'text:{txt}@@visibility:visible')
                if btn: btn.click(by_js=True)
        except: pass

    # ============================
    # Sync-Async Bridge & AI Calls
    # ============================

    def _call_ai_sync(self, coro):
        """åœ¨åŒæ­¥çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥åç¨‹å¹¶ç­‰å¾…ç»“æœ"""
        future = asyncio.run_coroutine_threadsafe(coro, self.loop)
        return future.result()

    def _analyze_and_save(self, url, title, text):
        """è°ƒç”¨ Brain åˆ¤æ–­ä»·å€¼å¹¶ä¿å­˜"""

        async def _judge_content_relevance() -> str:
            """
            [æ™ºèƒ½å°è„‘]
            1. å¦‚æœæ–‡ç« çŸ­ï¼Œç”Ÿæˆ Summaryï¼Œè¿”å› {"action": "save_summary", "content": "..."}
            2. å¦‚æœæ–‡ç« é•¿ï¼Œåªåˆ¤æ–­ç›¸å…³æ€§ï¼Œè¿”å› {"action": "save_full", "content": "(original text)"}
            3. å¦‚æœæ— å…³ï¼Œè¿”å› {"action": "skip"}
            """
            text_len = len(text)
            criteria = self.ctx.criteria
            lang = self.ctx.lang
            #self.logger.debug(f"è¯„ä¼°ï¼š {text[:200]} for {query} , criteria: {criteria}")
            # å…±åŒçš„è¯„ä¼°æ ‡å‡†ï¼ˆHigh Value Criteriaï¼‰
            
            
            # === åˆ†æ”¯ A: é•¿æ–‡æ¨¡å¼ (åªåˆ¤æ–­ï¼Œä¸æ€»ç»“) ===
            if text_len > self.SUMMARY_THRESHOLD:
                # åªçœ‹å¼€å¤´ï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—å­˜
                preview = text[:4000] 
                prompt = textwrap.dedent(f"""
                    Mission: Researching "{self.purpose}".
                    
                    EVALUATION STANDARDS:
                    {criteria}

                    Content Preview (First 4k chars of {text_len} chars):
                    {preview}
                    ...
                    
                    Task: Strictly evaluate if this content meets the High Value standards above.
                    
                    Output JSON ONLY:
                    - If High Value: {{"decision": "save_full"}}
                    - Otherwise: {{"decision": "skip"}}
                """)
                
                try:
                    # å‡è®¾ backend è¿”å› JSON
                    resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
                    self.logger.debug(f"å°è„‘æ€è€ƒ: {resp['reasoning']}")
                    decision = json.loads(resp['reply'].replace("```json", "").replace("```", "").strip())
                    self.logger.debug(f"å°è„‘åˆ¤æ–­: {decision}")
                    return decision
                except:
                    self.logger.exception(f"å°è„‘åˆ¤æ–­å¤±è´¥")
                    return {"decision": "skip"}

            # === åˆ†æ”¯ B: çŸ­æ–‡æ¨¡å¼ (ç”Ÿæˆæ‘˜è¦) ===
            else:
                prompt = textwrap.dedent(f"""
                    Mission: Researching "{self.purpose}".
                    
                    EVALUATION STANDARDS:
                    {criteria}
                    
                    Content:
                    {text}
                    
                    Task:
                    1. If content is low value/trash based on standards -> SKIP.
                    2. If high value -> Generate a dense Markdown Summary following the "Summary Style" above. Summary prefer to use {lang}
                    
                    Output JSON ONLY:
                    - If high value: {{"decision": "save_summary", "title":"generate a short title", "summary": "..."}}
                    - If low value/trash: {{"decision": "skip"}}
                """)
                
                try:
                    resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
                    self.logger.debug(f"å°è„‘æ€è€ƒ: {resp['reasoning']}")
                    decision = json.loads(resp['reply'].replace("```json", "").replace("```", "").strip())
                    self.logger.debug(f"å°è„‘åˆ¤æ–­: {decision}")

                    return decision
                    
                    
                except:
                    self.logger.exception(f"å°è„‘åˆ¤æ–­å¤±è´¥")
                    return {"decision": "skip"}
        

        try:
            result = self._call_ai_sync(_judge_content_relevance())
            if result.get('decision') == 'save_summary':
                title = result.get('title') or title
                #fname = f"{slugify(title)[:50]}.md"
                fname = re.sub(r'[<>:"/\\|?*.\s]', '_', title)[:50] + ".md"
                path = os.path.join(self.save_dir, fname)
                with open(path, "w", encoding='utf-8') as f:
                    f.write(f"# {title}\nSource: {url}\n\n{result.get('summary')}")
                self.logger.info(f"ğŸ’¾ Saved summary: {fname}")
            elif result.get('decision') == 'save_full':
                fname = re.sub(r'[<>:"/\\|?*.\s]', '_', title)[:50] + ".md"
                path = os.path.join(self.save_dir, fname)
                with open(path, "w", encoding='utf-8') as f:
                    f.write(f"# {title}\nSource: {url}\n\n{text}")
        except Exception as e:
            self.logger.error(f"AI Save failed: {e}")

    def _call_ai_filter_links(self, candidates):
        """è®©å°è„‘ç­›é€‰é“¾æ¥ (Return list of URLs)"""
        if not candidates: return []
        
        async def _task():
            # åªæä¾›æ–‡æœ¬ï¼Œä¸æä¾› URLï¼Œé˜²æ­¢æ¨¡å‹æ ¹æ® URL ççŒœ
            items_str = "\n".join([f"{i}. {c.text}" for i, c in enumerate(candidates)])
            prompt = textwrap.dedent(f"""
                Goal: "{self.purpose}"
                I am on a webpage. Which of these links are likely to lead to relevant information or sub-categories (like 'Investor Relations', 'Reports')?
                
                Candidates:
                {items_str}
                
                Return the INDICES of the best links as a JSON list, e.g. [0, 5, 8].
                If none, return [].
            """)
            try:
                resp = await self.ctx.backend.think(messages=[{"role": "user", "content": prompt}])
                indices = json.loads(resp['reply'])
                return [candidates[i].url for i in indices if isinstance(i, int) and 0 <= i < len(candidates)]
            except:
                return []
        
        return self._call_ai_sync(_task())

    def _call_ai_pick_interaction(self, candidates):
        """è®©å°è„‘é€‰æ‹©ä¸€ä¸ª Interaction (Return One Candidate)"""
        async def _task():
            items_str = "\n".join([f"{i}. {c.text}" for i, c in enumerate(candidates)])
            prompt = textwrap.dedent(f"""
                Goal: "{self.purpose}"
                I am on a webpage. Which button should I click to reveal more content or navigate to the next part?
                Focus on: "Next Page", "Load More", "Expand", "Download Report".
                Ignore: "Share", "Like", "Comment".
                
                Candidates:
                {items_str}
                
                Return the INDEX of the single best candidate as JSON, e.g. 5. 
                If none are worth clicking, return -1.
            """)
            try:
                resp = await self.ctx.backend.think(messages=[{"role": "user", "content": prompt}])
                idx = int(resp['reply'].strip())
                if 0 <= idx < len(candidates):
                    return candidates[idx]
            except:
                pass
            return None

        return self._call_ai_sync(_task())


# ==========================================
# 3. é›†æˆåˆ° Crawler Logic
# ==========================================

class MissionContext:
    def __init__(self, timeout_minutes, backend):
        self.deadline = time.time() + timeout_minutes * 60
        self.backend = backend # ä¼ é€’ AI åç«¯ä¾› HumanBrowser è°ƒç”¨
    
    def is_active(self):
        return time.time() < self.deadline

class RecursiveCrawlerMixin(SmartSearcherMixin):
    
    @register_action("ä»ç½‘ä¸Šæœç´¢å¹¶ä¸‹è½½å¯¹ç ”ç©¶ç›®æ ‡æœ‰ç”¨çš„ç›¸å…³èµ„æ–™", param_infos={...})
    async def research_crawler(self, search_phrase, purpose, topic, seed_urls=None, timeout=30, **kwargs):
        
        save_dir = os.path.join(self.workspace_root, "downloads", slugify(topic))
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. è·å–ç§å­
        if not seed_urls:
            search_res = await self._smart_search_entry(search_phrase, limit=10)
            if isinstance(search_res, str): return search_res
            seed_urls = [item['href'] for item in search_res]

        # 2. åˆå§‹åŒ–ç¯å¢ƒ
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥ self.cerebellum.backend ä»¥ä¾¿ Agent è°ƒç”¨ LLM
        ctx = MissionContext(timeout, self.cerebellum.backend)
        
        # å¯åŠ¨ DrissionPage (æ— å¤´æ¨¡å¼)
        co = ChromiumOptions()
        co.headless(True)
        co.set_argument('--no-sandbox')
        browser = ChromiumPage(co)
        
        # è·å–å½“å‰ asyncio loopï¼Œä»¥ä¾¿ä¼ ç»™ sync thread
        loop = asyncio.get_running_loop()
        
        agent = HumanBrowser(browser, ctx, save_dir, purpose, loop)
        
        try:
            self.logger.info("ğŸ¤– HumanBrowserAgent starting...")
            # å…³é”®ï¼šå°†æ•´ä¸ªåŒæ­¥çš„æµè§ˆè¿‡ç¨‹æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
            await asyncio.to_thread(agent.start, seed_urls)
        
        except Exception as e:
            self.logger.error(f"Crawler crashed: {e}")
        finally:
            browser.quit()

        # 3. æ±‡æŠ¥
        file_count = len([n for n in os.listdir(save_dir) if n.endswith('.md')])
        return f"Mission Complete. Saved {file_count} documents to {save_dir}."