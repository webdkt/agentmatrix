import asyncio
import time
import os
import json
import textwrap
import logging
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from curl_cffi.requests import AsyncSession
import trafilatura
from slugify import slugify
from core.action import register_action
from skills.search_tool import SmartSearcherMixin
import random
from urllib.parse import unquote
import re


# === 1. å…±äº«ä¸Šä¸‹æ–‡å¯¹è±¡ ===
class MissionContext:
    
    def __init__(self,  timeout_minutes: int, criteria, lang, max_concurrency: int = 5):
        
        self.visited = set()  # å…¨å±€å»é‡
        self.results = [] # å­˜å‚¨å­—å…¸: {'type': 'file'|'page', 'title':..., 'path':..., 'url':...}
        self.start_time = time.time()
        self.deadline = self.start_time + timeout_minutes*60
        # === å…¨å±€å¹¶å‘é” ===
        # è¿™æ„å‘³ç€åŒæ—¶æœ€å¤šåªæœ‰ 5 ä¸ª HTTP è¯·æ±‚åœ¨è·‘
        # å…¶ä»–çš„ task å¯ä»¥åœ¨åå°é€»è¾‘å¤„ç†ï¼ˆå°è„‘æ€è€ƒï¼‰ï¼Œä½†ç½‘ç»œè¯·æ±‚å¿…é¡»æ’é˜Ÿ
        self.sem = asyncio.Semaphore(max_concurrency)
        self.criteria = criteria
        self.lang = lang
        
        
    def is_active(self):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åº”è¯¥ç»§ç»­ï¼ˆè¶…æ—¶ï¼‰"""
        
        if time.time() > self.deadline:
            self.logger.info(f"ä»»åŠ¡è¶…æ—¶ï¼Œç»ˆæ­¢")
            return False
        return True



class RecursiveCrawlerMixin(SmartSearcherMixin):

    # å®šä¹‰é˜ˆå€¼ï¼šè¶…è¿‡è¿™ä¸ªé•¿åº¦å°±ä¸è®©å°è„‘åšæ€»ç»“äº†ï¼Œå¤ªç´¯ä¸”å®¹æ˜“å¹»è§‰
    SUMMARY_THRESHOLD = 15000 

    # === é¢„è®¾çš„é¢†åŸŸè¯„ä¼°æ ‡å‡† (Prompt Templates) ===
    PRESET_CRITERIA = {
        "STEM": textwrap.dedent("""
            [DOMAIN: SCIENCE, TECH, ENGINEERING, MATH]
            - **High Value Content**: Architecture diagrams, Mathematical formulas, Code snippets, Benchmark tables, Dataset specifications, Implementation details.
            - **Summary Style**: Structured bullet points with specific numbers and parameters.
            - **Trash**: Marketing fluff ("AI is the future"), Surface-level intros, SEO spam.
        """),

        "HUMANITIES": textwrap.dedent("""
            [DOMAIN: HISTORY, LITERATURE, PHILOSOPHY, ARTS]
            - **High Value Content**: Primary historical sources, Direct quotes, Critical analysis, Novel arguments, Chronology of events, Historiography.
            - **Summary Style**: Narrative summary capturing the core argument + Key Quotes.
            - **Trash**: Wikipedia-style generic summaries, Shallow listicles, Product sales.
        """),

        "BUSINESS": textwrap.dedent("""
            [DOMAIN: BUSINESS, FINANCE, NEWS]
            - **High Value Content**: Financial reports (10-K/10-Q), Official press releases, Executive statements, Market share data, Strategic analysis, Event timelines.
            - **Summary Style**: Executive Summary style. Focus on outcomes, numbers, and dates.
            - **Trash**: Clickbait headlines, Rumors without sources, Generic investment advice.
        """),
        "GENERAL": textwrap.dedent("""
            [DOMAIN: GENERAL KNOWLEDGE / LIFESTYLE / HOW-TO]
            - **High Value Content**: Comprehensive guides (Step-by-step), Verified factual data, Neutral encyclopedic overviews, Deep investigative reporting, High-quality tutorials.
            - **Summary Style**: Clear "How-to" steps, List of key facts, or Structured overview.
            - **Trash**: SEO Spam ("Top 10 products"), Shallow clickbait, User comments/Forum arguments (unless highly technical), Login walls.
        """)
    }

    _custom_log_level = logging.DEBUG

    async def _judge_content_relevance(self, text: str, query: str, context) -> str:
        """
        [æ™ºèƒ½å°è„‘]
        1. å¦‚æœæ–‡ç« çŸ­ï¼Œç”Ÿæˆ Summaryï¼Œè¿”å› {"action": "save_summary", "content": "..."}
        2. å¦‚æœæ–‡ç« é•¿ï¼Œåªåˆ¤æ–­ç›¸å…³æ€§ï¼Œè¿”å› {"action": "save_full", "content": "(original text)"}
        3. å¦‚æœæ— å…³ï¼Œè¿”å› {"action": "skip"}
        """
        text_len = len(text)
        criteria = context.criteria
        lang = context.lang
        #self.logger.debug(f"è¯„ä¼°ï¼š {text[:200]} for {query} , criteria: {criteria}")
        # å…±åŒçš„è¯„ä¼°æ ‡å‡†ï¼ˆHigh Value Criteriaï¼‰
        
        
        # === åˆ†æ”¯ A: é•¿æ–‡æ¨¡å¼ (åªåˆ¤æ–­ï¼Œä¸æ€»ç»“) ===
        if text_len > self.SUMMARY_THRESHOLD:
            # åªçœ‹å¼€å¤´ï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—å­˜
            preview = text[:4000] 
            prompt = textwrap.dedent(f"""
                Mission: Researching "{query}".
                
                EVALUATION STANDARDS:
                {criteria}

                Content Preview (First 4k chars of {text_len} chars):
                {preview}
                ...
                
                {criteria}
                
                Task: Strictly evaluate if this content meets the High Value standards above.
                
                Output JSON ONLY:
                - If High Value: {{"decision": "save_full"}}
                - Otherwise: {{"decision": "skip"}}
            """)
            
            try:
                # å‡è®¾ backend è¿”å› JSON
                resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
                self.logger.debug(f"å°è„‘æ€è€ƒ: {resp['reasoning']}")
                decision = json.loads(resp['reply'].replace("```json", "").replace("```", "").strip())
                self.logger.debug(f"å°è„‘åˆ¤æ–­: {decision}")
                if decision.get("decision") == "save_full":
                    # è¿”å›åŸæ–‡
                    return {"action": "save_full", "content": text}
                else:
                    return {"action": "skip"}
            except:
                self.logger.exception(f"å°è„‘åˆ¤æ–­å¤±è´¥")
                return {"action": "skip"}

        # === åˆ†æ”¯ B: çŸ­æ–‡æ¨¡å¼ (ç”Ÿæˆæ‘˜è¦) ===
        else:
            prompt = textwrap.dedent(f"""
                Mission: Researching "{query}".
                
                EVALUATION STANDARDS:
                {criteria}
                
                Content:
                {text}
                
                Task:
                1. If content is low value/trash based on standards -> SKIP.
                2. If high value -> Generate a dense Markdown Summary following the "Summary Style" above. Summary prefer to use {lang}
                
                Output JSON ONLY:
                - If high value: {{"decision": "save_summary", "summary": "..."}}
                - If low value/trash: {{"decision": "skip"}}
            """)
            
            try:
                resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
                self.logger.debug(f"å°è„‘æ€è€ƒ: {resp['reasoning']}")
                decision = json.loads(resp['reply'].replace("```json", "").replace("```", "").strip())
                self.logger.debug(f"å°è„‘åˆ¤æ–­: {decision}")
                
                if decision.get("decision") == "save_summary":
                    return {"action": "save_summary", "content": decision.get("summary")}
                else:
                    return {"action": "skip"}
            except:
                self.logger.exception(f"å°è„‘åˆ¤æ–­å¤±è´¥")
                return {"action": "skip"}

    async def _filter_links_by_cerebellum(self, links: list, query: str) -> list:
        """
        [å°è„‘] ä»ä¸€å †é“¾æ¥ä¸­æŒ‘é€‰å€¼å¾—ç»§ç»­è®¿é—®çš„ã€‚
        """
        if not links: return []
        
        # 1. é¢„å¤„ç†ï¼šç”Ÿæˆ Markdown ä¾› LLM é˜…è¯»ï¼ŒåŒæ—¶ç»´æŠ¤ URL ç™½åå•
        candidates_display = [] # ç»™ LLM çœ‹çš„
        
        # è¿‡æ»¤åƒåœ¾é“¾æ¥
        filtered_links = []
        for text, href in links:
            if len(text) < 2 or "login" in href or "signup" in href or "javascript:" in href:
                continue
            filtered_links.append((text, href))
            
        if not filtered_links: return []

        # 2. åˆ†æ‰¹å¤„ç†
        batch_size = 20
        selected_hrefs = []
        
        # ä¼˜åŒ–åçš„æ­£åˆ™ï¼šå…è®¸ URL ä¸­å‡ºç°æ‹¬å· (å¤„ç† Wiki é“¾æ¥)ï¼Œä½†æ’é™¤æœ«å°¾æ ‡ç‚¹
        # ç­–ç•¥ï¼šæ•è·éç©ºç™½å­—ç¬¦ï¼Œåç»­å†æ¸…æ´—
        URL_PATTERN = re.compile(r'https?://[^\s<>"]+|www\.[^\s<>"]+')

        for i in range(0, len(filtered_links), batch_size):
            batch = filtered_links[i:i+batch_size]
            
            # æ„å»ºè¯¥æ‰¹æ¬¡çš„â€œç™½åå•é›†åˆâ€ï¼Œç”¨äºç²¾å‡†éªŒè¯
            # è¿™æ ·æ— è®º LLM è¾“å‡ºä»€ä¹ˆå¹»è§‰ï¼Œåªè¦ä¸æ˜¯åŸå°ä¸åŠ¨çš„ URLï¼Œéƒ½ä¼šè¢«è¿‡æ»¤æ‰
            valid_href_set = {href for _, href in batch}
            
            # æ„å»º Prompt æ–‡æœ¬
            batch_str = "\n".join([f"- {href} (Content: {text})" for text, href in batch])
            
            prompt = textwrap.dedent(f"""
                Mission: Researching "{query}".
                
                Candidate Links:
                {batch_str}
                
                Task: output the URLs from the list above that are most likely to contain valuable information (PDFs, Articles, Documentation).
                
                Rules:
                1. Output ONLY the URLs. One per line.
                2. Do NOT output bullet points or explanations.
                3. If none are relevant, output "NONE".
            """)
            
            try:
                resp = await self.cerebellum.backend.think(messages=[{"role": "user", "content": prompt}])
                content = resp['reply']
                # self.logger.debug(f"å°è„‘ç­›é€‰ç»“æœ:\n{content}")

                # 3. æå–ä¸éªŒè¯
                found_candidates = URL_PATTERN.findall(content)
                
                valid_count = 0
                for url in found_candidates:
                    # æ¸…æ´—ï¼šæœ‰äº›æ¨¡å‹å–œæ¬¢åœ¨ URL åé¢åŠ å¥å·æˆ–é€—å·
                    clean_url = url.rstrip('.,;)]}') 
                    
                    # ç»´åŸºç™¾ç§‘ç‰¹ä¾‹ï¼šå¦‚æœåŸé“¾æ¥é‡Œç¡®å®æœ‰æ‹¬å·ï¼Œæ­£åˆ™å¯èƒ½æ²¡åˆ‡å¥½ï¼Œæˆ–è€… strip åˆ‡å¤šäº†
                    # ä½†æœ€ç¨³å¦¥çš„åŠæ³•æ˜¯ï¼šç›´æ¥å» valid_href_set é‡ŒæŸ¥
                    
                    # A. ç›´æ¥åŒ¹é…
                    if clean_url in valid_href_set:
                        selected_hrefs.append(clean_url)
                        valid_count += 1
                        continue
                        
                    # B. å®¹é”™åŒ¹é…ï¼šå¦‚æœæ¨¡å‹è¾“å‡ºäº† `https://.../foo)` (è¢«æ­£åˆ™æ•è·äº†å³æ‹¬å·)
                    # æˆ‘ä»¬å¯ä»¥å°è¯•æ¢å¤å®ƒã€‚æˆ–è€…ç®€å•ç‚¹ï¼Œç›´æ¥éå† batch æŸ¥æ‰¾å­ä¸²
                    # (ç”±äº batch åªæœ‰ 20 ä¸ªï¼Œéå†å¼€é”€å¯å¿½ç•¥)
                    for original_href in valid_href_set:
                        # å¦‚æœæ¨¡å‹è¾“å‡ºçš„ url åŒ…å«åœ¨åŸå§‹ url é‡Œï¼Œæˆ–è€…åŸå§‹ url åŒ…å«åœ¨è¾“å‡ºé‡Œ
                        # ä¸”é•¿åº¦å·®å¼‚å¾ˆå°ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºæ˜¯åŒä¸€ä¸ª
                        if original_href == url or original_href == clean_url:
                            if original_href not in selected_hrefs: # é˜²æ­¢é‡å¤æ·»åŠ 
                                selected_hrefs.append(original_href)
                                valid_count += 1
                            break
                            
                self.logger.info(f"âœ… Cerebellum filtered: kept {valid_count}/{len(batch)} links")

            except Exception as e:
                self.logger.error(f"Link filtering failed: {e}")
                continue
                
        return selected_hrefs
    
    

    def _generate_manifest(self, save_dir: str, topic: str, results: list):
        """
        ç”Ÿæˆæœºå™¨å¯è¯»çš„ JSON å’Œäººç±»å¯è¯»çš„ Markdown ç´¢å¼•
        """
        # 1. ç”Ÿæˆ JSON (æ•°æ®åº“)
        json_path = os.path.join(save_dir, "manifest.json")
        with open(json_path, "w", encoding='utf-8') as f:
            json.dump({
                "topic": topic,
                "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_items": len(results),
                "items": results
            }, f, indent=2, ensure_ascii=False)

        # 2. ç”Ÿæˆ Markdown (README.md) - è¿™æ˜¯ç»™ Brain ä»¥åé˜…è¯»ç”¨çš„ç´¢å¼•
        md_path = os.path.join(save_dir, "README.md")
        with open(md_path, "w", encoding='utf-8') as f:
            f.write(f"# Data Collection Manifest: {topic}\n\n")
            f.write(f"> Collection Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"> Total Items: {len(results)}\n\n")
            f.write("| Type | Title / Local Path | Original URL | Notes |\n")
            f.write("| :--- | :--- | :--- | :--- |\n")
            
            for item in results:
                icon = "ğŸ“„" if item['type'] == 'page' else "ğŸ’¾"
                # ç›¸å¯¹è·¯å¾„é“¾æ¥
                local_link = f"[{item['title']}](./{item['path']})"
                # æˆªæ–­é•¿ URL
                short_url = item['url'][:30] + "..." if len(item['url']) > 30 else item['url']
                note = item.get('summary', '') or f"{item.get('size_kb', 0):.1f} KB"
                # æ¸…æ´— note ä¸­çš„æ¢è¡Œç¬¦ä»¥å…ç ´åè¡¨æ ¼
                note = note.replace('\n', ' ').replace('|', '/')[:50]
                
                f.write(f"| {icon} {item['type']} | {local_link} | [{short_url}]({item['url']}) | {note} |\n")
        
        return md_path

    # === 2. æ ¸å¿ƒé€’å½’å‡½æ•° ===
    async def _recursive_explore(self, session, url: str, context: MissionContext, purpose: str, save_dir: str):
        # A. ç†”æ–­æ£€æŸ¥
        #self.logger.info(f"ğŸ” [Active: {url}] Exploring...")
        if not context.is_active() or url in context.visited:
            self.logger.info(f"ğŸ›‘ [{url}] Already visited or mission terminated.")
            return
        
        
        context.visited.add(url)
        
        

        try:
            # B. è®¿é—® (æ‰§è¡Œ)
            content = None
            resp = None
            async with context.sem:
                self.logger.info(f"ğŸš€ [Active] Fetching: {url}")
                
                # === æ”¹è¿›ç‚¹ 1: é‡è¯•æœºåˆ¶ ===
                max_retries = 3
                resp = None
                
                for attempt in range(max_retries):
                    try:
                        # === æ”¹è¿›ç‚¹ 2: å¯ç”¨ stream=True å’Œæ›´é•¿çš„ timeout ===
                        # stream=True æ„å‘³ç€åªè¦è¿ä¸Šå¹¶æ‹¿åˆ° Header å°±ç®—æˆåŠŸï¼Œä¸ä¼šç­‰ body ä¸‹è½½å®Œ
                        # timeout=60 å¯¹æ–‡ä»¶ä¸‹è½½æ›´å‹å¥½
                        await asyncio.sleep(random.uniform(0.4, 2.5)) # é¿å…è¯·æ±‚è¿‡å¿«è¢«å° IP
                        resp = await session.get(url, allow_redirects=True, timeout=60, stream=True)
                        if resp.status_code == 200:
                            break # æˆåŠŸæ‹¿åˆ°å“åº”å¤´ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    except Exception as e:
                        if attempt < max_retries - 1:
                            self.logger.warning(f"âš ï¸ Retry {attempt+1}/{max_retries} for {url}: {e}")
                            await asyncio.sleep(2) # é¿è®©ä¸€ä¸‹
                        else:
                            raise e # æœ€åä¸€æ¬¡è¿˜å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸

                if not resp or resp.status_code != 200:
                    self.logger.warning(f"âŒ HTTP {resp.status_code}: {url}")
                    return

                # è·å–è·³è½¬åçš„æœ€ç»ˆ URL
                final_url = str(resp.url)
                context.visited.add(final_url)
                content_type = resp.headers.get("content-type", "").lower()

                # --- åˆ†æ”¯ 1: æ˜¯æ–‡ä»¶ (PDF/Doc/Zip) ---
                if "text/html" not in content_type:
                    # è‡ªåŠ¨æ¨æ–­æ–‡ä»¶å
                    path_name = os.path.basename(urlparse(final_url).path)
                    # ç®€å•çš„æ¸…æ´—ï¼Œé˜²æ­¢æ–‡ä»¶åä¸ºç©ºæˆ–éæ³•
                    if not path_name or len(path_name) < 2:
                        path_name = f"file_{hash(final_url)}.bin"
                    
                    # åŠ ä¸Šæ‰©å±•åä¿®æ­£ï¼ˆå¦‚æœ url é‡Œæ²¡åç¼€ï¼Œcontent-type é‡Œæœ‰ï¼‰
                    if "pdf" in content_type and not path_name.endswith(".pdf"): path_name += ".pdf"
                    
                    fname = unquote(path_name) # è§£ç ä¸­æ–‡æ–‡ä»¶å (ä¾‹å¦‚ %E5%AE... -> å®‰åˆ©.pdf)
                    save_path = os.path.join(save_dir, fname)

                    # === æ”¹è¿›ç‚¹ 3: æµå¼ä¸‹è½½å†™å…¥ (Chunked Write) ===
                    # è¿™ç§æ–¹å¼ä¸ä¼šå› ä¸ºæ–‡ä»¶å¤§è€Œè¶…æ—¶ï¼Œåªè¦æ•°æ®è¿˜åœ¨æµè½¬ï¼Œè¿æ¥å°±ä¿æŒ
                    file_size = 0
                    with open(save_path, "wb") as f:
                        async for chunk in resp.aiter_content():
                            f.write(chunk)
                            file_size += len(chunk)
                    
                    self.logger.info(f"ğŸ’¾ File Saved: {fname} ({file_size/1024:.1f} KB)")
                    
                    context.results.append({
                        "type": "file",
                        "title": fname,
                        "path": fname,
                        "url": final_url,
                        "size_kb": file_size / 1024,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
                    return # æ–‡ä»¶æ˜¯ç»ˆç‚¹
                else:
                # --- åˆ†æ”¯ 2: æ˜¯ç½‘é¡µ ---
                # å¯¹äºç½‘é¡µï¼Œæˆ‘ä»¬éœ€è¦è¯»å–å…¨æ–‡æ¥åšåˆ†æ
                    content = b""
                    async for chunk in resp.aiter_content():
                        content += chunk
                    #è¯»å®Œè¦é‡Šæ”¾äº†context.sem
                
            html = content
    
            # 1. å°è¯•æå–å†…å®¹ (Harvest)
            text = trafilatura.extract(html)
            # å°è„‘å¿«é€Ÿæ‰«ä¸€çœ¼ï¼šè¿™é¡µå†…å®¹æœ¬èº«æœ‰ç”¨å—ï¼Ÿ
            if text and len(text) > 100:
                # æ™ºèƒ½åˆ¤æ–­ä¸å¤„ç†
                judgment = await self._judge_content_relevance(text, purpose,context)
                self.logger.info(f"ğŸ¤– [{url}] Relevance Judged: {judgment}")
                action = judgment.get("action")
                
                if action != "skip":
                    title = BeautifulSoup(html, 'lxml').title.string or "Untitled"
                    # å®‰å…¨æ–‡ä»¶å
                    safe_title = slugify(title)[:50]
                    fname = f"{safe_title}.md"
                    save_path = os.path.join(save_dir, fname)
                    
                    # æ„é€ æ–‡ä»¶å¤´éƒ¨å…ƒæ•°æ®
                    header = f"# {title}\n> Source: {final_url}\n> Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                    
                    # æ ¹æ®ç±»å‹å†™å…¥ä¸åŒçš„ Tag
                    if action == "save_summary":
                        header += "> Type: AI Summary (Original text discarded)\n\n"
                        # è¿™é‡Œå†™å…¥çš„æ˜¯å°è„‘ç”Ÿæˆçš„ Summary
                        body = judgment.get("content")
                        note_for_manifest = "AI Summary"
                    else:
                        header += "> Type: Full Text (Too long for summary)\n\n"
                        # è¿™é‡Œå†™å…¥çš„æ˜¯åŸå§‹ Text
                        body = text 
                        note_for_manifest = "Full Text"

                    with open(save_path, "w", encoding='utf-8') as f:
                        f.write(header + body)
                    
                    # è®°å½•åˆ° Context
                    context.results.append({
                        "type": "page",
                        "title": title,
                        "path": fname,
                        "url": final_url,
                        "summary": note_for_manifest, 
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
                else:
                    # self.logger.debug(f"Skipped irrelevant page: {final_url}")
                    pass

            # 2. å¯»æ‰¾ä¸‹ä¸€æ­¥ (Exploration)
            
            soup = BeautifulSoup(html, 'lxml')
            raw_links = []
            for a in soup.find_all('a', href=True):
                abs_url = urljoin(final_url, a['href'])
                txt = a.get_text(strip=True)
                if abs_url not in context.visited:
                    raw_links.append((txt, abs_url))
            
            # è®©å°è„‘ç­›é€‰ä¸‹ä¸€æ­¥å»å“ª (è¿™æ˜¯â€œé€’å½’ä¸éœ€è¦é™æ·±â€çš„å…³é”®)
            # å°è„‘ä¼šè‡ªåŠ¨è¿‡æ»¤æ‰â€œè”ç³»æˆ‘ä»¬â€ã€â€œé¦–é¡µâ€ç­‰æ— å…³é“¾æ¥ï¼Œåªä¿ç•™ç›¸å…³æ€§é«˜çš„
            targets = await self._filter_links_by_cerebellum(raw_links, purpose)
            self.logger.info(f"ğŸ¤– [{url}] Found {len(targets)} relevant links")
            # C. é€’å½’è°ƒç”¨ (å¹¶å‘åˆ†å‰)
            # æˆ‘ä»¬å¹¶å‘åœ°å»æ¢ç´¢è¿™äº›æœ‰ä»·å€¼çš„åˆ†æ”¯
            tasks = []
            for target_url in targets:
                # é€’å½’ï¼
                tasks.append(self._recursive_explore(session, target_url, context, purpose, save_dir))
            
            if tasks:
                await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            self.logger.warning(f"Crawling failed for {url}: {e}")

    # === 3. å¯¹å¤–æš´éœ²çš„ Action ===
    @register_action(
        "ä»ç½‘ä¸Šæœç´¢å¹¶ä¸‹è½½å¯¹ç ”ç©¶ç›®æ ‡æœ‰ç”¨çš„ç›¸å…³èµ„æ–™",
        param_infos={
            "search_phrase": "æœç´¢å…³é”®è¯",
            "purpose": "ç ”ç©¶æˆ–æœç´¢çš„ç›®çš„",
            "topic": "ä¿å­˜æ–‡ä»¶å¤¹",
            "domain": "ç ”ç©¶é¢†åŸŸï¼Œä¼˜å…ˆä»ä»¥ä¸‹é€‰æ‹©ä¸€ä¸ª: 'STEM', 'HUMANITIES', 'BUSINESS'ï¼Œå¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä½¿ç”¨ 'GENERAL' ",
            "seed_urls": "å¯é€‰ï¼Œç§å­URLåˆ—è¡¨ (å¦‚æœæœ‰)",
            "max_steps": "å¯é€‰ï¼Œæœ€å¤§è®¿é—®é¡µé¢æ•° (é»˜è®¤ 1000)",
            "timeout": "å¯é€‰æœ€å¤§è€—æ—¶åˆ†é’Ÿ (é»˜è®¤ 30)"
        }
    )
    async def research_crawler(self, search_phrase, purpose: str, topic: str, domain: str="STEM", seed_urls: list = None, max_steps: int = 1000, timeout: int = 30):
        domain = domain.upper()
        search_phrase_in_chinese = self._is_chinese_query(search_phrase)
        purpose_in_chinese = self._is_chinese_query(purpose)
        lang = "ä¸­æ–‡" if search_phrase_in_chinese or purpose_in_chinese else "English"
        if domain not in self.PRESET_CRITERIA:
            self.logger.warning(f"Unknown domain '{domain}', defaulting to STEM")
            domain = "STEM"
        criteria_text = self.PRESET_CRITERIA[domain]
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        ctx = MissionContext( timeout_minutes=timeout, criteria=criteria_text, lang=lang)
        save_dir = os.path.join(self.workspace_root, "downloads", slugify(topic))
        os.makedirs(save_dir, exist_ok=True)
        
        

        # å¦‚æœæ²¡æœ‰ç§å­ï¼Œå…ˆæœä¸€æ³¢
        if not seed_urls:
            search_res = await self._smart_search_entry(search_phrase, limit=20)
            if isinstance(search_res, str): return search_res
            seed_urls = [item['href'] for item in search_res]

        # å¯åŠ¨å¹¶å‘é€’å½’
        async with AsyncSession(impersonate="chrome") as session:
            tasks = []
            for url in seed_urls:
                tasks.append(self._recursive_explore(session, url, ctx, purpose, save_dir))
            
            await asyncio.gather(*tasks, return_exceptions=True)

        # ç”Ÿæˆæ¸…å•
        manifest_path = self._generate_manifest(save_dir, topic, ctx.results)
            
        # 2. æ„é€ ç»™ Brain çš„æœ€ç»ˆæ±‡æŠ¥
        # æ­¤æ—¶ Brain ä¸éœ€è¦çœ‹é‚£å‡ åæ¡çš„å…·ä½“å†…å®¹ï¼Œåªéœ€è¦çŸ¥é“â€œæ´»å„¿å¹²å®Œäº†ï¼Œä¸œè¥¿åœ¨å“ªâ€
        
        file_count = sum(1 for r in ctx.results if r['type'] == 'file')
        page_count = sum(1 for r in ctx.results if r['type'] == 'page')
        
        report_msg = textwrap.dedent(f"""
            âœ… **Collection Mission Complete**
            
            **Topic**: {topic}
            **Status**: Gathered {len(ctx.results)} items.
            - Files (PDF/Docs/etc): {file_count}
            - Pages (Markdown): {page_count}
            
            **Artifacts**:
            - All data saved to: `downloads/{topic}/`
            - Index file created: `downloads/{topic}/README.md` (See this file for details)
            
            é‡‡é›†å·¥ä½œå®Œæˆï¼Œå›å¤é‚®ä»¶åå°±å¯ä»¥ä¼‘æ¯äº†ï¼

        """)
        
        return report_msg