import os
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
from curl_cffi.requests import AsyncSession
import xml.etree.ElementTree as ET # ç”¨äºè§£æ ArXiv API

class SmartSearcherMixin:
    def _is_chinese_query(self, text: str) -> bool:
        """
        [Helper] æ£€æµ‹ query æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ã€‚
        åªè¦åŒ…å«å“ªæ€•ä¸€ä¸ªæ±‰å­—ï¼Œå°±å€¾å‘äºè®¤ä¸ºç”¨æˆ·æƒ³è¦ä¸­æ–‡ç»“æœã€‚
        """
        for char in text:
            if '\u4e00' <= char <= '\u9fff':
                return True
        return False

    # === 1. Google Search (VIP, éœ€è¦ä»£ç†) ===
    async def _search_google(self, session, query, limit=10):
        """
        [Hard Mode] ä¼ªè£…çˆ¬å– Googleã€‚
        Google åçˆ¬æä¸¥ï¼Œéœ€è¦é«˜è´¨é‡çš„ IP å’ŒæŒ‡çº¹ã€‚
        """
        # Google çš„æœç´¢ç»“æœç»“æ„ç»å¸¸å˜ï¼Œä½† h3 -> a çš„ç»“æ„ç›¸å¯¹ç¨³å®š
        base_url = "https://www.google.com/search"
        params = {"q": query, "num": str(limit + 5)}

        # è¯­è¨€é€‚é…
        if self._is_chinese_query(query):
            params["hl"] = "zh-CN" # Interface Language
            params["gl"] = "sg" # Language Restrict (å¯é€‰ï¼Œçœ‹ä½ æ˜¯å¦æƒ³è¦å¼ºåˆ¶çº¯ä¸­æ–‡)
            # å»ºè®®åªè®¾ hl=zh-CNï¼ŒGoogle å¾ˆèªæ˜ï¼Œä¼šè‡ªå·±æ··åˆç»“æœ
        else:
            params["hl"] = "en"
            params["gl"] = "us" # Geo Location preference
        
        try:
            # å¿…é¡»å¸¦ Headerï¼Œå¦åˆ™ Google ä»¥ä¸ºæ˜¯è„šæœ¬
            headers = {
                #"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8" if self._is_chinese_query(query) else "en-US,en;q=0.9",
                "Referer": "https://www.google.com/"
            }
            
            resp = await session.get(base_url, params=params, headers=headers, timeout=15)
            
            if resp.status_code == 429:
                self.logger.warning("Google Search: CAPTCHA triggered (429). Proxy might be dirty.")
                return []
            if resp.status_code != 200:
                self.logger.warning(f"Google Search failed: {resp.status_code}")
                return []

            soup = BeautifulSoup(resp.content, "lxml")
            results = []
            
            # Google æ ‡å‡†ç»“æœé€šå¸¸åœ¨ div.g é‡Œ
            for g in soup.select("div.g"):
                if len(results) >= limit: break
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    h3 = g.select_one("h3")
                    link = g.select_one("a")
                    
                    if h3 and link and link.has_attr("href"):
                        title = h3.get_text()
                        href = link["href"]
                        
                        # è·³è¿‡ Google çš„ç›¸å…³æœç´¢æ¨èç­‰æ— æ•ˆé“¾æ¥
                        if href.startswith("/search") or "google.com" in href:
                            continue

                        # å°è¯•æå–æ‘˜è¦ (Google çš„æ‘˜è¦ class å¾ˆä¹±ï¼Œé€šå¸¸æ˜¯ div é‡Œçš„æ–‡æœ¬)
                        # è¿™é‡Œç”¨ä¸€ç§æ¯”è¾ƒç²—æš´ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼šæ‰¾ h3 åé¢æœ€å¤§çš„é‚£æ®µå­—
                        snippet = "No snippet"
                        text_divs = g.select("div[style*='-webkit-line-clamp']") # å¾ˆå¤šæ—¶å€™æ‘˜è¦æœ‰è¿™ä¸ªå±æ€§
                        if not text_divs:
                             # Fallback: æ‰¾æ‰€æœ‰ span/div
                             text_divs = g.select("div span")
                        
                        for t in text_divs:
                            txt = t.get_text()
                            if len(txt) > 20: # è®¤ä¸ºæ˜¯æ‘˜è¦
                                snippet = txt
                                break

                        results.append({"title": title, "href": href, "body": snippet, "source": "Google"})
                except:
                    continue
            
            return results
        except Exception as e:
            self.logger.warning(f"Google Scrape Exception: {e}")
            return []

    # === 2. Bing Search (Fallback, ä¸­å›½ç›´è¿) ===
    async def _search_bing(self, session, query, limit=10):
        """
        [Easy Mode] ä¼˜åŒ–ç‰ˆ Bing æœç´¢ã€‚
        å¼ºåˆ¶ä½¿ç”¨å›½é™…ç‰ˆå‚æ•°ï¼Œé˜²æ­¢è¢«é‡å®šå‘åˆ°å›½å†…ç‰ˆã€‚
        """
        base_url = "https://www.bing.com/search"
        # 1. åŸºç¡€é…ç½®ï¼šå¼ºåˆ¶è¿æ¥ US æœåŠ¡å™¨ä»¥è·å–å®Œæ•´çš„ç´¢å¼•åº“ (Global Index)
        # 'cc=US' ä¸»è¦æ˜¯ä¸ºäº†ç‰©ç†å±‚é¢å‘Šè¯‰ Bing "æˆ‘æƒ³è¦å›½é™…ç‰ˆçš„åº“"ï¼Œé˜²æ­¢è¢«é‡å®šå‘åˆ°é˜‰å‰²ç‰ˆ
        params = {
            "q": query, 
            "count": str(limit + 5),
            "cc": "US" 
        }
        
        # 2. è¯­è¨€è‡ªé€‚åº”ï¼šæ ¹æ® Query å†³å®šâ€œå¸‚åœºåå¥½â€
        if self._is_chinese_query(query):
            self.logger.info(f"ğŸ‡¨ğŸ‡³ Detected Chinese Query: '{query}'. Tuning for Chinese results.")
            # æ˜¾å¼å‘Šè¯‰ Bingï¼šè™½ç„¶æˆ‘è¿çš„æ˜¯ US æœåŠ¡å™¨ï¼Œä½†è¯·ä¼˜å…ˆç»™æˆ‘ä¸­æ–‡å†…å®¹
            params["setlang"] = "zh-CN"
            # ç”šè‡³å¯ä»¥ä¸è®¾ setmktï¼Œè®©å®ƒè‡ªç„¶åŒ¹é…ï¼›æˆ–è€…è®¾ä¸º zh-CN
            # æ³¨æ„ï¼šå¦‚æœè®¾äº† setmkt=zh-CNï¼Œæœ‰äº›æ—¶å€™å¯èƒ½ä¼šè¢« Bing å¼ºè½¬å› cn.bing.comï¼Œ
            # æ‰€ä»¥ä¿å®ˆç­–ç•¥æ˜¯ï¼šåªè®¾ setlangï¼Œä¸è®¾ setmktï¼Œæˆ–è€… setmkt ç•™ç©º
        else:
            self.logger.info(f"ğŸ‡ºğŸ‡¸ Detected Non-Chinese Query: '{query}'. Tuning for Global/English results.")
            params["setlang"] = "en"
            params["setmkt"] = "en-US" # è‹±æ–‡æœç´¢æ—¶ï¼Œå¼ºåˆ¶ US å¸‚åœºç»“æœè´¨é‡æœ€é«˜

        try:
            # å¿…é¡»å¸¦ Headerï¼Œå¦åˆ™ Bing å¯èƒ½ä¼šæ ¹æ® IP å¼ºè¡Œé”å®šè¯­è¨€
            headers = {
                #"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8" if self._is_chinese_query(query) else "en-US,en;q=0.9"
            }
            
            resp = await session.get(base_url, params=params, headers=headers, timeout=10)
            if resp.status_code != 200: return []

            soup = BeautifulSoup(resp.content, "lxml")
            results = []
            
            for item in soup.select("li.b_algo"):
                if len(results) >= limit: break
                try:
                    h2 = item.select_one("h2 a")
                    if not h2: continue
                    
                    results.append({
                        "title": h2.get_text(),
                        "href": h2['href'],
                        "body": item.select_one(".b_caption p").get_text() if item.select_one(".b_caption p") else "",
                        "source": "Bing"
                    })
                except: continue
            return results
        except Exception as e:
            self.logger.error(f"Bing Search Error: {e}")
            return []

    # === 3. ArXiv API (STEM å¢å¼º) ===
    async def _search_arxiv(self, session, query, limit=5):
        """
        [Bonus] é’ˆå¯¹ STEM é¢†åŸŸï¼Œç›´æ¥æŸ¥ ArXiv APIã€‚
        è¿™æ˜¯æå…¶é«˜è´¨é‡çš„æºï¼Œç»å¯¹æ²¡æœ‰è¥é”€å·ã€‚
        """
        # ArXiv API ä¸éœ€è¦ä»£ç†ï¼Œä¹Ÿä¸éœ€è¦ Cookieï¼Œéå¸¸ç¨³
        api_url = "http://export.arxiv.org/api/query"
        # ç®€å•çš„é¢„å¤„ç†ï¼šæŠŠç©ºæ ¼æ¢æˆ AND
        safe_query = urllib.parse.quote(query)
        params = f"search_query=all:{safe_query}&start=0&max_results={limit}"
        
        try:
            # ArXiv å¾ˆå¿«ï¼Œä¸éœ€è¦ä¼ªè£…ï¼Œæ™®é€š get å³å¯
            resp = await session.get(f"{api_url}?{params}", timeout=10)
            if resp.status_code != 200: return []
            
            # è§£æ XML
            root = ET.fromstring(resp.content)
            results = []
            # XML å‘½åç©ºé—´
            ns = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', ns):
                title = entry.find('atom:title', ns).text.replace('\n', ' ').strip()
                summary = entry.find('atom:summary', ns).text.replace('\n', ' ').strip()[:200]
                link = entry.find('atom:id', ns).text # ArXiv çš„ ID url
                
                # ä¼˜å…ˆæ‰¾ PDF é“¾æ¥
                pdf_link = link
                for l in entry.findall('atom:link', ns):
                    if l.attrib.get('title') == 'pdf':
                        pdf_link = l.attrib['href']
                
                results.append({
                    "title": f"[ArXiv] {title}",
                    "href": pdf_link,
                    "body": summary,
                    "source": "ArXiv API"
                })
            return results
        except Exception as e:
            self.logger.warning(f"ArXiv Search Error: {e}")
            return []

    async def _search_baidu_xueshu(self, session, query, limit=5):
        """
        [CN Scholar] ç™¾åº¦å­¦æœ¯æœç´¢ã€‚
        èšåˆäº†çŸ¥ç½‘ã€ä¸‡æ–¹ç­‰å…ƒæ•°æ®ï¼Œä¸”èƒ½æ‰¾åˆ°éƒ¨åˆ†å…è´¹ PDF é“¾æ¥ã€‚
        """
        base_url = "https://xueshu.baidu.com/s"
        params = {"wd": query, "tn": "SE_baiduxueshu_c1gjeupa", "ie": "utf-8"}
        
        try:
            # ç™¾åº¦å­¦æœ¯å¯¹ header æ¯”è¾ƒæ•æ„Ÿï¼Œå»ºè®®æ¨¡æ‹Ÿå®Œæ•´ header
            headers = {
                #"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/110.0.0.0 Safari/537.36",
                "Referer": "https://xueshu.baidu.com/"
            }
            
            resp = await session.get(base_url, params=params, headers=headers, timeout=10)
            if resp.status_code != 200: return []
            
            soup = BeautifulSoup(resp.content, "lxml")
            results = []
            
            # ç™¾åº¦å­¦æœ¯çš„ç»“æœå¡ç‰‡é€šå¸¸æ˜¯ div.sc_content
            for item in soup.select("div.sc_content"):
                if len(results) >= limit: break
                try:
                    # 1. æ ‡é¢˜å’Œè¯¦æƒ…é¡µé“¾æ¥
                    h3 = item.select_one("h3.t a")
                    if not h3: continue
                    
                    title = h3.get_text().strip()
                    # è¿™æ˜¯ç™¾åº¦å­¦æœ¯çš„è¯¦æƒ…é¡µé“¾æ¥
                    detail_url = h3['href'] 
                    
                    # 2. æ‘˜è¦
                    abstract_div = item.select_one("div.c_abstract")
                    snippet = abstract_div.get_text().strip() if abstract_div else "No abstract"
                    
                    # 3. å°è¯•æå–â€œæ¥æºâ€ä¿¡æ¯ (ä¾‹å¦‚ï¼šçŸ¥ç½‘ã€ä¸‡æ–¹ã€æˆ–è€…æ˜¯ pdf é“¾æ¥)
                    # ç™¾åº¦å­¦æœ¯æœ‰æ—¶å€™ä¼šç›´æ¥ç»™å‡ºä¸‹è½½é“¾æ¥ï¼Œä½†æ›´å¤šæ—¶å€™åœ¨è¯¦æƒ…é¡µé‡Œ
                    # å¯¹äº Agentï¼Œå…ˆæŠŠè¯¦æƒ…é¡µç»™å®ƒï¼Œè®©é€’å½’çˆ¬è™«è¿›å»æ‰¾ä¸‹è½½é“¾æ¥æ˜¯æ›´ç¨³çš„ç­–ç•¥
                    
                    results.append({
                        "title": f"[ç™¾åº¦å­¦æœ¯] {title}",
                        "href": detail_url, # æ³¨æ„ï¼šè¿™æ˜¯ä¸­é—´é¡µï¼Œéœ€è¦ Hunter è¿›å» Deep Hunt
                        "body": snippet,
                        "source": "Baidu Xueshu"
                    })
                except:
                    continue
            return results
        except Exception as e:
            self.logger.warning(f"Baidu Xueshu Error: {e}")
            return []

    # === 4. æ™ºèƒ½è·¯ç”±å…¥å£ ===
    async def _smart_search_entry(self, query: str, limit: int = 10, domain: str = "STEM"):
        """
        [Master Controller] æ™ºèƒ½å†³å®šèµ°å“ªæ¡è·¯ã€‚
        """
        # æ£€æµ‹ä»£ç†
        proxy = os.environ.get("HTTP_PROXY") or os.environ.get("HTTPS_PROXY") or os.environ.get("ALL_PROXY")
        has_proxy = proxy is not None and len(proxy) > 0
        is_chinese = self._is_chinese_query(query)
        should_search_arxiv = (domain == "STEM") and (not is_chinese)
        all_results = []
        
        
        # ä½¿ç”¨ impersonate="chrome110" æ˜¯æœ€ç¨³çš„
        # å¦‚æœæœ‰ä»£ç†ï¼Œcurl_cffi ä¼šè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå¯ä»¥æ˜¾å¼ä¼ å…¥ proxies=...
        async with AsyncSession(impersonate="chrome") as session:
            
            # --- ç­–ç•¥ A: ä»£ç†ä¼˜å…ˆ (Google) ---
            if has_proxy:
                self.logger.info(f"ğŸŒ Proxy detected. Attempting Google Search for: {query}")
                google_results = await self._search_google(session, query, limit)
                if google_results:
                    all_results.extend(google_results)
                else:
                    self.logger.warning("Google failed despite proxy. Falling back to Bing.")
            
            # --- ç­–ç•¥ B: å…œåº•/ç›´è¿ (Bing) ---
            # å¦‚æœ Google æ²¡æœåˆ°ï¼Œæˆ–è€…æ²¡ä»£ç†ï¼Œç”¨ Bing è¡¥ä½
            if not all_results:
                self.logger.info(f"ğŸŒ Using Bing Search (Global Mode) for: {query}")
                bing_results = await self._search_bing(session, query, limit)
                all_results.extend(bing_results)

            # --- ç­–ç•¥ C: é¢†åŸŸå¢å¼º (ArXiv) ---
            # å¦‚æœæ˜¯ç†å·¥ç§‘ï¼Œå¼ºè¡Œæ³¨å…¥ ArXiv ç»“æœ (è¿™ä¸ª API åœ¨å›½å†…é€šå¸¸èƒ½ç›´è¿ï¼Œä¸è¡Œå°±èµ°ä»£ç†)
            if should_search_arxiv:
                self.logger.info(f"ğŸ§ª STEM domain detected. Injecting ArXiv results...")
                # ArXiv ç»“æœå°‘è€Œç²¾ï¼Œå– 3-5 ä¸ªå³å¯
                arxiv_results = await self._search_arxiv(session, query, limit=5)
                # æŠŠ ArXiv ç»“æœæ’åˆ°æœ€å‰é¢ï¼å› ä¸ºå®ƒä»¬è´¨é‡æœ€é«˜
                all_results = arxiv_results + all_results

            # === ç­–ç•¥ D: ä¸­æ–‡å­¦æœ¯å¢å¼º (ç™¾åº¦å­¦æœ¯) ===
            # å¦‚æœæ˜¯ä¸­æ–‡æœç´¢ï¼Œä¸”å±äº STEM æˆ– Humanities (å†å²/æ–‡å­¦æ›´éœ€è¦å­¦æœ¯æœç´¢)
            if is_chinese and domain in ["STEM", "HUMANITIES"]:
                self.logger.info(f"ğŸ“š Chinese Academic query detected. Injecting Baidu Xueshu results...")
                
                # ç™¾åº¦å­¦æœ¯ç»“æœé€šå¸¸æ¯”è¾ƒç²¾å‡†ï¼Œå– 3-5 ä¸ªå³å¯
                baidu_results = await self._search_baidu_xueshu(session, query, limit=5)
                
                # æ’å…¥åˆ°ç»“æœåˆ—è¡¨å‰é¢ï¼Œèµ‹äºˆé«˜ä¼˜å…ˆçº§
                all_results = baidu_results + all_results

        # ç®€å•çš„å»é‡ (ä»¥ href ä¸º key)
        seen_urls = set()
        unique_results = []
        for r in all_results:
            if r['href'] not in seen_urls:
                seen_urls.add(r['href'])
                unique_results.append(r)
        
        self.logger.info(f"âœ… Smart Search finished. Found {len(unique_results)} URLs from {[r['source'] for r in unique_results[:3]]}...")
        return unique_results[:limit] # è¿”å›è¯·æ±‚çš„æ•°é‡